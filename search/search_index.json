{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Introduction This is a software development guide. Getting started If you are new to this repository we want to get you started right away! Please have a look at one of the assignments . Setup This section described how to setup the repo for development. To preview changes in the finished format you need mkdocs . If you are using conda you should be able to run: conda create -n D_mkdocs pip install mkdocs # now you should be able to run mkdocs --help # to preview changes in a browser, navigate to the repo and run: mkdocs serve","title":"Home"},{"location":"#introduction","text":"This is a software development guide.","title":"Introduction"},{"location":"#getting-started","text":"If you are new to this repository we want to get you started right away! Please have a look at one of the assignments .","title":"Getting started"},{"location":"#setup","text":"This section described how to setup the repo for development. To preview changes in the finished format you need mkdocs . If you are using conda you should be able to run: conda create -n D_mkdocs pip install mkdocs # now you should be able to run mkdocs --help # to preview changes in a browser, navigate to the repo and run: mkdocs serve","title":"Setup"},{"location":"conda/","text":"Conda Introduction Environments Packages Version 1. Introduction Conda is a package, dependency and environment manager for any language. Conda actually consists of Anaconda and a smaller version called Miniconda. Miniconda is usually sufficient most of the time, but you can also install anaconda from miniconda using: $ conda create -n test_ananconda anaconda The conda command is the primary interface for managing installations of various packages and environments. 2. Environments A conda environment is a directory that contains a specific collection of conda packages that you have installed. If you change one environment, your other environments are not affected. Condas default environment is called base or root. To create an environment, run: $ conda create --name [environment] You can easily activate or deactivate environments, which is how you switch between them. To activate an environment, run: $ source activate [environment] Conda puts an asterisk (*) in front of the active environment. You can view your environments with: $ conda info --envs or $ conda env list Your active environment will also show in front of your prompt: (my_env) $ To deactivate your current environment: $ source deactivate You can copy your environment by: $ conda create --name [environment] --clone [copy_of_environment] You can also share your environment with someone by giving them a copy of your environment.yaml file. To remove an environment, run: $ conda remove --name [environment] --all 3. Packages A conda package is a compressed tarball file that contains system-level libraries, Python or other modules, executable programs and other components. Conda keeps track of the dependencies between packages and platforms. Conda packages are downloaded from remote channels, which are URLs to directories containing conda packages. The conda command searches a default set of channels, and packages are automatically downloaded and updated from a central repository. To install conda packages in your active environment (default or other), run: $ conda install [packagename_1] [packagename_2] To install conda packages in an environment, run: $ conda install --name [environemnt] [packagename_1] [packagename_2] It is also possible to create a new environment and install packages at the same time $ conda create --name [environment] [packagename] To list all packages in a conda environment, run: $ conda list --name [environment] To remove a package, run: $ conda remove --name [packagename] 3.1 Versions To install or change a specific version of a package use the syntax [packagename]=[version]. $ conda install samtools=1.6.0 vt=170135","title":"Conda"},{"location":"conda/#conda","text":"Introduction Environments Packages Version","title":"Conda"},{"location":"conda/#1-introduction","text":"Conda is a package, dependency and environment manager for any language. Conda actually consists of Anaconda and a smaller version called Miniconda. Miniconda is usually sufficient most of the time, but you can also install anaconda from miniconda using: $ conda create -n test_ananconda anaconda The conda command is the primary interface for managing installations of various packages and environments.","title":"1. Introduction"},{"location":"conda/#2-environments","text":"A conda environment is a directory that contains a specific collection of conda packages that you have installed. If you change one environment, your other environments are not affected. Condas default environment is called base or root. To create an environment, run: $ conda create --name [environment] You can easily activate or deactivate environments, which is how you switch between them. To activate an environment, run: $ source activate [environment] Conda puts an asterisk (*) in front of the active environment. You can view your environments with: $ conda info --envs or $ conda env list Your active environment will also show in front of your prompt: (my_env) $ To deactivate your current environment: $ source deactivate You can copy your environment by: $ conda create --name [environment] --clone [copy_of_environment] You can also share your environment with someone by giving them a copy of your environment.yaml file. To remove an environment, run: $ conda remove --name [environment] --all","title":"2. Environments"},{"location":"conda/#3-packages","text":"A conda package is a compressed tarball file that contains system-level libraries, Python or other modules, executable programs and other components. Conda keeps track of the dependencies between packages and platforms. Conda packages are downloaded from remote channels, which are URLs to directories containing conda packages. The conda command searches a default set of channels, and packages are automatically downloaded and updated from a central repository. To install conda packages in your active environment (default or other), run: $ conda install [packagename_1] [packagename_2] To install conda packages in an environment, run: $ conda install --name [environemnt] [packagename_1] [packagename_2] It is also possible to create a new environment and install packages at the same time $ conda create --name [environment] [packagename] To list all packages in a conda environment, run: $ conda list --name [environment] To remove a package, run: $ conda remove --name [packagename]","title":"3. Packages"},{"location":"conda/#31-versions","text":"To install or change a specific version of a package use the syntax [packagename]=[version]. $ conda install samtools=1.6.0 vt=170135","title":"3.1 Versions"},{"location":"conda/activate/","text":"Activate conda envs After you have created a conda environment, conda tells you the following: # # To activate this environment, use: # > source activate D_mkdocs # # To deactivate this environment, use: # > source deactivate D_mkdocs # Production and stage We have two special conda environments with special configurations: prod and stage . prod is the main environment in which not surprisingly, our production tools are running. stage is a recent copy of prod used for testing of your latest finished branch and to test deployment. What is so special about prod or stage ? They come with all tools already pointing to their configurations and need to be activated using a shell script. To activate prod environment, type: up or useprod To activate stage environment, type: us or usestage I want to know more us and usestage are aliases pointing to a script: alias usestage='source ${HOME}/servers/resources/activate-stage.sh' That script will activate the correct environment(s) and set the aliases for the environment.","title":"How do I activate an env?"},{"location":"conda/activate/#activate-conda-envs","text":"After you have created a conda environment, conda tells you the following: # # To activate this environment, use: # > source activate D_mkdocs # # To deactivate this environment, use: # > source deactivate D_mkdocs #","title":"Activate conda envs"},{"location":"conda/activate/#production-and-stage","text":"We have two special conda environments with special configurations: prod and stage . prod is the main environment in which not surprisingly, our production tools are running. stage is a recent copy of prod used for testing of your latest finished branch and to test deployment. What is so special about prod or stage ? They come with all tools already pointing to their configurations and need to be activated using a shell script. To activate prod environment, type: up or useprod To activate stage environment, type: us or usestage","title":"Production and stage"},{"location":"conda/activate/#i-want-to-know-more","text":"us and usestage are aliases pointing to a script: alias usestage='source ${HOME}/servers/resources/activate-stage.sh' That script will activate the correct environment(s) and set the aliases for the environment.","title":"I want to know more"},{"location":"conda/conda_conventions/","text":"Conda conventions Avoid installing anything in the conda default environment (root) if possible Naming conventions for conda environments: Production: [env_type=P]_[logical_name]_[creation_date] Development: [env_type=D]_[logical_name]_[creation_date]_[signature] Archive: [env_type=A]_[logical_name]_[timestamp]_[signature] env_type: environment types are D (Develop), P (Production), or A (Archive). creation_date: %y%m%d timestamp: %y%m%d %H%M%S . logical_name: whatever makes sense. signature: something to show who created the environment. Use the two- or three letter name acronyms assigned to you.","title":"How to name your conda env"},{"location":"conda/conda_conventions/#conda-conventions","text":"Avoid installing anything in the conda default environment (root) if possible Naming conventions for conda environments: Production: [env_type=P]_[logical_name]_[creation_date] Development: [env_type=D]_[logical_name]_[creation_date]_[signature] Archive: [env_type=A]_[logical_name]_[timestamp]_[signature] env_type: environment types are D (Develop), P (Production), or A (Archive). creation_date: %y%m%d timestamp: %y%m%d %H%M%S . logical_name: whatever makes sense. signature: something to show who created the environment. Use the two- or three letter name acronyms assigned to you.","title":"Conda conventions"},{"location":"conda/confinement/","text":"Confinement It is possible to truely compartimentalize your conda env with the inclusion of bash functions, bash env variables, and aliases! Use case: aliases So, you want to have your aliases available in your conda env only? Solution? create a pre-activate script and post-deactivate script! cd ${your_conda_env} # e.g. cd ./miniconda/envs/production/ mkdir -p ./etc/conda/activate.d mkdir -p ./etc/conda/deactivate.d touch ./etc/conda/activate.d/alias.sh touch ./etc/conda/deactivate.d/unalias.sh The ./etc/conda/activate.d/alias.sh is the pre-activate script and will be run before your conda env becomes active. The ./etc/conda/deactivate.d/unalias.sh is the post-deactivate script and will be run after your conda env is deactivated. cat ./etc/conda/activate.d/alias.sh alias lol='ls -ltr' cat ./etc/conda/deactivate.d/unalias.sh unalias lol Sources https://conda.io/docs/user-guide/tasks/manage-environments.html#saving-environment-variables","title":"Confinement"},{"location":"conda/confinement/#confinement","text":"It is possible to truely compartimentalize your conda env with the inclusion of bash functions, bash env variables, and aliases!","title":"Confinement"},{"location":"conda/confinement/#use-case-aliases","text":"So, you want to have your aliases available in your conda env only? Solution? create a pre-activate script and post-deactivate script! cd ${your_conda_env} # e.g. cd ./miniconda/envs/production/ mkdir -p ./etc/conda/activate.d mkdir -p ./etc/conda/deactivate.d touch ./etc/conda/activate.d/alias.sh touch ./etc/conda/deactivate.d/unalias.sh The ./etc/conda/activate.d/alias.sh is the pre-activate script and will be run before your conda env becomes active. The ./etc/conda/deactivate.d/unalias.sh is the post-deactivate script and will be run after your conda env is deactivated. cat ./etc/conda/activate.d/alias.sh alias lol='ls -ltr' cat ./etc/conda/deactivate.d/unalias.sh unalias lol","title":"Use case: aliases"},{"location":"conda/confinement/#sources","text":"https://conda.io/docs/user-guide/tasks/manage-environments.html#saving-environment-variables","title":"Sources"},{"location":"editors/","text":"Editors Choosing an editor is a person and highly subjective decision. We don't advocate any particular flavor but here are a few competent ones that we recommend you start with: Sublime Text Atom Vim, GVim, MACVim etc. Textmate Visual Studio Code","title":"Choose your editor :)"},{"location":"editors/#editors","text":"Choosing an editor is a person and highly subjective decision. We don't advocate any particular flavor but here are a few competent ones that we recommend you start with: Sublime Text Atom Vim, GVim, MACVim etc. Textmate Visual Studio Code","title":"Editors"},{"location":"git/","text":"Quick reference guide to Github Purpose This document offers an overview of the most important commands of git and it is intended to be a quick start guide to work with git repositories. Download and setup Download and install Git for Linux If you are working on a Debian-based version of Linux you can install the basic Git tools via APT: sudo apt-get install git-all Here are instructions on how to install Git under other Linux distributions. Download and install Git for OSX The latest version of Git for Mac can be downloaded here , use this binary installer to get an up-to date version of he software. If you are installing on a Mac with OS X 10.9 you can install Git from command line with Homebrew (get Homebrew here ) by typing: brew install git Configure username and email The first time you are using git from command line you should set up a username and a password. To ease the authentication for each action, go to your local repository and type in the following commands: git config user.email \"you@example.com\" git config user.name \"YourUsername\" If you are working locally (your computer only), you could also add the --global flag to set these values for all your repositories. Create a new repository There are several ways how one can create a new local repository Create a new repository from scratch Create a directory with the name of the new repository and enter it from the command line. To initialize the repository type: git init This command will create a .git subfolder. Clone a repository from a project in the clinical-genomics GitHub organisation git clone https://github.com/clinical-genomics/development.git Where the url which can be obtained from the web page of your local fork, by clicking on the \"clone or download\" green button. The git clone command then creates a folder with the project and pulls the latest data from your fork in it. Clone any other repository When you are contributing to an existing repository, the first thing you should do is forking this repository to your github space. A fork is basically a copy of the original project, on which you can experiment as much as you want without changing the original project. how to fork a repository Then clone your fork. git clone https://github.com/[your user name]/development.git Keep your fork up to date Before committing and pushing changes to your remote repository you should check that you are working on an updated version of it. To update your fork follow these instructions: First you need to get the latest version of the original project you forked from. To do so, from command line move to the main folder containing your project and type: git remote add upstream path/to/repo/you/forked/from git fetch upstream git pull upstream master git pull upstream develop # develop is the version you start from for your work! You don't have to call the remote \"upstream\" but you can chose any name for it. To visualize all remotes you are working with and their respective shortnames use the command: git remote -v To remove remotes instead, use the command \"git remove rm\": git remote rm name_of_remote After pulling data from the remote your local copy of the repo should be up-to-date with the upstream. Now you need to update your fork (the copy existing online, in GitHub) accordingly. If you wish to update you master branch on your fork, for instance, you'd type these commands: # be sure to place yourself in your master branch git checkout master git rebase upstream/master git push You use the same commands to update any other branch of your forked repository. Git data transport structure and commands The following image shows the structure of the git commands and spaces: Your workspace is the folder that holds the actual working files while the \"index\" is the space is a staging area where you add your changes before committing them to the local repository or \"head\". To send the changes to your remote repository you use the \"push\" command. Here is a link to a page explaining the common Git terminology: https://help.github.com/articles/github-glossary Create a new branch or feature for your work To create a new branch, from command line move to the main folder containing your project and type: git checkout -b name_of_your_feature develop If everything went fine then you'll get the following message: \"Switched to a new branch name_of_your_feature\". This means you are already inside the new feature. To make sure that you are in the right branch type: git branch Whenever you want to move to another branch/feature the command is: git checkout [my-topic] Make changes and commit them to your local repository Before committing any changes to your local repository, and finally to the remote repository, make sure you were working on the branch created. After you are satisfied with your code you can start to commit to the remote repository. To check that there are actually changes to add and commit, type: git status This command will give you info on the status of your working directory comparing it with the local and remote repositories. To update the local repository to the latest status, by fetchin and merging remote changes, type: git pull The you can start adding your changes to the index with the command: git add filename_to_send_to_index The process of adding a file to the index is also called staging. Be sure to add all the files you want to include to the index. Before merging the changes you can preview them with the command: git diff source_branch target_branch Calling the \"diff\" command without arguments will list all the differences between the index and your workspace, i.e. all the files that you could add to the index. To commit changes from the index to the local repository (HEAD), type: git commit Alternatively you can add and commit the files to the local repository in only one command: git commit -a NOTE: Be very careful with the above command, since it gives you less control over the single steps of the workflow and the files you really want to commit. Before using it, make sure everything is under control by using the \"git status\" and \"git diff\" commands. Prevent commiting tmp files your editor makes! To include a description of the commit use the -m flag git commit -m \"Example of a commit message\" It is possible to close a GitHub issue by referencing it in a commit message or pull request. The issue will be closed when the code is merged to the default branch git commit -m \"Fixes #36\" For more examples on how to close issues using keywords see this guide . Pushing changes to your remote repository After \"git add\" and \"git commit\", or \"commit -a\", your changes are in the HEAD of your local repository. To send them to the remote repository use the command \"push\": git push Create a pull request to the parent repository You create a \"pull request\" when you want to propose the introduction of your changes to the repository where your project was cloned or forked from. Pull requests are created on github on the page of the github repository. By default pull requests are directed to the default branch (master). For a more detailed explanation of pull requests visit the Github help page. Once the pull request is merged and closed you can safely remove the branch you've been working on. To remove a generic branch both remotely and locally type: git branch -d name_of_your_branch #removes it locally git push origin --delete name_of_your_branch #removes it remotely It is possible to close a GitHub issue by referencing it in a pull request title or description, e.g. \"Fixes #46\". The issue will be closed when the code is merged to the default branch. A more in-depth description can be found in this guide . Github usage references Github short guide: (https://guides.github.com/activities/hello-world) Github book: (https://git-scm.com/book/en/v2) GitHub help page: (https://help.github.com) Glossary: (https://help.github.com/articles/github-glossary) How to write Github documentation Markdown is the documentation format of choice for code on GitHub. If you want to live-preview files in the format they will show up before pushing you can use the handy Grip tool. It super easy to use! All you need to do is: pip install grip # cd to your project directory grip # open your web-browser at http://localhost:6419 .gitignore This is a special file that's usually part of every repo. Here you can tell git which files it should exclude from version control. Examples include private files with sensitive data, temporary log files, and large files used for testing. GitHub hosts a brief guide about the format. There's also a great resource for finding templates for every language to use as starting points.","title":"Quick reference guide to Github"},{"location":"git/#quick-reference-guide-to-github","text":"","title":"Quick reference guide to Github"},{"location":"git/#purpose","text":"This document offers an overview of the most important commands of git and it is intended to be a quick start guide to work with git repositories.","title":"Purpose"},{"location":"git/#download-and-setup","text":"","title":"Download and setup"},{"location":"git/#download-and-install-git-for-linux","text":"If you are working on a Debian-based version of Linux you can install the basic Git tools via APT: sudo apt-get install git-all Here are instructions on how to install Git under other Linux distributions.","title":"Download and install Git for Linux"},{"location":"git/#download-and-install-git-for-osx","text":"The latest version of Git for Mac can be downloaded here , use this binary installer to get an up-to date version of he software. If you are installing on a Mac with OS X 10.9 you can install Git from command line with Homebrew (get Homebrew here ) by typing: brew install git","title":"Download and install Git for OSX"},{"location":"git/#configure-username-and-email","text":"The first time you are using git from command line you should set up a username and a password. To ease the authentication for each action, go to your local repository and type in the following commands: git config user.email \"you@example.com\" git config user.name \"YourUsername\" If you are working locally (your computer only), you could also add the --global flag to set these values for all your repositories.","title":"Configure username and email"},{"location":"git/#create-a-new-repository","text":"There are several ways how one can create a new local repository","title":"Create a new repository"},{"location":"git/#create-a-new-repository-from-scratch","text":"Create a directory with the name of the new repository and enter it from the command line. To initialize the repository type: git init This command will create a .git subfolder.","title":"Create a new repository from scratch"},{"location":"git/#clone-a-repository-from-a-project-in-the-clinical-genomics-github-organisation","text":"git clone https://github.com/clinical-genomics/development.git Where the url which can be obtained from the web page of your local fork, by clicking on the \"clone or download\" green button. The git clone command then creates a folder with the project and pulls the latest data from your fork in it.","title":"Clone a repository from a project in the clinical-genomics GitHub organisation"},{"location":"git/#clone-any-other-repository","text":"When you are contributing to an existing repository, the first thing you should do is forking this repository to your github space. A fork is basically a copy of the original project, on which you can experiment as much as you want without changing the original project. how to fork a repository Then clone your fork. git clone https://github.com/[your user name]/development.git","title":"Clone any other repository"},{"location":"git/#keep-your-fork-up-to-date","text":"Before committing and pushing changes to your remote repository you should check that you are working on an updated version of it. To update your fork follow these instructions: First you need to get the latest version of the original project you forked from. To do so, from command line move to the main folder containing your project and type: git remote add upstream path/to/repo/you/forked/from git fetch upstream git pull upstream master git pull upstream develop # develop is the version you start from for your work! You don't have to call the remote \"upstream\" but you can chose any name for it. To visualize all remotes you are working with and their respective shortnames use the command: git remote -v To remove remotes instead, use the command \"git remove rm\": git remote rm name_of_remote After pulling data from the remote your local copy of the repo should be up-to-date with the upstream. Now you need to update your fork (the copy existing online, in GitHub) accordingly. If you wish to update you master branch on your fork, for instance, you'd type these commands: # be sure to place yourself in your master branch git checkout master git rebase upstream/master git push You use the same commands to update any other branch of your forked repository.","title":"Keep your fork up to date"},{"location":"git/#git-data-transport-structure-and-commands","text":"The following image shows the structure of the git commands and spaces: Your workspace is the folder that holds the actual working files while the \"index\" is the space is a staging area where you add your changes before committing them to the local repository or \"head\". To send the changes to your remote repository you use the \"push\" command. Here is a link to a page explaining the common Git terminology: https://help.github.com/articles/github-glossary","title":"Git data transport structure and commands"},{"location":"git/#create-a-new-branch-or-feature-for-your-work","text":"To create a new branch, from command line move to the main folder containing your project and type: git checkout -b name_of_your_feature develop If everything went fine then you'll get the following message: \"Switched to a new branch name_of_your_feature\". This means you are already inside the new feature. To make sure that you are in the right branch type: git branch Whenever you want to move to another branch/feature the command is: git checkout [my-topic]","title":"Create a new branch or feature for your work"},{"location":"git/#make-changes-and-commit-them-to-your-local-repository","text":"Before committing any changes to your local repository, and finally to the remote repository, make sure you were working on the branch created. After you are satisfied with your code you can start to commit to the remote repository. To check that there are actually changes to add and commit, type: git status This command will give you info on the status of your working directory comparing it with the local and remote repositories. To update the local repository to the latest status, by fetchin and merging remote changes, type: git pull The you can start adding your changes to the index with the command: git add filename_to_send_to_index The process of adding a file to the index is also called staging. Be sure to add all the files you want to include to the index. Before merging the changes you can preview them with the command: git diff source_branch target_branch Calling the \"diff\" command without arguments will list all the differences between the index and your workspace, i.e. all the files that you could add to the index. To commit changes from the index to the local repository (HEAD), type: git commit Alternatively you can add and commit the files to the local repository in only one command: git commit -a NOTE: Be very careful with the above command, since it gives you less control over the single steps of the workflow and the files you really want to commit. Before using it, make sure everything is under control by using the \"git status\" and \"git diff\" commands. Prevent commiting tmp files your editor makes! To include a description of the commit use the -m flag git commit -m \"Example of a commit message\" It is possible to close a GitHub issue by referencing it in a commit message or pull request. The issue will be closed when the code is merged to the default branch git commit -m \"Fixes #36\" For more examples on how to close issues using keywords see this guide .","title":"Make changes and commit them to your local repository"},{"location":"git/#pushing-changes-to-your-remote-repository","text":"After \"git add\" and \"git commit\", or \"commit -a\", your changes are in the HEAD of your local repository. To send them to the remote repository use the command \"push\": git push","title":"Pushing changes to your remote repository"},{"location":"git/#create-a-pull-request-to-the-parent-repository","text":"You create a \"pull request\" when you want to propose the introduction of your changes to the repository where your project was cloned or forked from. Pull requests are created on github on the page of the github repository. By default pull requests are directed to the default branch (master). For a more detailed explanation of pull requests visit the Github help page. Once the pull request is merged and closed you can safely remove the branch you've been working on. To remove a generic branch both remotely and locally type: git branch -d name_of_your_branch #removes it locally git push origin --delete name_of_your_branch #removes it remotely It is possible to close a GitHub issue by referencing it in a pull request title or description, e.g. \"Fixes #46\". The issue will be closed when the code is merged to the default branch. A more in-depth description can be found in this guide .","title":"Create a pull request to the parent repository"},{"location":"git/#github-usage-references","text":"Github short guide: (https://guides.github.com/activities/hello-world) Github book: (https://git-scm.com/book/en/v2) GitHub help page: (https://help.github.com) Glossary: (https://help.github.com/articles/github-glossary)","title":"Github usage references"},{"location":"git/#how-to-write-github-documentation","text":"Markdown is the documentation format of choice for code on GitHub. If you want to live-preview files in the format they will show up before pushing you can use the handy Grip tool. It super easy to use! All you need to do is: pip install grip # cd to your project directory grip # open your web-browser at http://localhost:6419","title":"How to write Github documentation"},{"location":"git/#gitignore","text":"This is a special file that's usually part of every repo. Here you can tell git which files it should exclude from version control. Examples include private files with sensitive data, temporary log files, and large files used for testing. GitHub hosts a brief guide about the format. There's also a great resource for finding templates for every language to use as starting points.","title":".gitignore"},{"location":"git/code-review/","text":"Code review Code reviews exist to spot mistakes that were overlooked during initial development. They are also an excellent opportunity for spreading knowledge and best practices among your team members. I've basically stolen all of this from Thoughbot's guide on Code Review . Establish the idea that everybody gets reviewed. No one stands above the rest. Every patch gets reviewed. However small, there is no valid argument to not review it. As far as possible, code should be reviewed before it gets merged. Make liberal use of GitHub and pull requests; use line-specific comments for example. For bigger patches, don't be afraid to ask to separate them into smaller units. Document code review practices. Role: Reviewer The reviewer should focus on things in this order: Intent (why) Design (how) Implementation Grammar Collect the suggested changes in TODO lists, questions for the code author, and suggested follow-ups for later patches. Workflow We are on a bi-weekly schedule with 2 pull request responsibles. Their job is to review all pull requests that are requested to be merged and put into production. The person reviewing the code signs off on the pull request with a \"\ud83d\udc4d\" or simliar and then merges the code. Hot fixes are excluded from code reviews and can be merged by the author herself. Automate the automatable Aim to discover mistakes as early as possible. This includes running lints, checking for conformance to style guide, etc. Encourage team members to run these tools locally as well as considering them as part of your continuous integration flow. It can also be worth mentioning that it's often easier to take criticism from computers rather than your peers.","title":"Code review"},{"location":"git/code-review/#code-review","text":"Code reviews exist to spot mistakes that were overlooked during initial development. They are also an excellent opportunity for spreading knowledge and best practices among your team members. I've basically stolen all of this from Thoughbot's guide on Code Review . Establish the idea that everybody gets reviewed. No one stands above the rest. Every patch gets reviewed. However small, there is no valid argument to not review it. As far as possible, code should be reviewed before it gets merged. Make liberal use of GitHub and pull requests; use line-specific comments for example. For bigger patches, don't be afraid to ask to separate them into smaller units. Document code review practices.","title":"Code review"},{"location":"git/code-review/#role-reviewer","text":"The reviewer should focus on things in this order: Intent (why) Design (how) Implementation Grammar Collect the suggested changes in TODO lists, questions for the code author, and suggested follow-ups for later patches.","title":"Role: Reviewer"},{"location":"git/code-review/#workflow","text":"We are on a bi-weekly schedule with 2 pull request responsibles. Their job is to review all pull requests that are requested to be merged and put into production. The person reviewing the code signs off on the pull request with a \"\ud83d\udc4d\" or simliar and then merges the code. Hot fixes are excluded from code reviews and can be merged by the author herself.","title":"Workflow"},{"location":"git/code-review/#automate-the-automatable","text":"Aim to discover mistakes as early as possible. This includes running lints, checking for conformance to style guide, etc. Encourage team members to run these tools locally as well as considering them as part of your continuous integration flow. It can also be worth mentioning that it's often easier to take criticism from computers rather than your peers.","title":"Automate the automatable"},{"location":"git/issue-reports/","text":"User stories User stories are short, simple descriptions of a feature told from the perspective of the person who desires the new capability, usually a user or customer of the system. They typically follow a simple template: As a < type of user >, I want < some goal > so that < some reason >.1 As the name suggests, a user story describes how a customer or user employs the product; it is told from the user\u2019s perspective.2 A suggested minimum description: Title: Problem description Steps to reproduce Expected outcome / suggested solution Questions answered Add label PBL Teamname Estimated Title Describe the issue in a sentence that summarises the issue - Example: Clinical-Genomics - Missing feedback on failed login Steps to reproduce Describe the steps that are needed to provoke the encountered issue so that anyone can reproduce the issue. Any special circumstances that you think is affecting the outcome should be described (e.g. environment/browser etc.) Example: 1. Go to Clinical-Genomics in production environment (https://clinical.scilifelab.se) with a web browser 1. Press \"Sign in with Google\" 1. Select any of your accounts that don't have access to the system The aim is to guide the reader towards the problem without having to spend any time on figuring out how or having to communicate with the issue reporter on what to do to provoke the system in order to reproduce the issue. Expected outcome Describe the expected output so that is clear what the goal state is. The aim is to know when issue is resolved. Example: A red toast message saying: This account is not whitelisted Actual outcome Describe what happened when the Steps to reproduce where followed. The aim is to guide the reader towards the root of the problem. (E.g. Is it something that relates to this specific user or something general). Example: There is no message at all about what went wrong when I tried to login A screenshot of the outcome can often be of great help for the reader to understand what you see 1: https://www.mountaingoatsoftware.com/agile/user-stories 2: https://www.romanpichler.com/blog/10-tips-writing-good-user-stories/","title":"User stories"},{"location":"git/issue-reports/#user-stories","text":"User stories are short, simple descriptions of a feature told from the perspective of the person who desires the new capability, usually a user or customer of the system. They typically follow a simple template: As a < type of user >, I want < some goal > so that < some reason >.1 As the name suggests, a user story describes how a customer or user employs the product; it is told from the user\u2019s perspective.2","title":"User stories"},{"location":"git/issue-reports/#a-suggested-minimum-description","text":"Title: Problem description Steps to reproduce Expected outcome / suggested solution Questions answered Add label PBL Teamname Estimated","title":"A suggested minimum description:"},{"location":"git/issue-reports/#title","text":"Describe the issue in a sentence that summarises the issue - Example: Clinical-Genomics - Missing feedback on failed login","title":"Title"},{"location":"git/issue-reports/#steps-to-reproduce","text":"Describe the steps that are needed to provoke the encountered issue so that anyone can reproduce the issue. Any special circumstances that you think is affecting the outcome should be described (e.g. environment/browser etc.) Example: 1. Go to Clinical-Genomics in production environment (https://clinical.scilifelab.se) with a web browser 1. Press \"Sign in with Google\" 1. Select any of your accounts that don't have access to the system The aim is to guide the reader towards the problem without having to spend any time on figuring out how or having to communicate with the issue reporter on what to do to provoke the system in order to reproduce the issue.","title":"Steps to reproduce"},{"location":"git/issue-reports/#expected-outcome","text":"Describe the expected output so that is clear what the goal state is. The aim is to know when issue is resolved. Example: A red toast message saying: This account is not whitelisted","title":"Expected outcome"},{"location":"git/issue-reports/#actual-outcome","text":"Describe what happened when the Steps to reproduce where followed. The aim is to guide the reader towards the root of the problem. (E.g. Is it something that relates to this specific user or something general). Example: There is no message at all about what went wrong when I tried to login A screenshot of the outcome can often be of great help for the reader to understand what you see 1: https://www.mountaingoatsoftware.com/agile/user-stories 2: https://www.romanpichler.com/blog/10-tips-writing-good-user-stories/","title":"Actual outcome"},{"location":"mkdocs/","text":"How do I publish this manual? Good. It seems you want to publish your changes? It is as simple as: mkdocs gh-deploy Tada! Changes should be available shortly on http://www.clinicalgenomics.se/development/ I want more detail We are using github's gh-pages to store the generated manual. Once mkdocs has generated HTML from the markdown, it will be stored in the branch gh-pages . More explanation on the deploying your docs page.","title":"How do I publish this manual?"},{"location":"mkdocs/#how-do-i-publish-this-manual","text":"Good. It seems you want to publish your changes? It is as simple as: mkdocs gh-deploy Tada! Changes should be available shortly on http://www.clinicalgenomics.se/development/","title":"How do I publish this manual?"},{"location":"mkdocs/#i-want-more-detail","text":"We are using github's gh-pages to store the generated manual. Once mkdocs has generated HTML from the markdown, it will be stored in the branch gh-pages . More explanation on the deploying your docs page.","title":"I want more detail"},{"location":"perl/","text":"Perl Introduction Perl is a general-purpose programming language originally developed for text manipulation and now used for a wide range of tasks including system administration, web development, network programming, GUI development, and more. The language is intended to be practical (easy to use, efficient, complete) rather than beautiful (tiny, elegant, minimal). Its major features are that it's easy to use, supports both procedural and object-oriented (OO) programming, has powerful built-in support for text processing, and has one of the world's most impressive collections of third-party modules. A good place to start learning what perl is about is the perlintro at perldocs .","title":"What if I need perl?"},{"location":"perl/#perl","text":"","title":"Perl"},{"location":"perl/#introduction","text":"Perl is a general-purpose programming language originally developed for text manipulation and now used for a wide range of tasks including system administration, web development, network programming, GUI development, and more. The language is intended to be practical (easy to use, efficient, complete) rather than beautiful (tiny, elegant, minimal). Its major features are that it's easy to use, supports both procedural and object-oriented (OO) programming, has powerful built-in support for text processing, and has one of the world's most impressive collections of third-party modules. A good place to start learning what perl is about is the perlintro at perldocs .","title":"Introduction"},{"location":"perl/best_practises/Identifiers/","text":"Identifiers The single most important practice when creating names is to devise a set of grammar rules to which all names must conform. A grammar rules specifies one or more templates (e.g., Noun :: Adjective :: Adjective) that describe how to form the entity on the left of the arrow (e.g., namespace). A suitable grammar rule for naming packages and classes is: namespace \u279d Noun :: Adjective :: Adjective | Noun :: Adjective | Noun This rule might produce package names such as: package Disk; package Disk::Audio; package Disk::DVD; package Disk::DVD::Rewritable; In this scheme, specialized versions of an existing namespace are named by adding adjectives to the name of the more general namespace. Variables should be named according to the data they will store, and as specifically as possible. Variables that are used in more than one block should always have a two-part (or longer) name. A variable is named with a noun, preceded by zero or more adjectives: variable \u279d [adjective _ ]* noun The choice of nouns and adjectives is critical. The nouns in particular should indicate what the variable does in terms of the problem domain, not in terms of the implementation. Use adjectives whenever you can. There is one extra grammatical variation that applies only to hashes and arrays that are used as look-up tables: lookup_variable \u279d [adjective _ ]* noun preposition Adding a preposition to the end of the name makes hash and array accesses much more readable. my %title_of; my @sales_from; For subroutines and methods, a suitable grammatical rule for forming names is: routine \u279d imperative_verb [ _ adjective]? _ noun _ preposition | imperative_verb [ _ adjective]? _ noun _ participle | imperative_verb [ _ adjective]? _ noun This rule results in subroutine names such as: sub get_record; # imperative_verb noun sub get_record_for; # imperative_verb noun preposition sub eat_cookie; # imperative_verb noun sub eat_previous_cookie; # imperative_verb adjective noun sub build_profile; # imperative_verb noun sub build_execution_profile; # imperative_verb adjective noun sub build_execution_profile_using; # imperative_verb adjective noun participle These naming rules particularly the two that put participles or prepositions at the ends of names create identifiers that read far more naturally, often eliminating the need for any additional comments.","title":"Identifiers"},{"location":"perl/best_practises/Identifiers/#identifiers","text":"The single most important practice when creating names is to devise a set of grammar rules to which all names must conform. A grammar rules specifies one or more templates (e.g., Noun :: Adjective :: Adjective) that describe how to form the entity on the left of the arrow (e.g., namespace). A suitable grammar rule for naming packages and classes is: namespace \u279d Noun :: Adjective :: Adjective | Noun :: Adjective | Noun This rule might produce package names such as: package Disk; package Disk::Audio; package Disk::DVD; package Disk::DVD::Rewritable; In this scheme, specialized versions of an existing namespace are named by adding adjectives to the name of the more general namespace. Variables should be named according to the data they will store, and as specifically as possible. Variables that are used in more than one block should always have a two-part (or longer) name. A variable is named with a noun, preceded by zero or more adjectives: variable \u279d [adjective _ ]* noun The choice of nouns and adjectives is critical. The nouns in particular should indicate what the variable does in terms of the problem domain, not in terms of the implementation. Use adjectives whenever you can. There is one extra grammatical variation that applies only to hashes and arrays that are used as look-up tables: lookup_variable \u279d [adjective _ ]* noun preposition Adding a preposition to the end of the name makes hash and array accesses much more readable. my %title_of; my @sales_from; For subroutines and methods, a suitable grammatical rule for forming names is: routine \u279d imperative_verb [ _ adjective]? _ noun _ preposition | imperative_verb [ _ adjective]? _ noun _ participle | imperative_verb [ _ adjective]? _ noun This rule results in subroutine names such as: sub get_record; # imperative_verb noun sub get_record_for; # imperative_verb noun preposition sub eat_cookie; # imperative_verb noun sub eat_previous_cookie; # imperative_verb adjective noun sub build_profile; # imperative_verb noun sub build_execution_profile; # imperative_verb adjective noun sub build_execution_profile_using; # imperative_verb adjective noun participle These naming rules particularly the two that put participles or prepositions at the ends of names create identifiers that read far more naturally, often eliminating the need for any additional comments.","title":"Identifiers"},{"location":"perl/best_practises/best_practises/","text":"Perl Best Practices Following best practices is a good way to create maintainable and readable code and should always be encouraged. However, learning what these best practices are and when they apply in the context of your code can be hard to determine. Luckily, there are several tools to help guide you on your way. Literature We have already mentioned perldocs . Damian Conway's book Perl Best Practices is a very good reference. Code standards Perl Critic Perlcritic is a Perl source code analyzer. It is the executable front-end to the Perl::Critic engine, which attempts to identify awkward, hard to read, error-prone, or unconventional constructs in your code. Most of the rules are based on Damian Conway's book Perl Best Practices. Perl critic allows different degrees of severity. Severity values are integers ranging from 1 (least severe) to 5 (most severe) when analyzing your code. You can also use the severity names if you think it hard to remember the meaning of the integers (1 = brutal and 5 = gentle). The level is controlled with the '--severity' flag. There is also a verbose flag, to print more information about the identified deviations from the perl critic best practises. There are 11 levels of verbosity. Examples perlcritic --severity 4 --verbose 11 my_perl_script.pl Perl critic also has web interface to instantly analyze your code. Perl Tidy Perltidy is a Perl script which indents and reformats Perl scripts to make them easier to read. If you write Perl scripts, or spend much time reading them, you will probably find it useful. Perltidy is an excellent way to automate the code standardisation with minimum of effort. Examples perltidy somefile.pl This will produce a file somefile.pl.tdy containing the script reformatted using the default options, which approximate the style suggested in perlstyle(1). The source file somefile.pl is unchanged. perltidy -b -bext='/' file1.pl file2.pl Create backups of files and modify files in place. The backup files file1.pl.bak and file2.pl.bak will be deleted if there are no errors.","title":"What are perl's best practices?"},{"location":"perl/best_practises/best_practises/#perl-best-practices","text":"Following best practices is a good way to create maintainable and readable code and should always be encouraged. However, learning what these best practices are and when they apply in the context of your code can be hard to determine. Luckily, there are several tools to help guide you on your way.","title":"Perl Best Practices"},{"location":"perl/best_practises/best_practises/#literature","text":"We have already mentioned perldocs . Damian Conway's book Perl Best Practices is a very good reference.","title":"Literature"},{"location":"perl/best_practises/best_practises/#code-standards","text":"","title":"Code standards"},{"location":"perl/best_practises/best_practises/#perl-critic","text":"Perlcritic is a Perl source code analyzer. It is the executable front-end to the Perl::Critic engine, which attempts to identify awkward, hard to read, error-prone, or unconventional constructs in your code. Most of the rules are based on Damian Conway's book Perl Best Practices. Perl critic allows different degrees of severity. Severity values are integers ranging from 1 (least severe) to 5 (most severe) when analyzing your code. You can also use the severity names if you think it hard to remember the meaning of the integers (1 = brutal and 5 = gentle). The level is controlled with the '--severity' flag. There is also a verbose flag, to print more information about the identified deviations from the perl critic best practises. There are 11 levels of verbosity.","title":"Perl Critic"},{"location":"perl/best_practises/best_practises/#examples","text":"perlcritic --severity 4 --verbose 11 my_perl_script.pl Perl critic also has web interface to instantly analyze your code.","title":"Examples"},{"location":"perl/best_practises/best_practises/#perl-tidy","text":"Perltidy is a Perl script which indents and reformats Perl scripts to make them easier to read. If you write Perl scripts, or spend much time reading them, you will probably find it useful. Perltidy is an excellent way to automate the code standardisation with minimum of effort.","title":"Perl Tidy"},{"location":"perl/best_practises/best_practises/#examples_1","text":"perltidy somefile.pl This will produce a file somefile.pl.tdy containing the script reformatted using the default options, which approximate the style suggested in perlstyle(1). The source file somefile.pl is unchanged. perltidy -b -bext='/' file1.pl file2.pl Create backups of files and modify files in place. The backup files file1.pl.bak and file2.pl.bak will be deleted if there are no errors.","title":"Examples"},{"location":"perl/installation/perlbrew/","text":"Perlbrew Perlbrew is a tool to manage multiple perl installations. Allowing for testing your production code against different perl versions, while leaving the vendor perl alone. You can even run your programs against all installations of perl. Perl Install a specific perl version and use it as default. $ perlbrew install perl-5.26.0 $ perlbrew switch perl-5.26.0 Use a specific perl version in your current shell, run: $ perlbrew use perl-5.26.1 Cpanm Is a lightweigth CPAN client, which facilitates installing CPAN perl modules. It is a good idea to install it together with Perlbrew to always make them available across each your perlbrew perl installations i.e the CPANM library will change with the perlbrew switch command. Installing CPANM with perlbrew is done by this command: $ perlbrew install-cpanm Perlbrew makes it easy to create and switch between multiple libraries of CPAN modules. You can even have several libraries for the same distribution of perl. Here are some examples: ## To create a library named Basil to perl version 5.26.0 $ perlbrew lib create perl-5.26.0@Basil ## To switch library for that perl distribution permanently $ perlbrew switch perl-5.26.0@Basil ## If you have another library that you want to use for your current session $ perlbrew use perl-5.26.0@Manuel ## To delete your Manuel library $ perlbrew lib delete perl-5.26.0@Manuel ## You can view your perl distributions and libraries by running: $ perlbrew list If you need to reinstall a specific version of a cpanm library, run: $ cpanm --reinstall [your_cpanm_lib]@[version] More information on perlbrew is available on metacpan .","title":"How do I manage perl and cpanm?"},{"location":"perl/installation/perlbrew/#perlbrew","text":"Perlbrew is a tool to manage multiple perl installations. Allowing for testing your production code against different perl versions, while leaving the vendor perl alone. You can even run your programs against all installations of perl.","title":"Perlbrew"},{"location":"perl/installation/perlbrew/#perl","text":"Install a specific perl version and use it as default. $ perlbrew install perl-5.26.0 $ perlbrew switch perl-5.26.0 Use a specific perl version in your current shell, run: $ perlbrew use perl-5.26.1","title":"Perl"},{"location":"perl/installation/perlbrew/#cpanm","text":"Is a lightweigth CPAN client, which facilitates installing CPAN perl modules. It is a good idea to install it together with Perlbrew to always make them available across each your perlbrew perl installations i.e the CPANM library will change with the perlbrew switch command. Installing CPANM with perlbrew is done by this command: $ perlbrew install-cpanm Perlbrew makes it easy to create and switch between multiple libraries of CPAN modules. You can even have several libraries for the same distribution of perl. Here are some examples: ## To create a library named Basil to perl version 5.26.0 $ perlbrew lib create perl-5.26.0@Basil ## To switch library for that perl distribution permanently $ perlbrew switch perl-5.26.0@Basil ## If you have another library that you want to use for your current session $ perlbrew use perl-5.26.0@Manuel ## To delete your Manuel library $ perlbrew lib delete perl-5.26.0@Manuel ## You can view your perl distributions and libraries by running: $ perlbrew list If you need to reinstall a specific version of a cpanm library, run: $ cpanm --reinstall [your_cpanm_lib]@[version] More information on perlbrew is available on metacpan .","title":"Cpanm"},{"location":"publish/envs/","text":"Stage What is stage? The environment in which we test our proposed solution before we push it to production is called stage. It should as closely as possible resemble production in terms of software dependencies, configuration, hardware, and data. A staging environment can be destroyed and you cannot trust that the a previously set up stage is still fully reflecting production. Meaning one needs to be able to quickly setup stage again. No development should happen in stage. Locations All scripts are found in ~/servers/resources on both rasta and clinical-db. All staging websites are found on clinical-db in directory ~/STAGE/ . All staging names, both packages and databases, have been named with -stage appended to the package name, e.g. trailblazer-stage. How to activate stage Rasta and clinical-db cd ~/servers/resources . activate-stage.sh How to update stage Each software package, or better, each component of a software package, be it frontend or cli, should have an update-component.sh script.","title":"Stage"},{"location":"publish/envs/#stage","text":"","title":"Stage"},{"location":"publish/envs/#what-is-stage","text":"The environment in which we test our proposed solution before we push it to production is called stage. It should as closely as possible resemble production in terms of software dependencies, configuration, hardware, and data. A staging environment can be destroyed and you cannot trust that the a previously set up stage is still fully reflecting production. Meaning one needs to be able to quickly setup stage again. No development should happen in stage.","title":"What is stage?"},{"location":"publish/envs/#locations","text":"All scripts are found in ~/servers/resources on both rasta and clinical-db. All staging websites are found on clinical-db in directory ~/STAGE/ . All staging names, both packages and databases, have been named with -stage appended to the package name, e.g. trailblazer-stage.","title":"Locations"},{"location":"publish/envs/#how-to-activate-stage","text":"Rasta and clinical-db cd ~/servers/resources . activate-stage.sh","title":"How to activate stage"},{"location":"publish/envs/#how-to-update-stage","text":"Each software package, or better, each component of a software package, be it frontend or cli, should have an update-component.sh script.","title":"How to update stage"},{"location":"publish/prod/","text":"How to install your tool in production? Your code is ready for deployment. Well done bucko! Now what do I do? The steps below may vary according to your tool. The gist of installing your tool to production will be along following lines: (all scripts are in ~/servers/resources ) Create a PR Get the PR reviewed and approved ... but don't merge yet Get someone to review your code Is the code pythonic? Where is the unit test? Did any signatures of existing functions or methods change? Test the PR on stage by deploying your branch Do this with your reviewer Announce on TTNs daily standup Delete current stage Clone prod to stage: condacopy-prod-to-stage.sh Switch to stage: source activate-stage.sh Install branch into stage with the tool's update script: update-tool-stage.sh If it passes, merge the PR into master Delete the branch Bumpversion on master Add change to change log Test the installation of the new master on stage Only test if installation succeeds If it passes, deploy on prod! Announce on stand up Take backup of prod: e.g. savetheconda prod170926 Install with the tool's update script: update-tool-prod.sh","title":"How to install your tool in production?"},{"location":"publish/prod/#how-to-install-your-tool-in-production","text":"Your code is ready for deployment. Well done bucko! Now what do I do? The steps below may vary according to your tool. The gist of installing your tool to production will be along following lines: (all scripts are in ~/servers/resources ) Create a PR Get the PR reviewed and approved ... but don't merge yet Get someone to review your code Is the code pythonic? Where is the unit test? Did any signatures of existing functions or methods change? Test the PR on stage by deploying your branch Do this with your reviewer Announce on TTNs daily standup Delete current stage Clone prod to stage: condacopy-prod-to-stage.sh Switch to stage: source activate-stage.sh Install branch into stage with the tool's update script: update-tool-stage.sh If it passes, merge the PR into master Delete the branch Bumpversion on master Add change to change log Test the installation of the new master on stage Only test if installation succeeds If it passes, deploy on prod! Announce on stand up Take backup of prod: e.g. savetheconda prod170926 Install with the tool's update script: update-tool-prod.sh","title":"How to install your tool in production?"},{"location":"publish/update-scripts/","text":"How to easily update your tool? You will want to create two scripts, at least: one to update your tool on stage one to update your tool on prod Yes, I just wrote that out in full. It's that important! Stage CLI update script How does such a script look like. Well, let's have a look at one that closest resembles a template, update-trailblazer-stage.sh #!/usr/bin/env bash # exit on error set -e # make sure we run as hiseq.clinical sh ./assert_user.sh hiseq.clinical # make sure we run on rasta. Leave this assert out if this script can be run on all servers. sh ./assert_host.sh rastapopoulos.scilifelab.se # One optional argument BRANCH=${1} # make sure we can have access to aliases shopt -s expand_aliases # make sure we are closely resembling production source ~/.bashrc # go to stage. Yes, this conda env should already exist source activate stage # install with latest changes on \"master\" pip install -U git+https://github.com/Clinical-Genomics/trailblazer@$BRANCH You run it: cd ~/servers/resources sh update-trailblazer-stage.sh Or you update stage to a specific branch: cd ~/servers/resources sh update-trailblazer-stage.sh beta Web update script For web frontend, one can add everything you need to update a staging env: #!/usr/bin/env bash # exit on error set -e # make sure we run as hiseq.clinical sh ./assert_user.sh hiseq.clinical # make sure we run on clinical-db sh ./assert_host.sh clinical-db.scilifelab.se # One optional argument, prefilled BRANCH=${1-master} # make sure we can have access to aliases shopt -s expand_aliases # make sure we are closely resembling production source ~/.bashrc # go to stage. Yes, this conda env should already exist source activate stage # trailblazer-stage is already installed in a predefined location. The repo should already be cloned cd ~/STAGE/trailblazer # Update! git fetch git checkout $BRANCH git pull # install and update pip dependencies pip install -U --editable . # restart the Flask service supervisorctl restart trailblazer-stage supervisorctl restart trailblazer-api-stage # Drop you back to the initial directory cd - Naming of scripts The suggestion is to name your script: update-$component-stage.sh with $component the name of your component, be it trailblazer for the cli or trailblazer-ui for the web. What about config files? Premade config files for all packages used in production and stage have been made in the config dir of the servers repo. This can be found on rasta and clinical-db in the same location: cd ~/servers/config All config files point to the stage location of the package, if any, to lims-stage, and to the staging databases. What about databases? MySQL All databases are located on clinical-db. To make sure you have the latest uncorrupted data, you can make a copy of one or all databases like this: On clinical-db, copying all databases: cd ~/servers/resources bash dbcopy-prod-to-stage.sh On clinical-db, copy one database: cd ~/servers/resources bash dbcopy-prod-to-stage.sh trailblazer The credentials to the stage databases have already been set in the stage configs. Mongo All mongodbs are located on clinical-db. To copy both scout and loqusdb, you can issue: cd ~/servers/resources bash dbcopy-mongoprod-to-stage.sh This will reinstate a backuped version of scout (dated: 2018-05-18) and a fresh copy of loqusdb. Please be aware that this process takes several days to complete! The mongo-stage server is not running by default. To start the mongo-stage run: cd ~/servers/resources bash start-mongo-stage.sh & So, what is beta? The beta environment, for now, is not an environment at all. It is a bunch of branches in git for cg and trailblazer that are affected by the MIP6/Scout4 update. To update stage to use those branches, run Rasta: bash update-beta.sh","title":"How make updating your tool easy?"},{"location":"publish/update-scripts/#how-to-easily-update-your-tool","text":"You will want to create two scripts, at least: one to update your tool on stage one to update your tool on prod Yes, I just wrote that out in full. It's that important!","title":"How to easily update your tool?"},{"location":"publish/update-scripts/#stage-cli-update-script","text":"How does such a script look like. Well, let's have a look at one that closest resembles a template, update-trailblazer-stage.sh #!/usr/bin/env bash # exit on error set -e # make sure we run as hiseq.clinical sh ./assert_user.sh hiseq.clinical # make sure we run on rasta. Leave this assert out if this script can be run on all servers. sh ./assert_host.sh rastapopoulos.scilifelab.se # One optional argument BRANCH=${1} # make sure we can have access to aliases shopt -s expand_aliases # make sure we are closely resembling production source ~/.bashrc # go to stage. Yes, this conda env should already exist source activate stage # install with latest changes on \"master\" pip install -U git+https://github.com/Clinical-Genomics/trailblazer@$BRANCH You run it: cd ~/servers/resources sh update-trailblazer-stage.sh Or you update stage to a specific branch: cd ~/servers/resources sh update-trailblazer-stage.sh beta","title":"Stage CLI update script"},{"location":"publish/update-scripts/#web-update-script","text":"For web frontend, one can add everything you need to update a staging env: #!/usr/bin/env bash # exit on error set -e # make sure we run as hiseq.clinical sh ./assert_user.sh hiseq.clinical # make sure we run on clinical-db sh ./assert_host.sh clinical-db.scilifelab.se # One optional argument, prefilled BRANCH=${1-master} # make sure we can have access to aliases shopt -s expand_aliases # make sure we are closely resembling production source ~/.bashrc # go to stage. Yes, this conda env should already exist source activate stage # trailblazer-stage is already installed in a predefined location. The repo should already be cloned cd ~/STAGE/trailblazer # Update! git fetch git checkout $BRANCH git pull # install and update pip dependencies pip install -U --editable . # restart the Flask service supervisorctl restart trailblazer-stage supervisorctl restart trailblazer-api-stage # Drop you back to the initial directory cd -","title":"Web update script"},{"location":"publish/update-scripts/#naming-of-scripts","text":"The suggestion is to name your script: update-$component-stage.sh with $component the name of your component, be it trailblazer for the cli or trailblazer-ui for the web.","title":"Naming of scripts"},{"location":"publish/update-scripts/#what-about-config-files","text":"Premade config files for all packages used in production and stage have been made in the config dir of the servers repo. This can be found on rasta and clinical-db in the same location: cd ~/servers/config All config files point to the stage location of the package, if any, to lims-stage, and to the staging databases.","title":"What about config files?"},{"location":"publish/update-scripts/#what-about-databases","text":"","title":"What about databases?"},{"location":"publish/update-scripts/#mysql","text":"All databases are located on clinical-db. To make sure you have the latest uncorrupted data, you can make a copy of one or all databases like this: On clinical-db, copying all databases: cd ~/servers/resources bash dbcopy-prod-to-stage.sh On clinical-db, copy one database: cd ~/servers/resources bash dbcopy-prod-to-stage.sh trailblazer The credentials to the stage databases have already been set in the stage configs.","title":"MySQL"},{"location":"publish/update-scripts/#mongo","text":"All mongodbs are located on clinical-db. To copy both scout and loqusdb, you can issue: cd ~/servers/resources bash dbcopy-mongoprod-to-stage.sh This will reinstate a backuped version of scout (dated: 2018-05-18) and a fresh copy of loqusdb. Please be aware that this process takes several days to complete! The mongo-stage server is not running by default. To start the mongo-stage run: cd ~/servers/resources bash start-mongo-stage.sh &","title":"Mongo"},{"location":"publish/update-scripts/#so-what-is-beta","text":"The beta environment, for now, is not an environment at all. It is a bunch of branches in git for cg and trailblazer that are affected by the MIP6/Scout4 update. To update stage to use those branches, run Rasta: bash update-beta.sh","title":"So, what is beta?"},{"location":"python/","text":"Python Python is a popular and easy-to-learn interpreted programming language. To get a feel for the language, take a look at this comprehensive reference . With the surge in adoption of the most recent version of Python we are encouraging all new code to support Python 3 first and foremost . Separate topics Testing Logging Packaging Style guide We generally follow PEP8. Written code should pass linting using PyLint . Ignored rules include: E501: \"wrap lines at 80 characters\" => we use a 100 character limit Linting code A linter is a great way to ensure you are writing code of good quality; following best practices and without avoidable errors relating to missing imports and misspelled variable names. We recommend PyLint for working with Python code. You can set it up to run in the background when you edit code. Documenting code We try to make comments and document what we do as much as possible, rather more than less. At least make proper docstrings that explains the logic. The reasoning should be that if I read the docstring I should not have to look at the code. Try to document the following: Module documentation: At the top of the file document in broad sentences Docstring: What is the purpose of class/function. Input/Output Inline comments: Why this if? Why this variable etc.. We try to follow googles docstring convention Packaging Packaging Python code is a known pain point that many developers struggle with. There are some moves to standardize the experience but for now a great place to start would be to follow this guide . It will give you an idea of the minimal structure you need and beyond. We also have our own guide to packaging here For detailed information or if you need to look up specific options there a very detailed resource available as well. We should be using Pipfile, pipenv. Uploading a package to Pypi In order to be able to install a Python package directly from the pip command, or from the requirements file of another software package, it is necessary to upload the package to PyPI . A basic tutorial on how to do that is available here . Conda setup See conda . Awesome Python A curated list of awesome Python tools and libraries! Command line interface Click : composable and Flask-like CLI framework Halo : beautiful terminal spinners Testing Py.test Py.test Coverage : easily integrate test coverage with Py.test Py.test Flask : easily integrate Py.test and Flask All plugin-ins : list of all Py.test plugins! Suggested command: bash py.test --cov-report html --cov \"$(basename \"$PWD\")\" --verbose --color=yes tests/ Logging Coloredlogs : super simple setup for colorized logging","title":"Python"},{"location":"python/#python","text":"Python is a popular and easy-to-learn interpreted programming language. To get a feel for the language, take a look at this comprehensive reference . With the surge in adoption of the most recent version of Python we are encouraging all new code to support Python 3 first and foremost .","title":"Python"},{"location":"python/#separate-topics","text":"Testing Logging Packaging","title":"Separate topics"},{"location":"python/#style-guide","text":"We generally follow PEP8. Written code should pass linting using PyLint . Ignored rules include: E501: \"wrap lines at 80 characters\" => we use a 100 character limit","title":"Style guide"},{"location":"python/#linting-code","text":"A linter is a great way to ensure you are writing code of good quality; following best practices and without avoidable errors relating to missing imports and misspelled variable names. We recommend PyLint for working with Python code. You can set it up to run in the background when you edit code.","title":"Linting code"},{"location":"python/#documenting-code","text":"We try to make comments and document what we do as much as possible, rather more than less. At least make proper docstrings that explains the logic. The reasoning should be that if I read the docstring I should not have to look at the code. Try to document the following: Module documentation: At the top of the file document in broad sentences Docstring: What is the purpose of class/function. Input/Output Inline comments: Why this if? Why this variable etc.. We try to follow googles docstring convention","title":"Documenting code"},{"location":"python/#packaging","text":"Packaging Python code is a known pain point that many developers struggle with. There are some moves to standardize the experience but for now a great place to start would be to follow this guide . It will give you an idea of the minimal structure you need and beyond. We also have our own guide to packaging here For detailed information or if you need to look up specific options there a very detailed resource available as well. We should be using Pipfile, pipenv.","title":"Packaging"},{"location":"python/#uploading-a-package-to-pypi","text":"In order to be able to install a Python package directly from the pip command, or from the requirements file of another software package, it is necessary to upload the package to PyPI . A basic tutorial on how to do that is available here .","title":"Uploading a package to Pypi"},{"location":"python/#conda-setup","text":"See conda .","title":"Conda setup"},{"location":"python/#awesome-python","text":"A curated list of awesome Python tools and libraries! Command line interface Click : composable and Flask-like CLI framework Halo : beautiful terminal spinners Testing Py.test Py.test Coverage : easily integrate test coverage with Py.test Py.test Flask : easily integrate Py.test and Flask All plugin-ins : list of all Py.test plugins! Suggested command: bash py.test --cov-report html --cov \"$(basename \"$PWD\")\" --verbose --color=yes tests/ Logging Coloredlogs : super simple setup for colorized logging","title":"Awesome Python"},{"location":"python/conventions/","text":"Conventions Code is read much more often than it is written. - A Wise Pythonista PEP8 and \ud83d\udc8e PEP20 ( import this ) should be considered as the starting points. Unless specified otherwise, we follow the guidelines laid out there. For a brief overview, read the Python Guide 's entry on the subject. The official guide has some very nice tips on code layout . This can be a nice reference when you are unsure about how to e.g. style your hanging indents. Futhermore, we use Python 3.6+. We encourage the use of type annotations (even if it doesn't cover 100% of your functions) and the new format strings: name = 'Paul Anderson' string = f\"Hello {name}!\" Indentation Use 4 spaces to indent code. Never use hard tabs. Spaces are preferred over tabs for the following reason: spaces are spaces on every editor on every operating system. Tabs can be configured to act as 2, 4, 8 \"spaces\" and this can make code unreadable when being shared. Maximum line length: 100 chars Limit all lines to a maximum of 100 characters. Break down the line if it exceeds the maximum length. For example: # Python automatically concatenates consecutive strings a_long_string = (\"I am a very long string that can be split across\" \"multiple lines by automatic string concatenation.\") # multi conditional if-statements can be split into multiple statements # notice how the code reads a lot like regular English! first_condition = 'genius' in 'Paul Thomas Anderson' second_condition = 'genius' in 'Daniel Day-Lewis' if first_condition and second_condition: print('And the Oscar goes to... There Will Be Blood!') Imports I like to split up imports into three separate groups: 1. standard library imports, 2. third party imports, 3. intra-package imports. import os import re import numpy as np from path import path from .utils import read_config Variable names PEP8 recommends to use short variable names. Achieving that can take some coding hygiene and some experience. Get more advice in our variable hygiene section . Folders/packages A common folder structure for a Python projects looks like: myPackage \u251c\u2500\u2500 __init__.py \u251c\u2500\u2500 exc.py <-- custom exceptions \u251c\u2500\u2500 cli/ \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u251c\u2500\u2500 base.py <-- read app config, set up logging, import subcommands \u2502 \u2514\u2500\u2500 subcommand.py \u251c\u2500\u2500 server/ \u2502 \u251c\u2500\u2500 templates/ \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u2514\u2500\u2500 app.py \u2514\u2500\u2500 store/ \u251c\u2500\u2500 __init__.py \u251c\u2500\u2500 api.py \u2514\u2500\u2500 models.py <-- database models, DB schema definitions The important thing to note is how functionality is split up across the project. It's common that the same or similar functionality is available through many interfaces. For example it might be possible to list all recently added records in a database: on the command line printed to the console over HTTP through a JSON API programmatically through accessing an API class The cli package contains all command line related code and nothing more. Logic such as how to handle and report error messages to the user goes here. Avoid putting logic in CLI modules that directly handle things like manually filtering database queries. More generally think DRY - avoid doing the same thing manually in both CLI and server code. Always go through your Python API to interact with e.g. the database. Command line interface (CLI) We highly recommend implementing the CLI using Click . It has a very nice Pythonic interface for defining your interface and comes with helpers for reading input, printing colorful output, etc. Logging We recommend using Coloredlogs which makes it super easy to enable logging for your package: import coloredlogs coloredlogs.install(level=log_level) Read more in the logging section Server If it makes sense, a general app exposes a JSON API over HTTP(S). This API can then be consumed by a client side web app. Sometimes it makes sense to provide a the web interface as part of the Python package and other times it will be consumed by the central web portal interface. We write our web servers in Flask which shares many similarities with Click . To connect to the store/backend we use Flask-Alchy . We generally configure servers using environment variables which is a flexible and well supported option. Store We start out with a SQL backend unless our needs require something different. We run all production SQL databases in MySQL. For development and testing it's often nice to work against a SQLite copy. For that reason and to make database interactions more pythonic, we make heavy use of the SQLAlchemy-wrapper, Alchy . Note that Alchy has been deprecated in favor of SQLService, however, we have not been able to successfully set up the latter package with Flask to work with the request context which is why we stay with Alchy for now. Store API Alchy shares the database connection with associated model classes. This allows you to perform queries by simply importing them like: # ... connect to database from trailblazer.store import models for record in models.Sample.query: print(record) However, this feels a little magic and it's not so clear how the model exactly got access to that database connection. Therefore, we advocate explicitly passing around an API class holding the database connection. from . import models class DatabaseAPI(): Sample = models.Sample def __init__(self, uri): self.connect(uri) def samples(self): return self.Sample.query Exceptions We recommend that you base all your custom exceptions on a app-specific exception class. This way, 3-party packages that depend on it easily catch all exceptions from your package if they so choose. class TrailblazerError(Exception): def __init__(self, message): self.message = message class MissingFileError(TrailblazerError): pass Helpful tools We encourage using a linter to continuously check the syntax of your code. The Python community maintains a number of linters such as pep8, and PyFlakes . If your repo has been set up with continous integration, with for example Travis, you can easily make linting part of the build process together with Git-Lint . This will give you step wise improvements by automatic linting of those parts of the code that are new or changed. Another resource that can greatly ease collaboration is EditorConfig . It's an effort to provide a cross editor configuration format and in placed in your project and committed to source control. You also need to install a plugin for your favorite editor, e.g. Sublime .","title":"conventions"},{"location":"python/conventions/#conventions","text":"Code is read much more often than it is written. - A Wise Pythonista PEP8 and \ud83d\udc8e PEP20 ( import this ) should be considered as the starting points. Unless specified otherwise, we follow the guidelines laid out there. For a brief overview, read the Python Guide 's entry on the subject. The official guide has some very nice tips on code layout . This can be a nice reference when you are unsure about how to e.g. style your hanging indents. Futhermore, we use Python 3.6+. We encourage the use of type annotations (even if it doesn't cover 100% of your functions) and the new format strings: name = 'Paul Anderson' string = f\"Hello {name}!\"","title":"Conventions"},{"location":"python/conventions/#indentation","text":"Use 4 spaces to indent code. Never use hard tabs. Spaces are preferred over tabs for the following reason: spaces are spaces on every editor on every operating system. Tabs can be configured to act as 2, 4, 8 \"spaces\" and this can make code unreadable when being shared.","title":"Indentation"},{"location":"python/conventions/#maximum-line-length-100-chars","text":"Limit all lines to a maximum of 100 characters. Break down the line if it exceeds the maximum length. For example: # Python automatically concatenates consecutive strings a_long_string = (\"I am a very long string that can be split across\" \"multiple lines by automatic string concatenation.\") # multi conditional if-statements can be split into multiple statements # notice how the code reads a lot like regular English! first_condition = 'genius' in 'Paul Thomas Anderson' second_condition = 'genius' in 'Daniel Day-Lewis' if first_condition and second_condition: print('And the Oscar goes to... There Will Be Blood!')","title":"Maximum line length: 100 chars"},{"location":"python/conventions/#imports","text":"I like to split up imports into three separate groups: 1. standard library imports, 2. third party imports, 3. intra-package imports. import os import re import numpy as np from path import path from .utils import read_config","title":"Imports"},{"location":"python/conventions/#variable-names","text":"PEP8 recommends to use short variable names. Achieving that can take some coding hygiene and some experience. Get more advice in our variable hygiene section .","title":"Variable names"},{"location":"python/conventions/#folderspackages","text":"A common folder structure for a Python projects looks like: myPackage \u251c\u2500\u2500 __init__.py \u251c\u2500\u2500 exc.py <-- custom exceptions \u251c\u2500\u2500 cli/ \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u251c\u2500\u2500 base.py <-- read app config, set up logging, import subcommands \u2502 \u2514\u2500\u2500 subcommand.py \u251c\u2500\u2500 server/ \u2502 \u251c\u2500\u2500 templates/ \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u2514\u2500\u2500 app.py \u2514\u2500\u2500 store/ \u251c\u2500\u2500 __init__.py \u251c\u2500\u2500 api.py \u2514\u2500\u2500 models.py <-- database models, DB schema definitions The important thing to note is how functionality is split up across the project. It's common that the same or similar functionality is available through many interfaces. For example it might be possible to list all recently added records in a database: on the command line printed to the console over HTTP through a JSON API programmatically through accessing an API class The cli package contains all command line related code and nothing more. Logic such as how to handle and report error messages to the user goes here. Avoid putting logic in CLI modules that directly handle things like manually filtering database queries. More generally think DRY - avoid doing the same thing manually in both CLI and server code. Always go through your Python API to interact with e.g. the database.","title":"Folders/packages"},{"location":"python/conventions/#command-line-interface-cli","text":"We highly recommend implementing the CLI using Click . It has a very nice Pythonic interface for defining your interface and comes with helpers for reading input, printing colorful output, etc.","title":"Command line interface (CLI)"},{"location":"python/conventions/#logging","text":"We recommend using Coloredlogs which makes it super easy to enable logging for your package: import coloredlogs coloredlogs.install(level=log_level) Read more in the logging section","title":"Logging"},{"location":"python/conventions/#server","text":"If it makes sense, a general app exposes a JSON API over HTTP(S). This API can then be consumed by a client side web app. Sometimes it makes sense to provide a the web interface as part of the Python package and other times it will be consumed by the central web portal interface. We write our web servers in Flask which shares many similarities with Click . To connect to the store/backend we use Flask-Alchy . We generally configure servers using environment variables which is a flexible and well supported option.","title":"Server"},{"location":"python/conventions/#store","text":"We start out with a SQL backend unless our needs require something different. We run all production SQL databases in MySQL. For development and testing it's often nice to work against a SQLite copy. For that reason and to make database interactions more pythonic, we make heavy use of the SQLAlchemy-wrapper, Alchy . Note that Alchy has been deprecated in favor of SQLService, however, we have not been able to successfully set up the latter package with Flask to work with the request context which is why we stay with Alchy for now.","title":"Store"},{"location":"python/conventions/#store-api","text":"Alchy shares the database connection with associated model classes. This allows you to perform queries by simply importing them like: # ... connect to database from trailblazer.store import models for record in models.Sample.query: print(record) However, this feels a little magic and it's not so clear how the model exactly got access to that database connection. Therefore, we advocate explicitly passing around an API class holding the database connection. from . import models class DatabaseAPI(): Sample = models.Sample def __init__(self, uri): self.connect(uri) def samples(self): return self.Sample.query","title":"Store API"},{"location":"python/conventions/#exceptions","text":"We recommend that you base all your custom exceptions on a app-specific exception class. This way, 3-party packages that depend on it easily catch all exceptions from your package if they so choose. class TrailblazerError(Exception): def __init__(self, message): self.message = message class MissingFileError(TrailblazerError): pass","title":"Exceptions"},{"location":"python/conventions/#helpful-tools","text":"We encourage using a linter to continuously check the syntax of your code. The Python community maintains a number of linters such as pep8, and PyFlakes . If your repo has been set up with continous integration, with for example Travis, you can easily make linting part of the build process together with Git-Lint . This will give you step wise improvements by automatic linting of those parts of the code that are new or changed. Another resource that can greatly ease collaboration is EditorConfig . It's an effort to provide a cross editor configuration format and in placed in your project and committed to source control. You also need to install a plugin for your favorite editor, e.g. Sublime .","title":"Helpful tools"},{"location":"python/logging/","text":"Logging Logging is the logical next step when you realize the limitations of printing. Python includes a very useful logging module in the standard library so it's rather easy to get started. First make sure you know the basics . Now when you want to log some progress or program state in a module: import logging LOG = logging.getLogger(__name__) def foo(bar): \"\"\"My fancy function.\"\"\" LOG.info(\"incrementing the input: %s\", bar) return bar + 1 The not-so-intuitive part is that you need to configure the logging module to see any output from these calls. Luckily we can reduce the setup to a simple function call most of the time using the coloredlogs package. During e.g. your CLI initialization include: import click import coloredlogs import logging LOG = logging.getLogger(__name__) @click.command() @click.option('-l', '--log-level', default='INFO', help='Log message level to display') def cli(log_level): \"\"\"Base command line entry point.\"\"\" coloredlogs.install(level=log_level) LOG.info(\"Running cli\") # ... more code here","title":"logging"},{"location":"python/logging/#logging","text":"Logging is the logical next step when you realize the limitations of printing. Python includes a very useful logging module in the standard library so it's rather easy to get started. First make sure you know the basics . Now when you want to log some progress or program state in a module: import logging LOG = logging.getLogger(__name__) def foo(bar): \"\"\"My fancy function.\"\"\" LOG.info(\"incrementing the input: %s\", bar) return bar + 1 The not-so-intuitive part is that you need to configure the logging module to see any output from these calls. Luckily we can reduce the setup to a simple function call most of the time using the coloredlogs package. During e.g. your CLI initialization include: import click import coloredlogs import logging LOG = logging.getLogger(__name__) @click.command() @click.option('-l', '--log-level', default='INFO', help='Log message level to display') def cli(log_level): \"\"\"Base command line entry point.\"\"\" coloredlogs.install(level=log_level) LOG.info(\"Running cli\") # ... more code here","title":"Logging"},{"location":"python/testing/","text":"Python testing In general we use pytest for testing Python code. Here are a few guides to get you started writing tests: part 1 , part 2 , and part 3 .","title":"How to write unit test?"},{"location":"python/testing/#python-testing","text":"In general we use pytest for testing Python code. Here are a few guides to get you started writing tests: part 1 , part 2 , and part 3 .","title":"Python testing"},{"location":"python/uploading_to_pypi/","text":"Uploading a Python package to PyPI First thing to do is registering a user account on both TestPyPI and PyPI . TestPyPI is a test site where you can verify that the software package is ok before uploading it to the official PyPI repository. To handle these accounts and use the same settings for every submission to PyPI you'll do in the future, you could create a .pypirc file in your home directory. This file should contain the following lines: [distutils] index-servers = pypi testpypi [pypi] username = your pypi username password = your pypi password [testpypi] repository = https://test.pypi.org/legacy/ username = your testpypi username password = your testpypi password Otherwise you could always specify repository name, user and password every time you upload a package. This latter would be the safest option because this way you'd be avoiding storing passwords in plaintext in your home directory. After packaging your software as described here , you are ready to upload it to the Python Package Index (PyPI), a repository specific for Python software. Be sure that all files that have to be included in the package along with with the code are listed in the manifest file (MANIFEST.in). It is necessary for instance to include files such as requirements.txt and LICENSE.txt. Please note that markdown-formatted pages are rendered poorly on the PyPI web pages, so you might want to convert the README.md file to reStructuredText format (.rst). This can be done by using Pandoc . Next step of preparing your package for PyPI would be fixing the setup.py file to include a download_url link to a tarball url. According to the official documentation available on packaging.python.org, packages version should follow the flexible public version scheme specified in PEP 440 . Some examples: 1.2.0.dev1 # Development release 1.2.0a1 # Alpha Release 1.2.0b1 # Beta Release 1.2.0rc1 # Release Candidate 1.2.0 # Final Release 1.2.0.post1 # Post Release 15.10 # Date based release 23 # Serial release is link should be taking into account the version number of the software. If for instance the version of your software is 1.3.0, the setup.py file should be modified by adding this line: setup( ... download_url = 'https://github.com/Clinical-Genomics/Name_of_your_package/tarball/1.3.0', ... ) Note that this url doesn't exist yet and will be created at a later stage. After committing this change and fixing this version of the repository you should add a git tag to the package. This commands adds basically a version number and release to the project and will be showed under the \"releases\" page of the git project. The syntax to add a tag is the following: git tag 1.3.0 -m \"Added a 1.3.0 version tag for PyPI\" git push --tags origin master Next step consists in creating a distribution release of the package. If your package has different requirements under different platforms (Linux, Mac or Windows) you should create platform wheels to taking care of this issue. Here's a documentation page explaining how to create wheel files . If you don't need to care about wheels instead you can just create a distribution using the setup file: python setup.py sdist After creating a distribution you can test the upload on test.pypi. To do the actual upload you'll need twine (pip install twine if it isn't installed already). Upload to test.pypi: twine upload --repository-url https://test.pypi.org/legacy/ dist/* Check now that your repository is on PyPI test, under \"your packages\". You could now attempt to install the package from pip. To do that type: pip install --index-url https://test.pypi.org/simple --extra-index-url https://pypi.org/simple name_of_package The \"--extra-index-url https://pypi.org/simple\" option would instruct pip to retrieve all dependencies for your package from pypi itself. If your package can be found on the test repository and everything is as you expect, it is safe to upload it on the PyPI repository: twine upload --repository-url https://upload.pypi.org/legacy/ dist/* Your package should be now available for installation over internet by using the command \"pip install your_package\".","title":"Uploading a Python package to PyPI"},{"location":"python/uploading_to_pypi/#uploading-a-python-package-to-pypi","text":"First thing to do is registering a user account on both TestPyPI and PyPI . TestPyPI is a test site where you can verify that the software package is ok before uploading it to the official PyPI repository. To handle these accounts and use the same settings for every submission to PyPI you'll do in the future, you could create a .pypirc file in your home directory. This file should contain the following lines: [distutils] index-servers = pypi testpypi [pypi] username = your pypi username password = your pypi password [testpypi] repository = https://test.pypi.org/legacy/ username = your testpypi username password = your testpypi password Otherwise you could always specify repository name, user and password every time you upload a package. This latter would be the safest option because this way you'd be avoiding storing passwords in plaintext in your home directory. After packaging your software as described here , you are ready to upload it to the Python Package Index (PyPI), a repository specific for Python software. Be sure that all files that have to be included in the package along with with the code are listed in the manifest file (MANIFEST.in). It is necessary for instance to include files such as requirements.txt and LICENSE.txt. Please note that markdown-formatted pages are rendered poorly on the PyPI web pages, so you might want to convert the README.md file to reStructuredText format (.rst). This can be done by using Pandoc . Next step of preparing your package for PyPI would be fixing the setup.py file to include a download_url link to a tarball url. According to the official documentation available on packaging.python.org, packages version should follow the flexible public version scheme specified in PEP 440 . Some examples: 1.2.0.dev1 # Development release 1.2.0a1 # Alpha Release 1.2.0b1 # Beta Release 1.2.0rc1 # Release Candidate 1.2.0 # Final Release 1.2.0.post1 # Post Release 15.10 # Date based release 23 # Serial release is link should be taking into account the version number of the software. If for instance the version of your software is 1.3.0, the setup.py file should be modified by adding this line: setup( ... download_url = 'https://github.com/Clinical-Genomics/Name_of_your_package/tarball/1.3.0', ... ) Note that this url doesn't exist yet and will be created at a later stage. After committing this change and fixing this version of the repository you should add a git tag to the package. This commands adds basically a version number and release to the project and will be showed under the \"releases\" page of the git project. The syntax to add a tag is the following: git tag 1.3.0 -m \"Added a 1.3.0 version tag for PyPI\" git push --tags origin master Next step consists in creating a distribution release of the package. If your package has different requirements under different platforms (Linux, Mac or Windows) you should create platform wheels to taking care of this issue. Here's a documentation page explaining how to create wheel files . If you don't need to care about wheels instead you can just create a distribution using the setup file: python setup.py sdist After creating a distribution you can test the upload on test.pypi. To do the actual upload you'll need twine (pip install twine if it isn't installed already). Upload to test.pypi: twine upload --repository-url https://test.pypi.org/legacy/ dist/* Check now that your repository is on PyPI test, under \"your packages\". You could now attempt to install the package from pip. To do that type: pip install --index-url https://test.pypi.org/simple --extra-index-url https://pypi.org/simple name_of_package The \"--extra-index-url https://pypi.org/simple\" option would instruct pip to retrieve all dependencies for your package from pypi itself. If your package can be found on the test repository and everything is as you expect, it is safe to upload it on the PyPI repository: twine upload --repository-url https://upload.pypi.org/legacy/ dist/* Your package should be now available for installation over internet by using the command \"pip install your_package\".","title":"Uploading a Python package to PyPI"},{"location":"python/variables/","text":"Variable hygiene PEP8 recommends short variable names, but achieving this requires good programming hygiene. Here are some advices to keep variable names short. Variable name are not full descriptors First, do not think of variable names as full descriptors of their content. Names should be clear mainly to allow to keep track of where they come from and only then can they give a bit about the content. # This is very descriptive, but we can infer that by looking at the expression students_with_grades_above_90 = filter(lambda s: s.grade > 90, students) # This is short and still allows the reader to infer what the value was good_students = filter(lambda s: s.grade > 90, students) Put details in comments Use comments and doc strings for description of what is going on, not variable names. # We feel we need a long variable description because the function is not documented def get_good_students(students): return filter(lambda s: s.grade > 90, students) students_with_grades_above_90 = get_good_students(students) # If the reader is not sure about what get_good_students returns, # their reflex should be to look at the function docstring def get_good_students(students): \"\"\"Return students with grade above 90\"\"\" return filter(lambda s: s.grade > 90, students) # You can optionally comment here that good_students are the ones with grade > 90 good_students = get_good_students(students) Too specific name might mean too specific code If you feel you need a very specific name for a function, it might be that the function is too specific itself. # Very long name because very specific behaviour def get_students_with_grade_above_90(students): return filter(lambda s: s.grade > 90, students) # Adding a level of abstraction shortens our method name here def get_students_above(grade, students): return filter(lambda s: s.grade > grade, students) # What we mean here is very clear and the code is reusable good_students = get_students_above(90, students) Keep short scopes for quick lookup Finally, encapsulate logic in shorter scopes. This way, you don't need to give that much detail in the variable name, as it can be looked up quickly a few lines above. A rule of thumb is to make your functions fit in your IDE without scrolling and encapsulate some logic in new function if you go beyond that. # line 6 names = ['John', 'Jane'] ... # Hundreds of lines of code # line 371 # Wait what was names again? if 'Kath' in names: ... Here I lost track of what names was and scrolling up will make me lose track of even more. This is better: # line 6 names = ['John', 'Jane'] # I encapsulate part of the logic in another function to keep scope short x = some_function() y = maybe_a_second_function(x) # line 13 # Sure I can lookup names, it is right above if 'Kath' in names: ... Spend time thinking about readability The last but not least is to devote time thinking about how you can make your code more readable. This is as important as the time you spend thinking about the logic and code optimization. The best code is the code that everyone can read, and thus everyone can improve. The above text was shamelessly copied from StackOverflow","title":"variables"},{"location":"python/variables/#variable-hygiene","text":"PEP8 recommends short variable names, but achieving this requires good programming hygiene. Here are some advices to keep variable names short.","title":"Variable hygiene"},{"location":"python/variables/#variable-name-are-not-full-descriptors","text":"First, do not think of variable names as full descriptors of their content. Names should be clear mainly to allow to keep track of where they come from and only then can they give a bit about the content. # This is very descriptive, but we can infer that by looking at the expression students_with_grades_above_90 = filter(lambda s: s.grade > 90, students) # This is short and still allows the reader to infer what the value was good_students = filter(lambda s: s.grade > 90, students)","title":"Variable name are not full descriptors"},{"location":"python/variables/#put-details-in-comments","text":"Use comments and doc strings for description of what is going on, not variable names. # We feel we need a long variable description because the function is not documented def get_good_students(students): return filter(lambda s: s.grade > 90, students) students_with_grades_above_90 = get_good_students(students) # If the reader is not sure about what get_good_students returns, # their reflex should be to look at the function docstring def get_good_students(students): \"\"\"Return students with grade above 90\"\"\" return filter(lambda s: s.grade > 90, students) # You can optionally comment here that good_students are the ones with grade > 90 good_students = get_good_students(students)","title":"Put details in comments"},{"location":"python/variables/#too-specific-name-might-mean-too-specific-code","text":"If you feel you need a very specific name for a function, it might be that the function is too specific itself. # Very long name because very specific behaviour def get_students_with_grade_above_90(students): return filter(lambda s: s.grade > 90, students) # Adding a level of abstraction shortens our method name here def get_students_above(grade, students): return filter(lambda s: s.grade > grade, students) # What we mean here is very clear and the code is reusable good_students = get_students_above(90, students)","title":"Too specific name might mean too specific code"},{"location":"python/variables/#keep-short-scopes-for-quick-lookup","text":"Finally, encapsulate logic in shorter scopes. This way, you don't need to give that much detail in the variable name, as it can be looked up quickly a few lines above. A rule of thumb is to make your functions fit in your IDE without scrolling and encapsulate some logic in new function if you go beyond that. # line 6 names = ['John', 'Jane'] ... # Hundreds of lines of code # line 371 # Wait what was names again? if 'Kath' in names: ... Here I lost track of what names was and scrolling up will make me lose track of even more. This is better: # line 6 names = ['John', 'Jane'] # I encapsulate part of the logic in another function to keep scope short x = some_function() y = maybe_a_second_function(x) # line 13 # Sure I can lookup names, it is right above if 'Kath' in names: ...","title":"Keep short scopes for quick lookup"},{"location":"python/variables/#spend-time-thinking-about-readability","text":"The last but not least is to devote time thinking about how you can make your code more readable. This is as important as the time you spend thinking about the logic and code optimization. The best code is the code that everyone can read, and thus everyone can improve. The above text was shamelessly copied from StackOverflow","title":"Spend time thinking about readability"},{"location":"python/count_variants/count_module/","text":"3. First module {#first_module} Ok so finally we can start to write some actual python code! In this section we will learn about file structure, imports and a little bit about command line interfaces. When we finnish this section we should have a small program that counts the number of variants in a vcf. We will try to keep the code separated inside our projekt as described here , we store the counting code in a submodule called utils and the cli code in a module called cli. So run: mkdir count_variants/cli count_variants/cli touch count_variants/cli/__init__.py touch count_variants/utils/__init__.py touch count_variants/cli/root.py touch count_variants/utils/count.py Ok a lot of files created here, the __init__.py files we know from before, they are there to tell that this is a module. count_variants/cli/root.py will include the command line interface to our program and count_variants/utils/count.py will include the code that counts vcf variants. 3.1 Writing the first function {#first_function} Open count_variants/utils/count.py and write the following def count_variants(vcf): \"\"\"Count the number of variants in a vcf file Args: vcf(iterable): An iterable with variants Returns: nr_variants(int): Number of variants in file \"\"\" nr_variants = 0 for variant in vcf: nr_variants += 1 return nr_variants The text within quotation marks is a docstring, EVERY function you write should have a docstring that at least explains what the intention of the function is and input and output. 3.2 Writing the CLI {#cli} Now open count_variants/cli/root.py and write: import logging import click import coloredlogs from cyvcf2 import VCF from count_variants.utils.count import count_variants LOG = logging.getLogger(__name__) @click.command() @click.argument('vcf', type=click.Path(exists=True)) def cli(vcf): \"\"\"Count the variants in a vcf\"\"\" coloredlogs.install(level='INFO') LOG.info(\"Reading vcf file: %s\", vcf) vcf_obj = VCF(vcf) nr_variants = count_variants(vcf_obj) click.echo(\"Nr variants in vcf: %s\" % nr_variants) It is a good convention to import standard library modules first, then third party modules and finally local imports. In this case we will go through the code line by line. logging is the standard library solution for logging in python. click is our prefered package to build command line interfaces. coloredlogs is a wrapper for logging that makes logging look good and super easy. cyvcf2 is a reliable and fast parser for VCF files count_variants.utils.count on this line we import the function that we wrote in the previous section LOG = logging.getLogger(__name__) here we create a logging object and name it LOG @click.command() is a decorator , you do not need to understand the concept right now. It basically give the following function some properties @click.argument() this is how an argument is defined in click Then comes the function definition, the docstring and some code that should be rather straight forward. 3.3 Missing modules The modules in standard library follows with the python distribution so there is no need to install those. As you may have noticed there are some modules used here that are missing in our requirements.txt file. It can be a bit hard to know what third party modules that will be used when starting a project. So lets add coloredlogs and cyvcf2 to requirements.txt , when you are done it should look like: $ cat requirements.txt click coloredlogs cyvcf2 If everything is done correct the folder structure should look like this now: count_variants/ \u251c\u2500\u2500 LICENSE \u251c\u2500\u2500 README.md \u251c\u2500\u2500 MANIFEST.in \u251c\u2500\u2500 .gitignore \u251c\u2500\u2500 setup.py \u251c\u2500\u2500 count_variants/ \u251c\u2500\u2500 __init__.py \u251c\u2500\u2500 __version__.py \u251c\u2500\u2500 cli/ \u251c\u2500\u2500 __init__.py \u251c\u2500\u2500 root.py \u251c\u2500\u2500 utils/ \u251c\u2500\u2500 __init__.py \u251c\u2500\u2500 count.py If you haven't done so already, stage all the files and commit them!","title":"3. First module {#first_module}"},{"location":"python/count_variants/count_module/#3-first-module-first_module","text":"Ok so finally we can start to write some actual python code! In this section we will learn about file structure, imports and a little bit about command line interfaces. When we finnish this section we should have a small program that counts the number of variants in a vcf. We will try to keep the code separated inside our projekt as described here , we store the counting code in a submodule called utils and the cli code in a module called cli. So run: mkdir count_variants/cli count_variants/cli touch count_variants/cli/__init__.py touch count_variants/utils/__init__.py touch count_variants/cli/root.py touch count_variants/utils/count.py Ok a lot of files created here, the __init__.py files we know from before, they are there to tell that this is a module. count_variants/cli/root.py will include the command line interface to our program and count_variants/utils/count.py will include the code that counts vcf variants.","title":"3. First module {#first_module}"},{"location":"python/count_variants/count_module/#31-writing-the-first-function-first_function","text":"Open count_variants/utils/count.py and write the following def count_variants(vcf): \"\"\"Count the number of variants in a vcf file Args: vcf(iterable): An iterable with variants Returns: nr_variants(int): Number of variants in file \"\"\" nr_variants = 0 for variant in vcf: nr_variants += 1 return nr_variants The text within quotation marks is a docstring, EVERY function you write should have a docstring that at least explains what the intention of the function is and input and output.","title":"3.1 Writing the first function {#first_function}"},{"location":"python/count_variants/count_module/#32-writing-the-cli-cli","text":"Now open count_variants/cli/root.py and write: import logging import click import coloredlogs from cyvcf2 import VCF from count_variants.utils.count import count_variants LOG = logging.getLogger(__name__) @click.command() @click.argument('vcf', type=click.Path(exists=True)) def cli(vcf): \"\"\"Count the variants in a vcf\"\"\" coloredlogs.install(level='INFO') LOG.info(\"Reading vcf file: %s\", vcf) vcf_obj = VCF(vcf) nr_variants = count_variants(vcf_obj) click.echo(\"Nr variants in vcf: %s\" % nr_variants) It is a good convention to import standard library modules first, then third party modules and finally local imports. In this case we will go through the code line by line. logging is the standard library solution for logging in python. click is our prefered package to build command line interfaces. coloredlogs is a wrapper for logging that makes logging look good and super easy. cyvcf2 is a reliable and fast parser for VCF files count_variants.utils.count on this line we import the function that we wrote in the previous section LOG = logging.getLogger(__name__) here we create a logging object and name it LOG @click.command() is a decorator , you do not need to understand the concept right now. It basically give the following function some properties @click.argument() this is how an argument is defined in click Then comes the function definition, the docstring and some code that should be rather straight forward.","title":"3.2 Writing the CLI {#cli}"},{"location":"python/count_variants/count_module/#33-missing-modules","text":"The modules in standard library follows with the python distribution so there is no need to install those. As you may have noticed there are some modules used here that are missing in our requirements.txt file. It can be a bit hard to know what third party modules that will be used when starting a project. So lets add coloredlogs and cyvcf2 to requirements.txt , when you are done it should look like: $ cat requirements.txt click coloredlogs cyvcf2 If everything is done correct the folder structure should look like this now: count_variants/ \u251c\u2500\u2500 LICENSE \u251c\u2500\u2500 README.md \u251c\u2500\u2500 MANIFEST.in \u251c\u2500\u2500 .gitignore \u251c\u2500\u2500 setup.py \u251c\u2500\u2500 count_variants/ \u251c\u2500\u2500 __init__.py \u251c\u2500\u2500 __version__.py \u251c\u2500\u2500 cli/ \u251c\u2500\u2500 __init__.py \u251c\u2500\u2500 root.py \u251c\u2500\u2500 utils/ \u251c\u2500\u2500 __init__.py \u251c\u2500\u2500 count.py If you haven't done so already, stage all the files and commit them!","title":"3.3 Missing modules"},{"location":"python/count_variants/installing/","text":"Installing So it is time to install our package and see that everything works, exciting! \ud83d\ude03 There is only one thing missing; tell setup.py how to use the command line interface that we wrote earlier. Open setup.py and look for entry_points section. Change it to look like: entry_points={ 'console_scripts': [ 'count_variants = count_variants.cli.root:cli' ], }, We are telling setup.py that if someone write count_variants on the command line run the function cli() in the file count_variants.cli.root . Now we are going to run the command that installs all dependencies and the package itself: pip install --editable . Watch the magic happen! \ud83d\udd2e When you are done you should be able to write: $ count_variants Usage: count_variants [OPTIONS] VCF Error: Missing argument \"vcf\". Great job!!! If you have a vcf file lying around you could try and feed that to the program and see if it works. In the following sections we will add some complexity, write tests and push the code to GitHub so others can see it.","title":"Installing"},{"location":"python/count_variants/installing/#installing","text":"So it is time to install our package and see that everything works, exciting! \ud83d\ude03 There is only one thing missing; tell setup.py how to use the command line interface that we wrote earlier. Open setup.py and look for entry_points section. Change it to look like: entry_points={ 'console_scripts': [ 'count_variants = count_variants.cli.root:cli' ], }, We are telling setup.py that if someone write count_variants on the command line run the function cli() in the file count_variants.cli.root . Now we are going to run the command that installs all dependencies and the package itself: pip install --editable . Watch the magic happen! \ud83d\udd2e When you are done you should be able to write: $ count_variants Usage: count_variants [OPTIONS] VCF Error: Missing argument \"vcf\". Great job!!! If you have a vcf file lying around you could try and feed that to the program and see if it works. In the following sections we will add some complexity, write tests and push the code to GitHub so others can see it.","title":"Installing"},{"location":"python/count_variants/overview/","text":"Creating a Python package To be able to reuse code it is important to package things we are doing in a way that makes them easy to install and operate. There are of course a number of ways to handle this and we will describe our prefered way to do it here. I will use an example as we go along; we are going to create a package called count_variants that is used to count variants with different selection criteria in a VCF file. In this example we will tie together many of the topics described in this guide, like testing , conventions , git , logging and perhaps some more. Overview 1. Start a structure 1.1 Basics 1.2 README 1.3 LICENSE etc 1.4 First commit 2. setup.py 3. First Module 3.1 first function 3.2 cli 3.3 missing modules 3. Installing next","title":"Creating a Python package"},{"location":"python/count_variants/overview/#creating-a-python-package","text":"To be able to reuse code it is important to package things we are doing in a way that makes them easy to install and operate. There are of course a number of ways to handle this and we will describe our prefered way to do it here. I will use an example as we go along; we are going to create a package called count_variants that is used to count variants with different selection criteria in a VCF file. In this example we will tie together many of the topics described in this guide, like testing , conventions , git , logging and perhaps some more.","title":"Creating a Python package"},{"location":"python/count_variants/overview/#overview","text":"1. Start a structure 1.1 Basics 1.2 README 1.3 LICENSE etc 1.4 First commit 2. setup.py 3. First Module 3.1 first function 3.2 cli 3.3 missing modules 3. Installing next","title":"Overview"},{"location":"python/count_variants/setup_py/","text":"2. setup.py Every python package contains a special file called setup.py in the root directory. This file describes some meta information about the package and how to create a distribution. There is a excellent guide for how to setup a package and create the setup.py here . Lets continue with our example following @kennethreitz guide, please read the guide first. Copy setup.py from the guide and open it, if you do it right it should look like: #!/usr/bin/env python # -*- coding: utf-8 -*- # Note: To use the 'upload' functionality of this file, you must: # $ pip install twine import io import os import sys from shutil import rmtree from setuptools import find_packages, setup, Command # Package meta-data. NAME = 'mypackage' DESCRIPTION = 'My short description for my project. ' URL = 'https://github.com/me/myproject' EMAIL = 'me@example.com' AUTHOR = 'Awesome Soul' # What packages are required for this module to be executed? REQUIRED = [ # 'requests', 'maya', 'records', ] # The rest you shouldn't have to touch too much :) # ------------------------------------------------ # Except, perhaps the License and Trove Classifiers! # If you do change the License, remember to change the Trove Classifier for that! here = os.path.abspath(os.path.dirname(__file__)) # Import the README and use it as the long-description. # Note: this will only work if 'README.rst' is present in your MANIFEST.in file! with io.open(os.path.join(here, 'README.rst'), encoding='utf-8') as f: long_description = '\\n' + f.read() # Load the package's __version__.py module as a dictionary. about = {} with open(os.path.join(here, NAME, '__version__.py')) as f: exec(f.read(), about) class UploadCommand(Command): \"\"\"Support setup.py upload.\"\"\" description = 'Build and publish the package.' user_options = [] @staticmethod def status(s): \"\"\"Prints things in bold.\"\"\" print('\\033[1m{0}\\033[0m'.format(s)) def initialize_options(self): pass def finalize_options(self): pass def run(self): try: self.status('Removing previous builds\u2026') rmtree(os.path.join(here, 'dist')) except OSError: pass self.status('Building Source and Wheel (universal) distribution\u2026') os.system('{0} setup.py sdist bdist_wheel --universal'.format(sys.executable)) self.status('Uploading the package to PyPi via Twine\u2026') os.system('twine upload dist/*') sys.exit() # Where the magic happens: setup( name=NAME, version=about['__version__'], description=DESCRIPTION, long_description=long_description, author=AUTHOR, author_email=EMAIL, url=URL, packages=find_packages(exclude=('tests',)), # If your package is a single module, use this instead of 'packages': # py_modules=['mypackage'], # entry_points={ # 'console_scripts': ['mycli=mymodule:cli'], # }, install_requires=REQUIRED, include_package_data=True, license='MIT', classifiers=[ # Trove classifiers # Full list: https://pypi.python.org/pypi?%3Aaction=list_classifiers 'License :: OSI Approved :: MIT License', 'Programming Language :: Python', 'Programming Language :: Python :: 2.6', 'Programming Language :: Python :: 2.7', 'Programming Language :: Python :: 3', 'Programming Language :: Python :: 3.3', 'Programming Language :: Python :: 3.4', 'Programming Language :: Python :: 3.5', 'Programming Language :: Python :: 3.6', 'Programming Language :: Python :: Implementation :: CPython', 'Programming Language :: Python :: Implementation :: PyPy' ], # $ setup.py publish support. cmdclass={ 'upload': UploadCommand, }, ) We will now edit the file so to follow our package. We are going to change the name of our package, the author name and description . Since we keep our requirements in a special file we are going to edit how to tell setup.py about those. We will also change README.rst to README.md . When you are done the file should look like: #!/usr/bin/env python # -*- coding: utf-8 -*- # Note: To use the 'upload' functionality of this file, you must: # $ pip install twine import io import os import sys from shutil import rmtree from setuptools import find_packages, setup, Command # Package meta-data. NAME = 'count_variants' DESCRIPTION = 'Count variants in VCF files based on different criterias' URL = 'https://github.com/moonso/count_variants' EMAIL = 'mans.magnusson@scilifelab.se' AUTHOR = 'M\u00e5ns Magnusson' here = os.path.abspath(os.path.dirname(__file__)) def parse_reqs(req_path='./requirements.txt'): \"\"\"Recursively parse requirements from nested pip files.\"\"\" install_requires = [] with io.open(os.path.join(here, 'requirements.txt'), encoding='utf-8') as handle: # remove comments and empty lines lines = (line.strip() for line in handle if line.strip() and not line.startswith('#')) for line in lines: # check for nested requirements files if line.startswith('-r'): # recursively call this function install_requires += parse_reqs(req_path=line[3:]) else: # add the line as a new requirement install_requires.append(line) return install_requires # What packages are required for this module to be executed? REQUIRED = parse_reqs() # The rest you shouldn't have to touch too much :) # ------------------------------------------------ # Except, perhaps the License and Trove Classifiers! # If you do change the License, remember to change the Trove Classifier for that! # Import the README and use it as the long-description. # Note: this will only work if 'README.rst' is present in your MANIFEST.in file! with io.open(os.path.join(here, 'README.md'), encoding='utf-8') as f: long_description = '\\n' + f.read() # Load the package's __version__.py module as a dictionary. about = {} with open(os.path.join(here, NAME, '__version__.py')) as f: exec(f.read(), about) class UploadCommand(Command): \"\"\"Support setup.py upload.\"\"\" description = 'Build and publish the package.' user_options = [] @staticmethod def status(s): \"\"\"Prints things in bold.\"\"\" print('\\033[1m{0}\\033[0m'.format(s)) def initialize_options(self): pass def finalize_options(self): pass def run(self): try: self.status('Removing previous builds\u2026') rmtree(os.path.join(here, 'dist')) except OSError: pass self.status('Building Source and Wheel (universal) distribution\u2026') os.system('{0} setup.py sdist bdist_wheel --universal'.format(sys.executable)) self.status('Uploading the package to PyPi via Twine\u2026') os.system('twine upload dist/*') sys.exit() # Where the magic happens: setup( name=NAME, version=about['__version__'], description=DESCRIPTION, long_description=long_description, author=AUTHOR, author_email=EMAIL, url=URL, packages=find_packages(exclude=('tests',)), # If your package is a single module, use this instead of 'packages': # py_modules=['mypackage'], # entry_points={ # 'console_scripts': ['mycli=mymodule:cli'], # }, install_requires=REQUIRED, include_package_data=True, license='MIT', classifiers=[ # Trove classifiers # Full list: https://pypi.python.org/pypi?%3Aaction=list_classifiers 'License :: OSI Approved :: MIT License', 'Programming Language :: Python', 'Programming Language :: Python :: 2.6', 'Programming Language :: Python :: 2.7', 'Programming Language :: Python :: 3', 'Programming Language :: Python :: 3.3', 'Programming Language :: Python :: 3.4', 'Programming Language :: Python :: 3.5', 'Programming Language :: Python :: 3.6', 'Programming Language :: Python :: Implementation :: CPython', 'Programming Language :: Python :: Implementation :: PyPy' ], # $ setup.py publish support. cmdclass={ 'upload': UploadCommand, }, ) Now we are about to add the directory where we will write the actual python code. Each module in the package must have a file called __init__.py to tell python that this is a module. From the documentation The init .py files are required to make Python treat the directories as containing packages; this is done to prevent directories with a common name, such as string, from unintentionally hiding valid modules that occur later (deeper) on the module search path. In the simplest case, init .py can just be an empty file, but it can also execute initialization code for the package or set the all variable, described later. mkdir count_variants touch count_variants/__init__.py touch count_variants/__version__.py open count_variants/__version__.py and write: __version__ = '0.1.0' The folder structure should now look like: count_variants/ \u251c\u2500\u2500 LICENSE \u251c\u2500\u2500 README.md \u251c\u2500\u2500 MANIFEST.in \u251c\u2500\u2500 .gitignore \u251c\u2500\u2500 setup.py \u251c\u2500\u2500 count_variants/ \u251c\u2500\u2500 __init__.py \u251c\u2500\u2500 __version__.py Add all the new files to a new git commit. (Tip: use git status to see if all files are committed.)","title":"2. setup.py"},{"location":"python/count_variants/setup_py/#2-setuppy","text":"Every python package contains a special file called setup.py in the root directory. This file describes some meta information about the package and how to create a distribution. There is a excellent guide for how to setup a package and create the setup.py here . Lets continue with our example following @kennethreitz guide, please read the guide first. Copy setup.py from the guide and open it, if you do it right it should look like: #!/usr/bin/env python # -*- coding: utf-8 -*- # Note: To use the 'upload' functionality of this file, you must: # $ pip install twine import io import os import sys from shutil import rmtree from setuptools import find_packages, setup, Command # Package meta-data. NAME = 'mypackage' DESCRIPTION = 'My short description for my project. ' URL = 'https://github.com/me/myproject' EMAIL = 'me@example.com' AUTHOR = 'Awesome Soul' # What packages are required for this module to be executed? REQUIRED = [ # 'requests', 'maya', 'records', ] # The rest you shouldn't have to touch too much :) # ------------------------------------------------ # Except, perhaps the License and Trove Classifiers! # If you do change the License, remember to change the Trove Classifier for that! here = os.path.abspath(os.path.dirname(__file__)) # Import the README and use it as the long-description. # Note: this will only work if 'README.rst' is present in your MANIFEST.in file! with io.open(os.path.join(here, 'README.rst'), encoding='utf-8') as f: long_description = '\\n' + f.read() # Load the package's __version__.py module as a dictionary. about = {} with open(os.path.join(here, NAME, '__version__.py')) as f: exec(f.read(), about) class UploadCommand(Command): \"\"\"Support setup.py upload.\"\"\" description = 'Build and publish the package.' user_options = [] @staticmethod def status(s): \"\"\"Prints things in bold.\"\"\" print('\\033[1m{0}\\033[0m'.format(s)) def initialize_options(self): pass def finalize_options(self): pass def run(self): try: self.status('Removing previous builds\u2026') rmtree(os.path.join(here, 'dist')) except OSError: pass self.status('Building Source and Wheel (universal) distribution\u2026') os.system('{0} setup.py sdist bdist_wheel --universal'.format(sys.executable)) self.status('Uploading the package to PyPi via Twine\u2026') os.system('twine upload dist/*') sys.exit() # Where the magic happens: setup( name=NAME, version=about['__version__'], description=DESCRIPTION, long_description=long_description, author=AUTHOR, author_email=EMAIL, url=URL, packages=find_packages(exclude=('tests',)), # If your package is a single module, use this instead of 'packages': # py_modules=['mypackage'], # entry_points={ # 'console_scripts': ['mycli=mymodule:cli'], # }, install_requires=REQUIRED, include_package_data=True, license='MIT', classifiers=[ # Trove classifiers # Full list: https://pypi.python.org/pypi?%3Aaction=list_classifiers 'License :: OSI Approved :: MIT License', 'Programming Language :: Python', 'Programming Language :: Python :: 2.6', 'Programming Language :: Python :: 2.7', 'Programming Language :: Python :: 3', 'Programming Language :: Python :: 3.3', 'Programming Language :: Python :: 3.4', 'Programming Language :: Python :: 3.5', 'Programming Language :: Python :: 3.6', 'Programming Language :: Python :: Implementation :: CPython', 'Programming Language :: Python :: Implementation :: PyPy' ], # $ setup.py publish support. cmdclass={ 'upload': UploadCommand, }, ) We will now edit the file so to follow our package. We are going to change the name of our package, the author name and description . Since we keep our requirements in a special file we are going to edit how to tell setup.py about those. We will also change README.rst to README.md . When you are done the file should look like: #!/usr/bin/env python # -*- coding: utf-8 -*- # Note: To use the 'upload' functionality of this file, you must: # $ pip install twine import io import os import sys from shutil import rmtree from setuptools import find_packages, setup, Command # Package meta-data. NAME = 'count_variants' DESCRIPTION = 'Count variants in VCF files based on different criterias' URL = 'https://github.com/moonso/count_variants' EMAIL = 'mans.magnusson@scilifelab.se' AUTHOR = 'M\u00e5ns Magnusson' here = os.path.abspath(os.path.dirname(__file__)) def parse_reqs(req_path='./requirements.txt'): \"\"\"Recursively parse requirements from nested pip files.\"\"\" install_requires = [] with io.open(os.path.join(here, 'requirements.txt'), encoding='utf-8') as handle: # remove comments and empty lines lines = (line.strip() for line in handle if line.strip() and not line.startswith('#')) for line in lines: # check for nested requirements files if line.startswith('-r'): # recursively call this function install_requires += parse_reqs(req_path=line[3:]) else: # add the line as a new requirement install_requires.append(line) return install_requires # What packages are required for this module to be executed? REQUIRED = parse_reqs() # The rest you shouldn't have to touch too much :) # ------------------------------------------------ # Except, perhaps the License and Trove Classifiers! # If you do change the License, remember to change the Trove Classifier for that! # Import the README and use it as the long-description. # Note: this will only work if 'README.rst' is present in your MANIFEST.in file! with io.open(os.path.join(here, 'README.md'), encoding='utf-8') as f: long_description = '\\n' + f.read() # Load the package's __version__.py module as a dictionary. about = {} with open(os.path.join(here, NAME, '__version__.py')) as f: exec(f.read(), about) class UploadCommand(Command): \"\"\"Support setup.py upload.\"\"\" description = 'Build and publish the package.' user_options = [] @staticmethod def status(s): \"\"\"Prints things in bold.\"\"\" print('\\033[1m{0}\\033[0m'.format(s)) def initialize_options(self): pass def finalize_options(self): pass def run(self): try: self.status('Removing previous builds\u2026') rmtree(os.path.join(here, 'dist')) except OSError: pass self.status('Building Source and Wheel (universal) distribution\u2026') os.system('{0} setup.py sdist bdist_wheel --universal'.format(sys.executable)) self.status('Uploading the package to PyPi via Twine\u2026') os.system('twine upload dist/*') sys.exit() # Where the magic happens: setup( name=NAME, version=about['__version__'], description=DESCRIPTION, long_description=long_description, author=AUTHOR, author_email=EMAIL, url=URL, packages=find_packages(exclude=('tests',)), # If your package is a single module, use this instead of 'packages': # py_modules=['mypackage'], # entry_points={ # 'console_scripts': ['mycli=mymodule:cli'], # }, install_requires=REQUIRED, include_package_data=True, license='MIT', classifiers=[ # Trove classifiers # Full list: https://pypi.python.org/pypi?%3Aaction=list_classifiers 'License :: OSI Approved :: MIT License', 'Programming Language :: Python', 'Programming Language :: Python :: 2.6', 'Programming Language :: Python :: 2.7', 'Programming Language :: Python :: 3', 'Programming Language :: Python :: 3.3', 'Programming Language :: Python :: 3.4', 'Programming Language :: Python :: 3.5', 'Programming Language :: Python :: 3.6', 'Programming Language :: Python :: Implementation :: CPython', 'Programming Language :: Python :: Implementation :: PyPy' ], # $ setup.py publish support. cmdclass={ 'upload': UploadCommand, }, ) Now we are about to add the directory where we will write the actual python code. Each module in the package must have a file called __init__.py to tell python that this is a module. From the documentation The init .py files are required to make Python treat the directories as containing packages; this is done to prevent directories with a common name, such as string, from unintentionally hiding valid modules that occur later (deeper) on the module search path. In the simplest case, init .py can just be an empty file, but it can also execute initialization code for the package or set the all variable, described later. mkdir count_variants touch count_variants/__init__.py touch count_variants/__version__.py open count_variants/__version__.py and write: __version__ = '0.1.0' The folder structure should now look like: count_variants/ \u251c\u2500\u2500 LICENSE \u251c\u2500\u2500 README.md \u251c\u2500\u2500 MANIFEST.in \u251c\u2500\u2500 .gitignore \u251c\u2500\u2500 setup.py \u251c\u2500\u2500 count_variants/ \u251c\u2500\u2500 __init__.py \u251c\u2500\u2500 __version__.py Add all the new files to a new git commit. (Tip: use git status to see if all files are committed.)","title":"2. setup.py"},{"location":"python/count_variants/structure/","text":"1. Setup a structure To start with let's create a environment and a directory for our package. Assuming you have conda installed, run the following: 1.1 Basics conda create -n count_variants python=3 source activate count_variants mkdir count_variants cd count_variants We wan't to do version handling with git, to make this a git repository do git init touch .gitignore The .gitignore file is explained in the git section , for now we only need to know that this file tells git what to care and not care about. 1.2 README Every decent python package should at least include a README file that describes the intention of the writer. Let's create a README.md file, this will be updated as we go along. For now do touch README.md open the file with an editor of your choice and write the following: # count_variants A package to count variants in VCF files based on different criterias. You might recognize this as \"markdown\", a markup language for formatting text. You can read more about it under Documentation . 1.3 LICENSE, MANIFEST.in, requirements.txt {#license} Next step is to include a LICENSE file, every project should have one. There are many different licenses, you can read more here . For now we will choose a MIT license . Open a file called LICENSE and copy paste the MIT License and fill in year and name. MANIFEST.in is a file that explains what non python static files should be included in the distribution. For now we will leave that one empty. Just do: touch MANIFEST.in requirements.txt is for declaring which third party software our package depends on. Right now we only know that we will be using Click to build a cli, so run: echo click > requirements.txt 1.4 First commit {#first_commit} Save the changes. It is now time to do our first commit! Run: $ git status On branch master Initial commit Untracked files: (use \"git add <file>...\" to include in what will be committed) .gitignore README.md LICENSE MANIFEST.in requirements.txt nothing added to commit but untracked files present (use \"git add\" to track) $ git add .gitignore README.md LICENSE MANIFEST.in requirements.txt $ git commit -m \"First commit\" $ git status On branch master nothing to commit, working tree clean","title":"1. Setup a structure"},{"location":"python/count_variants/structure/#1-setup-a-structure","text":"To start with let's create a environment and a directory for our package. Assuming you have conda installed, run the following:","title":"1. Setup a structure"},{"location":"python/count_variants/structure/#11-basics","text":"conda create -n count_variants python=3 source activate count_variants mkdir count_variants cd count_variants We wan't to do version handling with git, to make this a git repository do git init touch .gitignore The .gitignore file is explained in the git section , for now we only need to know that this file tells git what to care and not care about.","title":"1.1 Basics"},{"location":"python/count_variants/structure/#12-readme","text":"Every decent python package should at least include a README file that describes the intention of the writer. Let's create a README.md file, this will be updated as we go along. For now do touch README.md open the file with an editor of your choice and write the following: # count_variants A package to count variants in VCF files based on different criterias. You might recognize this as \"markdown\", a markup language for formatting text. You can read more about it under Documentation .","title":"1.2 README"},{"location":"python/count_variants/structure/#13-license-manifestin-requirementstxt-license","text":"Next step is to include a LICENSE file, every project should have one. There are many different licenses, you can read more here . For now we will choose a MIT license . Open a file called LICENSE and copy paste the MIT License and fill in year and name. MANIFEST.in is a file that explains what non python static files should be included in the distribution. For now we will leave that one empty. Just do: touch MANIFEST.in requirements.txt is for declaring which third party software our package depends on. Right now we only know that we will be using Click to build a cli, so run: echo click > requirements.txt","title":"1.3 LICENSE, MANIFEST.in, requirements.txt  {#license}"},{"location":"python/count_variants/structure/#14-first-commit-first_commit","text":"Save the changes. It is now time to do our first commit! Run: $ git status On branch master Initial commit Untracked files: (use \"git add <file>...\" to include in what will be committed) .gitignore README.md LICENSE MANIFEST.in requirements.txt nothing added to commit but untracked files present (use \"git add\" to track) $ git add .gitignore README.md LICENSE MANIFEST.in requirements.txt $ git commit -m \"First commit\" $ git status On branch master nothing to commit, working tree clean","title":"1.4 First commit {#first_commit}"},{"location":"tools/","text":"1: Git, GitHub, and Markdown These are all essential skills in the modern developer toolbox. Git is our version control system. It's abundant in use in the industry and knowing it well is vital for collaborating and writing code. GitHub is a central Git-server which makes collaborating with code easy. We use it to store all our code. GitHub hosts an impressive amount of open source software and is something you should be intimately familiar with! Markdown is the go to markup language for authoring documentation, comments, websites, and more. Markdown let's you write documents in plain text without worrying about the formatting - great for productivity! It's another core tool that you want to make friends with \ud83d\ude04","title":"Home"},{"location":"tools/#1-git-github-and-markdown","text":"These are all essential skills in the modern developer toolbox. Git is our version control system. It's abundant in use in the industry and knowing it well is vital for collaborating and writing code. GitHub is a central Git-server which makes collaborating with code easy. We use it to store all our code. GitHub hosts an impressive amount of open source software and is something you should be intimately familiar with! Markdown is the go to markup language for authoring documentation, comments, websites, and more. Markdown let's you write documents in plain text without worrying about the formatting - great for productivity! It's another core tool that you want to make friends with \ud83d\ude04","title":"1: Git, GitHub, and Markdown"}]}