{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Introduction This is a software development guide. Getting started If you are new to this repository we want to get you started right away! Please have a look at one of the assignments . Setup This section described how to setup the repo for development. To preview changes in the finished format you need mkdocs . If you are using conda you should be able to run the following locally: # create an active conda environment conda create -n D_mkdocs source activate D_mkdocs # install our only dependency pip install mkdocs # now you should be able to run mkdocs --help # to preview changes in a browser, navigate to the repo and run: mkdocs serve # content is now served on http://127.0.0.1:8000 !","title":"Home"},{"location":"#introduction","text":"This is a software development guide.","title":"Introduction"},{"location":"#getting-started","text":"If you are new to this repository we want to get you started right away! Please have a look at one of the assignments .","title":"Getting started"},{"location":"#setup","text":"This section described how to setup the repo for development. To preview changes in the finished format you need mkdocs . If you are using conda you should be able to run the following locally: # create an active conda environment conda create -n D_mkdocs source activate D_mkdocs # install our only dependency pip install mkdocs # now you should be able to run mkdocs --help # to preview changes in a browser, navigate to the repo and run: mkdocs serve # content is now served on http://127.0.0.1:8000 !","title":"Setup"},{"location":"conda/","text":"Conda Introduction Environments Packages Version 1. Introduction Conda is a package, dependency and environment manager for any language. Conda actually consists of Anaconda and a smaller version called Miniconda. Miniconda is usually sufficient most of the time, but you can also install anaconda from miniconda using: $ conda create -n test_ananconda anaconda The conda command is the primary interface for managing installations of various packages and environments. 2. Environments A conda environment is a directory that contains a specific collection of conda packages that you have installed. If you change one environment, your other environments are not affected. Condas default environment is called base or root. To create an environment, run: $ conda create --name [environment] You can easily activate or deactivate environments, which is how you switch between them. To activate an environment, run: $ source activate [environment] Conda puts an asterisk (*) in front of the active environment. You can view your environments with: $ conda info --envs or $ conda env list Your active environment will also show in front of your prompt: (my_env) $ To deactivate your current environment: $ source deactivate You can copy your environment by: $ conda create --name [environment] --clone [copy_of_environment] You can also share your environment with someone by giving them a copy of your environment.yaml file. To remove an environment, run: $ conda remove --name [environment] --all 3. Packages A conda package is a compressed tarball file that contains system-level libraries, Python or other modules, executable programs and other components. Conda keeps track of the dependencies between packages and platforms. Conda packages are downloaded from remote channels, which are URLs to directories containing conda packages. The conda command searches a default set of channels, and packages are automatically downloaded and updated from a central repository. To install conda packages in your active environment (default or other), run: $ conda install [packagename_1] [packagename_2] To install conda packages in an environment, run: $ conda install --name [environemnt] [packagename_1] [packagename_2] It is also possible to create a new environment and install packages at the same time $ conda create --name [environment] [packagename] To list all packages in a conda environment, run: $ conda list --name [environment] To remove a package, run: $ conda remove --name [packagename] 3.1 Versions To install or change a specific version of a package use the syntax [packagename]=[version]. $ conda install samtools=1.6.0 vt=170135","title":"Conda"},{"location":"conda/#conda","text":"Introduction Environments Packages Version","title":"Conda"},{"location":"conda/#1-introduction","text":"Conda is a package, dependency and environment manager for any language. Conda actually consists of Anaconda and a smaller version called Miniconda. Miniconda is usually sufficient most of the time, but you can also install anaconda from miniconda using: $ conda create -n test_ananconda anaconda The conda command is the primary interface for managing installations of various packages and environments.","title":"1. Introduction"},{"location":"conda/#2-environments","text":"A conda environment is a directory that contains a specific collection of conda packages that you have installed. If you change one environment, your other environments are not affected. Condas default environment is called base or root. To create an environment, run: $ conda create --name [environment] You can easily activate or deactivate environments, which is how you switch between them. To activate an environment, run: $ source activate [environment] Conda puts an asterisk (*) in front of the active environment. You can view your environments with: $ conda info --envs or $ conda env list Your active environment will also show in front of your prompt: (my_env) $ To deactivate your current environment: $ source deactivate You can copy your environment by: $ conda create --name [environment] --clone [copy_of_environment] You can also share your environment with someone by giving them a copy of your environment.yaml file. To remove an environment, run: $ conda remove --name [environment] --all","title":"2. Environments"},{"location":"conda/#3-packages","text":"A conda package is a compressed tarball file that contains system-level libraries, Python or other modules, executable programs and other components. Conda keeps track of the dependencies between packages and platforms. Conda packages are downloaded from remote channels, which are URLs to directories containing conda packages. The conda command searches a default set of channels, and packages are automatically downloaded and updated from a central repository. To install conda packages in your active environment (default or other), run: $ conda install [packagename_1] [packagename_2] To install conda packages in an environment, run: $ conda install --name [environemnt] [packagename_1] [packagename_2] It is also possible to create a new environment and install packages at the same time $ conda create --name [environment] [packagename] To list all packages in a conda environment, run: $ conda list --name [environment] To remove a package, run: $ conda remove --name [packagename]","title":"3. Packages"},{"location":"conda/#31-versions","text":"To install or change a specific version of a package use the syntax [packagename]=[version]. $ conda install samtools=1.6.0 vt=170135","title":"3.1 Versions"},{"location":"conda/activate/","text":"Activate conda envs After you have created a conda environment, conda tells you the following: # # To activate this environment, use: # > source activate D_mkdocs # # To deactivate this environment, use: # > source deactivate D_mkdocs # Production and stage We have two special conda environments with special configurations: prod and stage . prod is the main environment in which not surprisingly, our production tools are running. stage is a recent copy of prod used for testing of your latest finished branch and to test deployment. What is so special about prod or stage ? They come with all tools already pointing to their configurations and need to be activated using a shell script. To activate prod environment, type: up or useprod To activate stage environment, type: us or usestage I want to know more us and usestage are aliases pointing to a script: alias usestage='source ${HOME}/servers/resources/activate-stage.sh' That script will activate the correct environment(s) and set the aliases for the environment.","title":"How do I activate an env?"},{"location":"conda/activate/#activate-conda-envs","text":"After you have created a conda environment, conda tells you the following: # # To activate this environment, use: # > source activate D_mkdocs # # To deactivate this environment, use: # > source deactivate D_mkdocs #","title":"Activate conda envs"},{"location":"conda/activate/#production-and-stage","text":"We have two special conda environments with special configurations: prod and stage . prod is the main environment in which not surprisingly, our production tools are running. stage is a recent copy of prod used for testing of your latest finished branch and to test deployment. What is so special about prod or stage ? They come with all tools already pointing to their configurations and need to be activated using a shell script. To activate prod environment, type: up or useprod To activate stage environment, type: us or usestage","title":"Production and stage"},{"location":"conda/activate/#i-want-to-know-more","text":"us and usestage are aliases pointing to a script: alias usestage='source ${HOME}/servers/resources/activate-stage.sh' That script will activate the correct environment(s) and set the aliases for the environment.","title":"I want to know more"},{"location":"conda/conda_conventions/","text":"Conda conventions Avoid installing anything in the conda default environment (root) if possible Naming conventions for conda environments: Production: [env_type=P]_[logical_name]_[creation_date] Development: [env_type=D]_[logical_name]_[creation_date]_[signature] Archive: [env_type=A]_[logical_name]_[timestamp]_[signature] env_type: environment types are D (Develop), P (Production), or A (Archive). creation_date: %y%m%d timestamp: %y%m%d %H%M%S . logical_name: whatever makes sense. signature: something to show who created the environment. Use the two- or three letter name acronyms assigned to you.","title":"How to name your conda env"},{"location":"conda/conda_conventions/#conda-conventions","text":"Avoid installing anything in the conda default environment (root) if possible Naming conventions for conda environments: Production: [env_type=P]_[logical_name]_[creation_date] Development: [env_type=D]_[logical_name]_[creation_date]_[signature] Archive: [env_type=A]_[logical_name]_[timestamp]_[signature] env_type: environment types are D (Develop), P (Production), or A (Archive). creation_date: %y%m%d timestamp: %y%m%d %H%M%S . logical_name: whatever makes sense. signature: something to show who created the environment. Use the two- or three letter name acronyms assigned to you.","title":"Conda conventions"},{"location":"conda/confinement/","text":"Confinement It is possible to truely compartimentalize your conda env with the inclusion of bash functions, bash env variables, and aliases! Use case: aliases So, you want to have your aliases available in your conda env only? Solution? create a pre-activate script and post-deactivate script! cd ${your_conda_env} # e.g. cd ./miniconda/envs/production/ mkdir -p ./etc/conda/activate.d mkdir -p ./etc/conda/deactivate.d touch ./etc/conda/activate.d/alias.sh touch ./etc/conda/deactivate.d/unalias.sh The ./etc/conda/activate.d/alias.sh is the pre-activate script and will be run before your conda env becomes active. The ./etc/conda/deactivate.d/unalias.sh is the post-deactivate script and will be run after your conda env is deactivated. cat ./etc/conda/activate.d/alias.sh alias lol='ls -ltr' cat ./etc/conda/deactivate.d/unalias.sh unalias lol Sources https://conda.io/docs/user-guide/tasks/manage-environments.html#saving-environment-variables","title":"Confinement"},{"location":"conda/confinement/#confinement","text":"It is possible to truely compartimentalize your conda env with the inclusion of bash functions, bash env variables, and aliases!","title":"Confinement"},{"location":"conda/confinement/#use-case-aliases","text":"So, you want to have your aliases available in your conda env only? Solution? create a pre-activate script and post-deactivate script! cd ${your_conda_env} # e.g. cd ./miniconda/envs/production/ mkdir -p ./etc/conda/activate.d mkdir -p ./etc/conda/deactivate.d touch ./etc/conda/activate.d/alias.sh touch ./etc/conda/deactivate.d/unalias.sh The ./etc/conda/activate.d/alias.sh is the pre-activate script and will be run before your conda env becomes active. The ./etc/conda/deactivate.d/unalias.sh is the post-deactivate script and will be run after your conda env is deactivated. cat ./etc/conda/activate.d/alias.sh alias lol='ls -ltr' cat ./etc/conda/deactivate.d/unalias.sh unalias lol","title":"Use case: aliases"},{"location":"conda/confinement/#sources","text":"https://conda.io/docs/user-guide/tasks/manage-environments.html#saving-environment-variables","title":"Sources"},{"location":"dev/gitflow/","text":"Gitflow Workflow The Gitflow Workflow defines a strict branching model designed around the project release. This provides a robust framework for managing business critical projects. Gitflow is ideally suited for projects that have a scheduled release cycle. It assigns very specific roles to different branches and defines how and when they should interact. In addition to feature branches, it uses individual branches for preparing, maintaining, and recording releases. TL;DR Gitflow is one of two styles of branching workflows you and your team can utilize. Some key takeaways to know about Gitflow are: The workflow is great for a release-based software workflow. Gitflow offers a dedicated channel for hotfixes to production. The workflow enables an easy rollback. It is ideal when there needs to be multiple versions in production. Gitflow is based in two main branches with infinite lifetime: master \u200a\u2014\u200athis branch contains production code. All development code is merged into master eventually. develop \u200a\u2014\u200athis branch contains pre-production code. When the features are finished then they are merged into develop. During the development cycle, a variety of supporting branches are used: feature-* \u200a\u2014\u200afeature branches are used to develop new features for the upcoming releases. May branch off from develop and must merge into develop. hotfix-* \u200a\u2014\u200ahotfix branches are necessary to act immediately upon an undesired status of master. May branch off from master and must merge into master and develop. release-* \u200a\u2014\u200arelease branches support preparation of a new production release. They allow many minor bug to be fixed and preparation of meta-data for a release. May branch off from develop and must merge into master and develop. Getting Started Gitflow is really just an abstract idea of a Git workflow. This means it dictates what kind of branches to set up and how to merge them together. We will touch on the purposes of the branches below. The git-flow toolset is an actual command line tool that has an installation process. Installation On OSX systems, you can execute brew install git-flow . On Linux install gitflow-avh with your favorite package manager. How it works Develop and Master Branches Instead of a single master branch, this workflow uses two branches to record the history of the project. The master branch stores the official release history, and the develop branch serves as an integration branch for features. The first step is to complement the default master with a develop branch. A simple way to do this is for one developer to create an empty develop branch locally and push it to the server: git branch develop git push -u origin develop This branch will contain the complete history of the project, whereas master will contain an abridged version. Other developers should now clone the central repository and create a tracking branch for develop. When using the git-flow extension library, executing git flow init on an existing repo will create the develop branch: $ git flow init Initialized empty Git repository in ~/project/.git/ No branches exist yet. Base branches must be created now. Branch name for production releases: [master] Branch name for \"next release\" development: [develop] How to name your supporting branch prefixes? Feature branches? [feature/] Release branches? [release/] Hotfix branches? [hotfix/] Support branches? [support/] Version tag prefix? [] $ git branch * develop master Feature Branches Each new feature should reside in its own branch, which can be pushed to the central repository for backup/collaboration. But, instead of branching off of master, feature branches use develop as their parent branch. When a feature is complete, it gets merged back into develop. Features should never interact directly with master. Creating a feature branch Without the git-flow extensions: git checkout develop git checkout -b feature_branch When using the git-flow extension: git flow feature start feature_branch Continue your work and use Git like you normally would. Release Branches Once develop has acquired enough features for a release (or a predetermined release date is approaching), you fork a release branch off of develop. Creating this branch starts the next release cycle, so no new features can be added after this point\u2014only bug fixes, documentation generation, and other release-oriented tasks should go in this branch. Once it's ready to ship, the release branch gets merged into master and tagged with a version number. In addition, it should be merged back into develop, which may have progressed since the release was initiated. Using a dedicated branch to prepare releases makes it possible for one team to polish the current release while another team continues working on features for the next release. It also creates well-defined phases of development (e.g., it's easy to say, \u201cThis week we're preparing for version 4.0,\u201d and to actually see it in the structure of the repository). A new release branch can be created using the following methods. Without the git-flow extensions: git checkout develop git checkout -b release/0.1.0 When using the git-flow extensions: $ git flow release start 0.1.0 Switched to a new branch 'release/0.1.0' Once the release is ready to ship, it will get merged it into master and develop, then the release branch will be deleted. It\u2019s important to merge back into develop because critical updates may have been added to the release branch and they need to be accessible to new features. To finish a release branch, use the following methods: Without the git-flow extensions: git checkout master git merge release/0.1.0 Or with the git-flow extension: git flow release finish '0.1.0' Hotfix Branches Maintenance or \u201chotfix\u201d branches are used to quickly patch production releases. Hotfix branches are a lot like release branches and feature branches except they're based on master instead of develop. This is the only branch that should fork directly off of master. As soon as the fix is complete, it should be merged into both master and develop (or the current release branch), and master should be tagged with an updated version number. Having a dedicated line of development for bug fixes lets your team address issues without interrupting the rest of the workflow or waiting for the next release cycle. You can think of maintenance branches as ad hoc release branches that work directly with master. A hotfix branch can be created using the following methods: Without the git-flow extensions: git checkout master git checkout -b hotfix_branch When using the git-flow extensions: $ git flow hotfix start hotfix_branch Similar to finishing a release branch, a hotfix branch gets merged into both master and develop. git checkout master git merge hotfix_branch git checkout develop git merge hotfix_branch git branch -D hotfix_branch $ git flow hotfix finish hotfix_branch Review The next section explains how to get a feature reviewed, approved, and merged into the development branch. Review feature branches Open a Pull Request Pull Requests initiate discussion about your commits. Because they're tightly integrated with the underlying Git repository, anyone can see exactly what changes would be merged if they accept your request. You can open a Pull Request at any point during the development process: when you have little or no code but want to share some screenshots or general ideas, when you're stuck and need help or advice, or when you're ready for someone to review your work. By using GitHub's @mention system in your Pull Request message, you can ask for feedback from specific people or teams. More information on how to make a pull request can be found on github . Describe how to test your changes Pull Requests frame a problem by describing it. Maybe you're trying to fix a bug, add a feature, or trying out a new code pattern. Having a proper problem description ensures that your reviewer knows what their getting into. To let your reviewer know how to run your code, describe how to test it properly. Lay out a scenario that gives a basic runthrough from set up to expected outcome. The usual template goes like this: This PR solves the waterleak in the coffee machine, fixes issue #33. How to test : 0. Clone this feature! `git clone -b your-awesome-feature https://github.com/clinical-genomics/<your-repo>` 1. copy test db 2. test feature Expected outcome: leak is now solved! Are there multiple angles you can test your feature with? Awesome! Specify a test case for each scenario. Not only does this help your reviewer navigate your code, it also helps you. By describing the feature step by step for someone else, it ensures you have not missed anything critical. Discuss and review your code Once a Pull Request has been opened, the person or team reviewing your changes may have questions or comments. Read how to request a pull request review . You can also continue to push to your branch in light of discussion and feedback about your commits. If someone comments that you forgot to do something or if there is a bug in the code, you can fix it in your branch and push up the change. GitHub will show your new commits and any additional feedback you may receive in the unified Pull Request view. ProTip Pull Request comments are written in Markdown, so you can embed images and emoji, use pre-formatted text blocks, and other lightweight formatting. You even can suggest commitable changes right in the comment. A review focusses on the code health and on production readiness. If you're unsure on how to review a pull request, read more about it on github . Code health Perhaps the coding style doesn't match project guidelines, the change is missing unit tests, or maybe everything looks great and props are in order. Pull Requests are designed to encourage and capture this type of conversation. ProTip You can suggest changes directly in a comment, by hittin the . Production readiness The Pull Request template lays out one or more scenarios on how to test your code. The reviewer will go through a scenario and document the results and whether or not the test is passing. The reviewer should be generous with comments, descriptions, and screenshots and ask for explanation or improvements as the test is progressing. The reviewer will describe if the test scenario passes. Test your changes Before any code is deployed into production it needs to be tested in a production like environment: stage. To make your life easier, read up on how to make an update script to update your tool for each environment right here . Merge your feature Now that your changes have been verified in a production like environment, it is time to merge your code. Make sure the code has been signed off before proceding! ProTip Squashing commits of a pull request improves the readability of the commit history. During development commit early and often. Once your work is ready merge with a focussed commit message. By default, the merge button for repositories will be set to \"Squash and merge\". Once merged, Pull Requests preserve a record of the historical changes to your code. Because they're searchable, they let anyone go back in time to understand why and how a decision was made. ProTip By incorporating certain keywords into the text of your Pull Request, you can associate issues with code. When your Pull Request is merged, the related issues are also closed. For example, entering the phrase Closes #32 would close issue number 32 in the repository. For more information, check out our help article . Example A complete example, excluding review, is as follows. Assuming we have a repo setup with a master branch. git checkout master git checkout -b develop git checkout -b feature_branch # work happens on feature branch git checkout develop git merge feature_branch git checkout master git merge develop git branch -d feature_branch In addition to the feature and release flow, a hotfix example is as follows: git checkout master git checkout -b hotfix_branch # work is done commits are added to the hotfix_branch git checkout develop git merge hotfix_branch git checkout master git merge hotfix_branch Sources atlassian medium","title":"git flow"},{"location":"dev/gitflow/#gitflow-workflow","text":"The Gitflow Workflow defines a strict branching model designed around the project release. This provides a robust framework for managing business critical projects. Gitflow is ideally suited for projects that have a scheduled release cycle. It assigns very specific roles to different branches and defines how and when they should interact. In addition to feature branches, it uses individual branches for preparing, maintaining, and recording releases.","title":"Gitflow Workflow"},{"location":"dev/gitflow/#tldr","text":"Gitflow is one of two styles of branching workflows you and your team can utilize. Some key takeaways to know about Gitflow are: The workflow is great for a release-based software workflow. Gitflow offers a dedicated channel for hotfixes to production. The workflow enables an easy rollback. It is ideal when there needs to be multiple versions in production. Gitflow is based in two main branches with infinite lifetime: master \u200a\u2014\u200athis branch contains production code. All development code is merged into master eventually. develop \u200a\u2014\u200athis branch contains pre-production code. When the features are finished then they are merged into develop. During the development cycle, a variety of supporting branches are used: feature-* \u200a\u2014\u200afeature branches are used to develop new features for the upcoming releases. May branch off from develop and must merge into develop. hotfix-* \u200a\u2014\u200ahotfix branches are necessary to act immediately upon an undesired status of master. May branch off from master and must merge into master and develop. release-* \u200a\u2014\u200arelease branches support preparation of a new production release. They allow many minor bug to be fixed and preparation of meta-data for a release. May branch off from develop and must merge into master and develop.","title":"TL;DR"},{"location":"dev/gitflow/#getting-started","text":"Gitflow is really just an abstract idea of a Git workflow. This means it dictates what kind of branches to set up and how to merge them together. We will touch on the purposes of the branches below. The git-flow toolset is an actual command line tool that has an installation process.","title":"Getting Started"},{"location":"dev/gitflow/#installation","text":"On OSX systems, you can execute brew install git-flow . On Linux install gitflow-avh with your favorite package manager.","title":"Installation"},{"location":"dev/gitflow/#how-it-works","text":"","title":"How it works"},{"location":"dev/gitflow/#develop-and-master-branches","text":"Instead of a single master branch, this workflow uses two branches to record the history of the project. The master branch stores the official release history, and the develop branch serves as an integration branch for features. The first step is to complement the default master with a develop branch. A simple way to do this is for one developer to create an empty develop branch locally and push it to the server: git branch develop git push -u origin develop This branch will contain the complete history of the project, whereas master will contain an abridged version. Other developers should now clone the central repository and create a tracking branch for develop. When using the git-flow extension library, executing git flow init on an existing repo will create the develop branch: $ git flow init Initialized empty Git repository in ~/project/.git/ No branches exist yet. Base branches must be created now. Branch name for production releases: [master] Branch name for \"next release\" development: [develop] How to name your supporting branch prefixes? Feature branches? [feature/] Release branches? [release/] Hotfix branches? [hotfix/] Support branches? [support/] Version tag prefix? [] $ git branch * develop master","title":"Develop and Master Branches"},{"location":"dev/gitflow/#feature-branches","text":"Each new feature should reside in its own branch, which can be pushed to the central repository for backup/collaboration. But, instead of branching off of master, feature branches use develop as their parent branch. When a feature is complete, it gets merged back into develop. Features should never interact directly with master.","title":"Feature Branches"},{"location":"dev/gitflow/#creating-a-feature-branch","text":"Without the git-flow extensions: git checkout develop git checkout -b feature_branch When using the git-flow extension: git flow feature start feature_branch Continue your work and use Git like you normally would.","title":"Creating a feature branch"},{"location":"dev/gitflow/#release-branches","text":"Once develop has acquired enough features for a release (or a predetermined release date is approaching), you fork a release branch off of develop. Creating this branch starts the next release cycle, so no new features can be added after this point\u2014only bug fixes, documentation generation, and other release-oriented tasks should go in this branch. Once it's ready to ship, the release branch gets merged into master and tagged with a version number. In addition, it should be merged back into develop, which may have progressed since the release was initiated. Using a dedicated branch to prepare releases makes it possible for one team to polish the current release while another team continues working on features for the next release. It also creates well-defined phases of development (e.g., it's easy to say, \u201cThis week we're preparing for version 4.0,\u201d and to actually see it in the structure of the repository). A new release branch can be created using the following methods. Without the git-flow extensions: git checkout develop git checkout -b release/0.1.0 When using the git-flow extensions: $ git flow release start 0.1.0 Switched to a new branch 'release/0.1.0' Once the release is ready to ship, it will get merged it into master and develop, then the release branch will be deleted. It\u2019s important to merge back into develop because critical updates may have been added to the release branch and they need to be accessible to new features. To finish a release branch, use the following methods: Without the git-flow extensions: git checkout master git merge release/0.1.0 Or with the git-flow extension: git flow release finish '0.1.0'","title":"Release Branches"},{"location":"dev/gitflow/#hotfix-branches","text":"Maintenance or \u201chotfix\u201d branches are used to quickly patch production releases. Hotfix branches are a lot like release branches and feature branches except they're based on master instead of develop. This is the only branch that should fork directly off of master. As soon as the fix is complete, it should be merged into both master and develop (or the current release branch), and master should be tagged with an updated version number. Having a dedicated line of development for bug fixes lets your team address issues without interrupting the rest of the workflow or waiting for the next release cycle. You can think of maintenance branches as ad hoc release branches that work directly with master. A hotfix branch can be created using the following methods: Without the git-flow extensions: git checkout master git checkout -b hotfix_branch When using the git-flow extensions: $ git flow hotfix start hotfix_branch Similar to finishing a release branch, a hotfix branch gets merged into both master and develop. git checkout master git merge hotfix_branch git checkout develop git merge hotfix_branch git branch -D hotfix_branch $ git flow hotfix finish hotfix_branch","title":"Hotfix Branches"},{"location":"dev/gitflow/#review","text":"The next section explains how to get a feature reviewed, approved, and merged into the development branch.","title":"Review"},{"location":"dev/gitflow/#review-feature-branches","text":"","title":"Review feature branches"},{"location":"dev/gitflow/#open-a-pull-request","text":"Pull Requests initiate discussion about your commits. Because they're tightly integrated with the underlying Git repository, anyone can see exactly what changes would be merged if they accept your request. You can open a Pull Request at any point during the development process: when you have little or no code but want to share some screenshots or general ideas, when you're stuck and need help or advice, or when you're ready for someone to review your work. By using GitHub's @mention system in your Pull Request message, you can ask for feedback from specific people or teams. More information on how to make a pull request can be found on github .","title":"Open a Pull Request"},{"location":"dev/gitflow/#describe-how-to-test-your-changes","text":"Pull Requests frame a problem by describing it. Maybe you're trying to fix a bug, add a feature, or trying out a new code pattern. Having a proper problem description ensures that your reviewer knows what their getting into. To let your reviewer know how to run your code, describe how to test it properly. Lay out a scenario that gives a basic runthrough from set up to expected outcome. The usual template goes like this: This PR solves the waterleak in the coffee machine, fixes issue #33. How to test : 0. Clone this feature! `git clone -b your-awesome-feature https://github.com/clinical-genomics/<your-repo>` 1. copy test db 2. test feature Expected outcome: leak is now solved! Are there multiple angles you can test your feature with? Awesome! Specify a test case for each scenario. Not only does this help your reviewer navigate your code, it also helps you. By describing the feature step by step for someone else, it ensures you have not missed anything critical.","title":"Describe how to test your changes"},{"location":"dev/gitflow/#discuss-and-review-your-code","text":"Once a Pull Request has been opened, the person or team reviewing your changes may have questions or comments. Read how to request a pull request review . You can also continue to push to your branch in light of discussion and feedback about your commits. If someone comments that you forgot to do something or if there is a bug in the code, you can fix it in your branch and push up the change. GitHub will show your new commits and any additional feedback you may receive in the unified Pull Request view. ProTip Pull Request comments are written in Markdown, so you can embed images and emoji, use pre-formatted text blocks, and other lightweight formatting. You even can suggest commitable changes right in the comment. A review focusses on the code health and on production readiness. If you're unsure on how to review a pull request, read more about it on github .","title":"Discuss and review your code"},{"location":"dev/gitflow/#code-health","text":"Perhaps the coding style doesn't match project guidelines, the change is missing unit tests, or maybe everything looks great and props are in order. Pull Requests are designed to encourage and capture this type of conversation. ProTip You can suggest changes directly in a comment, by hittin the .","title":"Code health"},{"location":"dev/gitflow/#production-readiness","text":"The Pull Request template lays out one or more scenarios on how to test your code. The reviewer will go through a scenario and document the results and whether or not the test is passing. The reviewer should be generous with comments, descriptions, and screenshots and ask for explanation or improvements as the test is progressing. The reviewer will describe if the test scenario passes.","title":"Production readiness"},{"location":"dev/gitflow/#test-your-changes","text":"Before any code is deployed into production it needs to be tested in a production like environment: stage. To make your life easier, read up on how to make an update script to update your tool for each environment right here .","title":"Test your changes"},{"location":"dev/gitflow/#merge-your-feature","text":"Now that your changes have been verified in a production like environment, it is time to merge your code. Make sure the code has been signed off before proceding! ProTip Squashing commits of a pull request improves the readability of the commit history. During development commit early and often. Once your work is ready merge with a focussed commit message. By default, the merge button for repositories will be set to \"Squash and merge\". Once merged, Pull Requests preserve a record of the historical changes to your code. Because they're searchable, they let anyone go back in time to understand why and how a decision was made. ProTip By incorporating certain keywords into the text of your Pull Request, you can associate issues with code. When your Pull Request is merged, the related issues are also closed. For example, entering the phrase Closes #32 would close issue number 32 in the repository. For more information, check out our help article .","title":"Merge your feature"},{"location":"dev/gitflow/#example","text":"A complete example, excluding review, is as follows. Assuming we have a repo setup with a master branch. git checkout master git checkout -b develop git checkout -b feature_branch # work happens on feature branch git checkout develop git merge feature_branch git checkout master git merge develop git branch -d feature_branch In addition to the feature and release flow, a hotfix example is as follows: git checkout master git checkout -b hotfix_branch # work is done commits are added to the hotfix_branch git checkout develop git merge hotfix_branch git checkout master git merge hotfix_branch Sources atlassian medium","title":"Example"},{"location":"dev/githubflow/","text":"Github Flow workflow GitHub Flow is a lightweight, branching workflow that supports teams and projects where deployments are made regularly. It is centered around a feature and small confined changes. How it works Create a branch When you're working on a project, you're going to have a bunch of different features or ideas in progress at any given time \u2013 some of which are ready to go, and others which are not. Branching exists to help you manage this workflow. When you create a branch in your project, you're creating an environment where you can try out new ideas. Changes you make on a branch don't affect the master branch, so you're free to experiment and commit changes, safe in the knowledge that your branch won't be merged until it's ready to be reviewed by someone you're collaborating with. git checkout -b your-awesome-feature git push --set-upstream origin your-awesome-feature ProTip Branching is a core concept in Git, and the entire GitHub flow is based upon it. There's only one rule: anything in the master branch is always deployable. Because of this, it's extremely important that your new branch is created off of master when working on a feature or a fix. Your branch name should be descriptive (e.g., refactor-authentication, user-content-cache-key, make-retina-avatars), so that others can see what is being worked on. Add commits Once your branch has been created, it's time to start making changes. Whenever you add, edit, or delete a file, you're making a commit, and adding them to your branch. This process of adding commits keeps track of your progress as you work on a feature branch. Make sure you are on the correct branch by listing it and if necessary switching to it: git branch git checkout your-awesome-feature Commits also create a transparent history of your work that others can follow to understand what you've done and why. Each commit has an associated commit message, which is a description explaining why a particular change was made. Furthermore, each commit is considered a separate unit of change. This lets you roll back changes if a bug is found, or if you decide to head in a different direction. git add your-awesome-file git commit git push ProTip Commit messages are important, especially since Git tracks your changes and then displays them as commits once they're pushed to the server. By writing clear commit messages, you can make it easier for other people to follow along and provide feedback. Open a Pull Request Pull Requests initiate discussion about your commits. Because they're tightly integrated with the underlying Git repository, anyone can see exactly what changes would be merged if they accept your request. You can open a Pull Request at any point during the development process: when you have little or no code but want to share some screenshots or general ideas, when you're stuck and need help or advice, or when you're ready for someone to review your work. By using GitHub's @mention system in your Pull Request message, you can ask for feedback from specific people or teams. More information on how to make a pull request can be found on github . Describe how to test your changes Pull Requests frame a problem by describing it. Maybe you're trying to fix a bug, add a feature, or trying out a new code pattern. Having a proper problem description ensures that your reviewer knows what their getting into. To let your reviewer know how to run your code, describe how to test it properly. Lay out a scenario that gives a basic runthrough from set up to expected outcome. The usual template goes like this: This PR solves the waterleak in the coffee machine, fixes issue #33. How to test : 0. Clone this feature! `git clone -b your-awesome-feature https://github.com/clinical-genomics/<your-repo>` 1. copy test db 2. test feature Expected outcome: leak is now solved! Are there multiple angles you can test your feature with? Awesome! Specify a test case for each scenario. Not only does this help your reviewer navigate your code, it also helps you. By describing the feature step by step for someone else, it ensures you have not missed anything critical. Discuss and review your code Once a Pull Request has been opened, the person or team reviewing your changes may have questions or comments. Read how to request a pull request review . You can also continue to push to your branch in light of discussion and feedback about your commits. If someone comments that you forgot to do something or if there is a bug in the code, you can fix it in your branch and push up the change. GitHub will show your new commits and any additional feedback you may receive in the unified Pull Request view. ProTip Pull Request comments are written in Markdown, so you can embed images and emoji, use pre-formatted text blocks, and other lightweight formatting. You even can suggest commitable changes right in the comment. A review focusses on the code health and on production readiness. If you're unsure on how to review a pull request, read more about it on github . Code health Perhaps the coding style doesn't match project guidelines, the change is missing unit tests, or maybe everything looks great and props are in order. Pull Requests are designed to encourage and capture this type of conversation. ProTip You can suggest changes directly in a comment, by hittin the . Production readiness The Pull Request template lays out one or more scenarios on how to test your code. The reviewer will go through a scenario and document the results and whether or not the test is passing. The reviewer should be generous with comments, descriptions, and screenshots and ask for explanation or improvements as the test is progressing. The reviewer will describe if the test scenario passes. Test your changes Before any code is deployed into production it needs to be tested in a production like environment: stage. To make your life easier, read up on how to make an update script to update your tool for each environment right here . Merge your feature Now that your changes have been verified in a production like environment, it is time to merge your code. Make sure the code has been signed off before proceding! ProTip Squashing commits of a pull request improves the readability of the commit history. During development commit early and often. Once your work is ready merge with a focussed commit message. By default, the merge button for repositories will be set to \"Squash and merge\". Once merged, Pull Requests preserve a record of the historical changes to your code. Because they're searchable, they let anyone go back in time to understand why and how a decision was made. ProTip By incorporating certain keywords into the text of your Pull Request, you can associate issues with code. When your Pull Request is merged, the related issues are also closed. For example, entering the phrase Closes #32 would close issue number 32 in the repository. For more information, check out our help article . source: github guides","title":"github flow"},{"location":"dev/githubflow/#github-flow-workflow","text":"GitHub Flow is a lightweight, branching workflow that supports teams and projects where deployments are made regularly. It is centered around a feature and small confined changes.","title":"Github Flow workflow"},{"location":"dev/githubflow/#how-it-works","text":"","title":"How it works"},{"location":"dev/githubflow/#create-a-branch","text":"When you're working on a project, you're going to have a bunch of different features or ideas in progress at any given time \u2013 some of which are ready to go, and others which are not. Branching exists to help you manage this workflow. When you create a branch in your project, you're creating an environment where you can try out new ideas. Changes you make on a branch don't affect the master branch, so you're free to experiment and commit changes, safe in the knowledge that your branch won't be merged until it's ready to be reviewed by someone you're collaborating with. git checkout -b your-awesome-feature git push --set-upstream origin your-awesome-feature ProTip Branching is a core concept in Git, and the entire GitHub flow is based upon it. There's only one rule: anything in the master branch is always deployable. Because of this, it's extremely important that your new branch is created off of master when working on a feature or a fix. Your branch name should be descriptive (e.g., refactor-authentication, user-content-cache-key, make-retina-avatars), so that others can see what is being worked on.","title":"Create a branch"},{"location":"dev/githubflow/#add-commits","text":"Once your branch has been created, it's time to start making changes. Whenever you add, edit, or delete a file, you're making a commit, and adding them to your branch. This process of adding commits keeps track of your progress as you work on a feature branch. Make sure you are on the correct branch by listing it and if necessary switching to it: git branch git checkout your-awesome-feature Commits also create a transparent history of your work that others can follow to understand what you've done and why. Each commit has an associated commit message, which is a description explaining why a particular change was made. Furthermore, each commit is considered a separate unit of change. This lets you roll back changes if a bug is found, or if you decide to head in a different direction. git add your-awesome-file git commit git push ProTip Commit messages are important, especially since Git tracks your changes and then displays them as commits once they're pushed to the server. By writing clear commit messages, you can make it easier for other people to follow along and provide feedback.","title":"Add commits"},{"location":"dev/githubflow/#open-a-pull-request","text":"Pull Requests initiate discussion about your commits. Because they're tightly integrated with the underlying Git repository, anyone can see exactly what changes would be merged if they accept your request. You can open a Pull Request at any point during the development process: when you have little or no code but want to share some screenshots or general ideas, when you're stuck and need help or advice, or when you're ready for someone to review your work. By using GitHub's @mention system in your Pull Request message, you can ask for feedback from specific people or teams. More information on how to make a pull request can be found on github .","title":"Open a Pull Request"},{"location":"dev/githubflow/#describe-how-to-test-your-changes","text":"Pull Requests frame a problem by describing it. Maybe you're trying to fix a bug, add a feature, or trying out a new code pattern. Having a proper problem description ensures that your reviewer knows what their getting into. To let your reviewer know how to run your code, describe how to test it properly. Lay out a scenario that gives a basic runthrough from set up to expected outcome. The usual template goes like this: This PR solves the waterleak in the coffee machine, fixes issue #33. How to test : 0. Clone this feature! `git clone -b your-awesome-feature https://github.com/clinical-genomics/<your-repo>` 1. copy test db 2. test feature Expected outcome: leak is now solved! Are there multiple angles you can test your feature with? Awesome! Specify a test case for each scenario. Not only does this help your reviewer navigate your code, it also helps you. By describing the feature step by step for someone else, it ensures you have not missed anything critical.","title":"Describe how to test your changes"},{"location":"dev/githubflow/#discuss-and-review-your-code","text":"Once a Pull Request has been opened, the person or team reviewing your changes may have questions or comments. Read how to request a pull request review . You can also continue to push to your branch in light of discussion and feedback about your commits. If someone comments that you forgot to do something or if there is a bug in the code, you can fix it in your branch and push up the change. GitHub will show your new commits and any additional feedback you may receive in the unified Pull Request view. ProTip Pull Request comments are written in Markdown, so you can embed images and emoji, use pre-formatted text blocks, and other lightweight formatting. You even can suggest commitable changes right in the comment. A review focusses on the code health and on production readiness. If you're unsure on how to review a pull request, read more about it on github .","title":"Discuss and review your code"},{"location":"dev/githubflow/#code-health","text":"Perhaps the coding style doesn't match project guidelines, the change is missing unit tests, or maybe everything looks great and props are in order. Pull Requests are designed to encourage and capture this type of conversation. ProTip You can suggest changes directly in a comment, by hittin the .","title":"Code health"},{"location":"dev/githubflow/#production-readiness","text":"The Pull Request template lays out one or more scenarios on how to test your code. The reviewer will go through a scenario and document the results and whether or not the test is passing. The reviewer should be generous with comments, descriptions, and screenshots and ask for explanation or improvements as the test is progressing. The reviewer will describe if the test scenario passes.","title":"Production readiness"},{"location":"dev/githubflow/#test-your-changes","text":"Before any code is deployed into production it needs to be tested in a production like environment: stage. To make your life easier, read up on how to make an update script to update your tool for each environment right here .","title":"Test your changes"},{"location":"dev/githubflow/#merge-your-feature","text":"Now that your changes have been verified in a production like environment, it is time to merge your code. Make sure the code has been signed off before proceding! ProTip Squashing commits of a pull request improves the readability of the commit history. During development commit early and often. Once your work is ready merge with a focussed commit message. By default, the merge button for repositories will be set to \"Squash and merge\". Once merged, Pull Requests preserve a record of the historical changes to your code. Because they're searchable, they let anyone go back in time to understand why and how a decision was made. ProTip By incorporating certain keywords into the text of your Pull Request, you can associate issues with code. When your Pull Request is merged, the related issues are also closed. For example, entering the phrase Closes #32 would close issue number 32 in the repository. For more information, check out our help article . source: github guides","title":"Merge your feature"},{"location":"dev/models/","text":"Branching workflows Chosing a branching workflow answers the question: how are we branching and merging our features? Does the workflow scale with team size, influence deployment speed, provide an easy way to undo mistakes, impose unnecessary cognative overhead for its users? As a team, does the workflow enable me to collaborate unintrusively with my coworkers, provide for an easy merge strategy, enforce basic coding standards? Based on the answers of these questions, we are recommending two kinds of branching workflows. To understand this documentation, knowledge of git and branching is recommended! Why don't you read up on our git page? Shared repository model Before we dive into how we are using branching workflows, a brief introduction on collaborative development models. An answer to the question: how are we cloning a repository? There are two main types of collaborative development models. In the fork and pull model , anyone can fork an existing repository and push changes to their personal fork without needing access to the source repository. The changes can be pulled into the source repository by the project maintainer. In the shared repository model , collaborators are granted push access to a single shared repository and feature branches are created when changes need to be made. Pull requests are useful in this model as they initiate code review and general discussion about a set of changes before the changes are merged into the main branch of development. We are using the shared repository model. Rolling release (github flow) Some key features of github flow are: Master is production ready Master is deployed to production Development happens on feature branches Fast deployment speed Ideal for one version in production. Requires a verification for each release When would you use github flow? The workflow is great for bleeding edge software development Meaning that github flow scales easily Do you want to know more ? Distinct release (git flow) Some key features of git flow: Master is production ready Master is deployed to production Development happens in a development branch, which in turn has feature branches Deploy when the the development of a release if finished and validated Ideal for multiple versions in production. Requires a validation for each release When would you use git flow? The workflow is great for a release-based software development The development branch forms a safety barrier between development and production Do you want to know more ?","title":"Overview"},{"location":"dev/models/#branching-workflows","text":"Chosing a branching workflow answers the question: how are we branching and merging our features? Does the workflow scale with team size, influence deployment speed, provide an easy way to undo mistakes, impose unnecessary cognative overhead for its users? As a team, does the workflow enable me to collaborate unintrusively with my coworkers, provide for an easy merge strategy, enforce basic coding standards? Based on the answers of these questions, we are recommending two kinds of branching workflows. To understand this documentation, knowledge of git and branching is recommended! Why don't you read up on our git page?","title":"Branching workflows"},{"location":"dev/models/#shared-repository-model","text":"Before we dive into how we are using branching workflows, a brief introduction on collaborative development models. An answer to the question: how are we cloning a repository? There are two main types of collaborative development models. In the fork and pull model , anyone can fork an existing repository and push changes to their personal fork without needing access to the source repository. The changes can be pulled into the source repository by the project maintainer. In the shared repository model , collaborators are granted push access to a single shared repository and feature branches are created when changes need to be made. Pull requests are useful in this model as they initiate code review and general discussion about a set of changes before the changes are merged into the main branch of development. We are using the shared repository model.","title":"Shared repository model"},{"location":"dev/models/#rolling-release-github-flow","text":"Some key features of github flow are: Master is production ready Master is deployed to production Development happens on feature branches Fast deployment speed Ideal for one version in production. Requires a verification for each release When would you use github flow? The workflow is great for bleeding edge software development Meaning that github flow scales easily Do you want to know more ?","title":"Rolling release (github flow)"},{"location":"dev/models/#distinct-release-git-flow","text":"Some key features of git flow: Master is production ready Master is deployed to production Development happens in a development branch, which in turn has feature branches Deploy when the the development of a release if finished and validated Ideal for multiple versions in production. Requires a validation for each release When would you use git flow? The workflow is great for a release-based software development The development branch forms a safety barrier between development and production Do you want to know more ?","title":"Distinct release (git flow)"},{"location":"editors/","text":"Editors Choosing an editor is a person and highly subjective decision. We don't advocate any particular flavor but here are a few competent ones that we recommend you start with: Atom PyCharm Sublime Text Textmate Vim, GVim, MACVim etc. Visual Studio Code","title":"Choose your editor :)"},{"location":"editors/#editors","text":"Choosing an editor is a person and highly subjective decision. We don't advocate any particular flavor but here are a few competent ones that we recommend you start with: Atom PyCharm Sublime Text Textmate Vim, GVim, MACVim etc. Visual Studio Code","title":"Editors"},{"location":"git/","text":"Quick reference guide to Github Purpose This document offers an overview of the most important commands of git and it is intended to be a quick start guide to work with git repositories. Download and setup Download and install Git for Linux If you are working on a Debian-based version of Linux you can install the basic Git tools via APT: sudo apt-get install git-all Here are instructions on how to install Git under other Linux distributions. Download and install Git for OSX The latest version of Git for Mac can be downloaded here , use this binary installer to get an up-to date version of he software. If you are installing on a Mac with OS X 10.9 you can install Git from command line with Homebrew (get Homebrew here ) by typing: brew install git Configure username and email The first time you are using git from command line you should set up a username and a password. To ease the authentication for each action, go to your local repository and type in the following commands: git config user.email \"you@example.com\" git config user.name \"YourUsername\" If you are working locally (your computer only), you could also add the --global flag to set these values for all your repositories. Create a new repository There are several ways to create a new local repository. Create a new repository from scratch Create a directory with the name of the new repository and enter it from the command line. To initialize the repository type: git init This command will create a .git subfolder. Clone a repository from a project in the clinical-genomics GitHub organisation git clone https://github.com/clinical-genomics/development.git Where the url which can be obtained from the web page of your local fork, by clicking on the \"clone or download\" green button. The git clone command then creates a folder with the project and pulls the latest data from your fork in it. Git data transport structure and commands The following image shows the structure of the git commands and spaces: Your workspace is the folder that holds the actual working files while the \"index\" is the space is a staging area where you add your changes before committing them to the local repository or HEAD . To send the changes to your remote repository you use the \"push\" command. Here is a link to a page explaining the common Git terminology: https://help.github.com/articles/github-glossary Create a new branch or feature for your work To create a new branch, from command line move to the main folder containing your project and type: git checkout -b name_of_your_feature If everything went fine then you'll get the following message: \"Switched to a new branch name_of_your_feature\". This means you are already inside the new feature. To make sure that you are in the right branch type: git branch Whenever you want to move to another branch/feature the command is: git checkout [my-branch] Make changes and commit them to your local repository Before committing any changes to your local repository, and finally to the remote repository, make sure you were working on the branch created. After you are satisfied with your code you can start to commit to the remote repository. To check that there are actually changes to add and commit, type: git status This command will give you info on the status of your working directory comparing it with the local and remote repositories. To update the local repository to the latest status, by fetchin and merging remote changes, type: git pull The you can start adding your changes to the index with the command: git add filename_to_send_to_index The process of adding a file to the index is also called staging. Be sure to add all the files you want to include to the index. Before merging the changes you can preview them with the command: git diff source_branch target_branch Calling the \"diff\" command without arguments will list all the differences between the index and your workspace, i.e. all the files that you could add to the index. To commit changes from the index to the local repository (HEAD), type: git commit Alternatively you can add and commit the files to the local repository in only one command: git commit -a ProTip Don't use git commit -a unless you're 100% sure of what you're doing. It gives you less control over the single steps of the workflow and the files you really want to commit. Before using it, make sure everything is under control by using the git status and git diff commands. Prevent commiting tmp files your editor makes! To include a description of the commit use the -m flag. Leaving it out is fine and will drop you into an $EDITOR . git commit -m \"Example of a commit message\" It is possible to close a GitHub issue by referencing it in a commit message or pull request. The issue will be closed when the code is merged to the default branch git commit -m \"Fixes #36\" For more examples on how to close issues using keywords see this guide . Pushing changes to your remote repository After \"git add\" and \"git commit\", or \"commit -a\", your changes are in the HEAD of your local repository. To send them to the remote repository use the command \"push\": git push .gitignore This is a special file that's usually part of every repo. Here you can tell git which files it should exclude from version control. Examples include private files with sensitive data, temporary log files, and large files used for testing. GitHub hosts a brief guide about the format. There's also a great resource for finding templates for every language to use as starting points. Github usage references Github short guide Github book GitHub help page Glossary","title":"Quick reference guide to Github"},{"location":"git/#quick-reference-guide-to-github","text":"","title":"Quick reference guide to Github"},{"location":"git/#purpose","text":"This document offers an overview of the most important commands of git and it is intended to be a quick start guide to work with git repositories.","title":"Purpose"},{"location":"git/#download-and-setup","text":"","title":"Download and setup"},{"location":"git/#download-and-install-git-for-linux","text":"If you are working on a Debian-based version of Linux you can install the basic Git tools via APT: sudo apt-get install git-all Here are instructions on how to install Git under other Linux distributions.","title":"Download and install Git for Linux"},{"location":"git/#download-and-install-git-for-osx","text":"The latest version of Git for Mac can be downloaded here , use this binary installer to get an up-to date version of he software. If you are installing on a Mac with OS X 10.9 you can install Git from command line with Homebrew (get Homebrew here ) by typing: brew install git","title":"Download and install Git for OSX"},{"location":"git/#configure-username-and-email","text":"The first time you are using git from command line you should set up a username and a password. To ease the authentication for each action, go to your local repository and type in the following commands: git config user.email \"you@example.com\" git config user.name \"YourUsername\" If you are working locally (your computer only), you could also add the --global flag to set these values for all your repositories.","title":"Configure username and email"},{"location":"git/#create-a-new-repository","text":"There are several ways to create a new local repository.","title":"Create a new repository"},{"location":"git/#create-a-new-repository-from-scratch","text":"Create a directory with the name of the new repository and enter it from the command line. To initialize the repository type: git init This command will create a .git subfolder.","title":"Create a new repository from scratch"},{"location":"git/#clone-a-repository-from-a-project-in-the-clinical-genomics-github-organisation","text":"git clone https://github.com/clinical-genomics/development.git Where the url which can be obtained from the web page of your local fork, by clicking on the \"clone or download\" green button. The git clone command then creates a folder with the project and pulls the latest data from your fork in it.","title":"Clone a repository from a project in the clinical-genomics GitHub organisation"},{"location":"git/#git-data-transport-structure-and-commands","text":"The following image shows the structure of the git commands and spaces: Your workspace is the folder that holds the actual working files while the \"index\" is the space is a staging area where you add your changes before committing them to the local repository or HEAD . To send the changes to your remote repository you use the \"push\" command. Here is a link to a page explaining the common Git terminology: https://help.github.com/articles/github-glossary","title":"Git data transport structure and commands"},{"location":"git/#create-a-new-branch-or-feature-for-your-work","text":"To create a new branch, from command line move to the main folder containing your project and type: git checkout -b name_of_your_feature If everything went fine then you'll get the following message: \"Switched to a new branch name_of_your_feature\". This means you are already inside the new feature. To make sure that you are in the right branch type: git branch Whenever you want to move to another branch/feature the command is: git checkout [my-branch]","title":"Create a new branch or feature for your work"},{"location":"git/#make-changes-and-commit-them-to-your-local-repository","text":"Before committing any changes to your local repository, and finally to the remote repository, make sure you were working on the branch created. After you are satisfied with your code you can start to commit to the remote repository. To check that there are actually changes to add and commit, type: git status This command will give you info on the status of your working directory comparing it with the local and remote repositories. To update the local repository to the latest status, by fetchin and merging remote changes, type: git pull The you can start adding your changes to the index with the command: git add filename_to_send_to_index The process of adding a file to the index is also called staging. Be sure to add all the files you want to include to the index. Before merging the changes you can preview them with the command: git diff source_branch target_branch Calling the \"diff\" command without arguments will list all the differences between the index and your workspace, i.e. all the files that you could add to the index. To commit changes from the index to the local repository (HEAD), type: git commit Alternatively you can add and commit the files to the local repository in only one command: git commit -a ProTip Don't use git commit -a unless you're 100% sure of what you're doing. It gives you less control over the single steps of the workflow and the files you really want to commit. Before using it, make sure everything is under control by using the git status and git diff commands. Prevent commiting tmp files your editor makes! To include a description of the commit use the -m flag. Leaving it out is fine and will drop you into an $EDITOR . git commit -m \"Example of a commit message\" It is possible to close a GitHub issue by referencing it in a commit message or pull request. The issue will be closed when the code is merged to the default branch git commit -m \"Fixes #36\" For more examples on how to close issues using keywords see this guide .","title":"Make changes and commit them to your local repository"},{"location":"git/#pushing-changes-to-your-remote-repository","text":"After \"git add\" and \"git commit\", or \"commit -a\", your changes are in the HEAD of your local repository. To send them to the remote repository use the command \"push\": git push","title":"Pushing changes to your remote repository"},{"location":"git/#gitignore","text":"This is a special file that's usually part of every repo. Here you can tell git which files it should exclude from version control. Examples include private files with sensitive data, temporary log files, and large files used for testing. GitHub hosts a brief guide about the format. There's also a great resource for finding templates for every language to use as starting points.","title":".gitignore"},{"location":"git/#github-usage-references","text":"Github short guide Github book GitHub help page Glossary","title":"Github usage references"},{"location":"git/code-review/","text":"Code review Code reviews exist to spot mistakes that were overlooked during initial development. They are also an excellent opportunity for spreading knowledge and best practices among your team members. I've basically stolen all of this from Thoughbot's guide on Code Review . Establish the idea that everybody gets reviewed. No one stands above the rest. Every patch gets reviewed. However small, there is no valid argument to not review it. As far as possible, code should be reviewed before it gets merged. Make liberal use of GitHub and pull requests; use line-specific comments for example. For bigger patches, don't be afraid to ask to separate them into smaller units. Document code review practices. Role: Reviewer The reviewer should focus on things in this order: Intent (why) Design (how) Implementation Grammar Collect the suggested changes in TODO lists, questions for the code author, and suggested follow-ups for later patches. Workflow The code needs to be reviewed and approved by a code owner. They are automatically assigned when you create the pull request. The person reviewing the code signs off on the pull request with a \"\ud83d\udc4d\" or similar. Hot fixes are excluded from code reviews and can be merged by the author herself. Automate the automatable Aim to discover mistakes as early as possible. This includes running lints, checking for conformance to style guide, etc. Encourage team members to run these tools locally as well as considering them as part of your continuous integration flow. It can also be worth mentioning that it's often easier to take criticism from computers rather than your peers.","title":"Code review"},{"location":"git/code-review/#code-review","text":"Code reviews exist to spot mistakes that were overlooked during initial development. They are also an excellent opportunity for spreading knowledge and best practices among your team members. I've basically stolen all of this from Thoughbot's guide on Code Review . Establish the idea that everybody gets reviewed. No one stands above the rest. Every patch gets reviewed. However small, there is no valid argument to not review it. As far as possible, code should be reviewed before it gets merged. Make liberal use of GitHub and pull requests; use line-specific comments for example. For bigger patches, don't be afraid to ask to separate them into smaller units. Document code review practices.","title":"Code review"},{"location":"git/code-review/#role-reviewer","text":"The reviewer should focus on things in this order: Intent (why) Design (how) Implementation Grammar Collect the suggested changes in TODO lists, questions for the code author, and suggested follow-ups for later patches.","title":"Role: Reviewer"},{"location":"git/code-review/#workflow","text":"The code needs to be reviewed and approved by a code owner. They are automatically assigned when you create the pull request. The person reviewing the code signs off on the pull request with a \"\ud83d\udc4d\" or similar. Hot fixes are excluded from code reviews and can be merged by the author herself.","title":"Workflow"},{"location":"git/code-review/#automate-the-automatable","text":"Aim to discover mistakes as early as possible. This includes running lints, checking for conformance to style guide, etc. Encourage team members to run these tools locally as well as considering them as part of your continuous integration flow. It can also be worth mentioning that it's often easier to take criticism from computers rather than your peers.","title":"Automate the automatable"},{"location":"git/issue-reports/","text":"User stories User stories are short, simple descriptions of a feature told from the perspective of the person who desires the new capability, usually a user or customer of the system. They typically follow a simple template: As a < type of user >, I want < some goal > so that < some reason >.1 As the name suggests, a user story describes how a customer or user employs the product; it is told from the user\u2019s perspective.2 A suggested minimum description: Title: Problem description Steps to reproduce Expected outcome / suggested solution Questions answered Add label PBL Teamname Estimated Title Describe the issue in a sentence that summarises the wanted outcome - Example: Clinical-Genomics - Failing login gives me understandable feedback Steps to reproduce Describe the steps that are needed to provoke the encountered issue so that anyone can reproduce the issue. Any special circumstances that you think is affecting the outcome should be described (e.g. environment/browser etc.) Example: 1. Go to Clinical-Genomics in production environment (https://clinical.scilifelab.se) with a web browser 1. Press \"Sign in with Google\" 1. Select any of your accounts that don't have access to the system The aim is to guide the reader towards the problem without having to spend any time on figuring out how or having to communicate with the issue reporter on what to do to provoke the system in order to reproduce the issue. Expected outcome Describe the expected output so that is clear what the goal state is. The aim is to know when issue is resolved. Example: A red toast message saying: This account is not whitelisted Actual outcome Describe what happened when the Steps to reproduce where followed. The aim is to guide the reader towards the root of the problem. (E.g. Is it something that relates to this specific user or something general). Example: There is no message at all about what went wrong when I tried to login A screenshot of the outcome can often be of great help for the reader to understand what you see 1: https://www.mountaingoatsoftware.com/agile/user-stories 2: https://www.romanpichler.com/blog/10-tips-writing-good-user-stories/","title":"User stories"},{"location":"git/issue-reports/#user-stories","text":"User stories are short, simple descriptions of a feature told from the perspective of the person who desires the new capability, usually a user or customer of the system. They typically follow a simple template: As a < type of user >, I want < some goal > so that < some reason >.1 As the name suggests, a user story describes how a customer or user employs the product; it is told from the user\u2019s perspective.2","title":"User stories"},{"location":"git/issue-reports/#a-suggested-minimum-description","text":"Title: Problem description Steps to reproduce Expected outcome / suggested solution Questions answered Add label PBL Teamname Estimated","title":"A suggested minimum description:"},{"location":"git/issue-reports/#title","text":"Describe the issue in a sentence that summarises the wanted outcome - Example: Clinical-Genomics - Failing login gives me understandable feedback","title":"Title"},{"location":"git/issue-reports/#steps-to-reproduce","text":"Describe the steps that are needed to provoke the encountered issue so that anyone can reproduce the issue. Any special circumstances that you think is affecting the outcome should be described (e.g. environment/browser etc.) Example: 1. Go to Clinical-Genomics in production environment (https://clinical.scilifelab.se) with a web browser 1. Press \"Sign in with Google\" 1. Select any of your accounts that don't have access to the system The aim is to guide the reader towards the problem without having to spend any time on figuring out how or having to communicate with the issue reporter on what to do to provoke the system in order to reproduce the issue.","title":"Steps to reproduce"},{"location":"git/issue-reports/#expected-outcome","text":"Describe the expected output so that is clear what the goal state is. The aim is to know when issue is resolved. Example: A red toast message saying: This account is not whitelisted","title":"Expected outcome"},{"location":"git/issue-reports/#actual-outcome","text":"Describe what happened when the Steps to reproduce where followed. The aim is to guide the reader towards the root of the problem. (E.g. Is it something that relates to this specific user or something general). Example: There is no message at all about what went wrong when I tried to login A screenshot of the outcome can often be of great help for the reader to understand what you see 1: https://www.mountaingoatsoftware.com/agile/user-stories 2: https://www.romanpichler.com/blog/10-tips-writing-good-user-stories/","title":"Actual outcome"},{"location":"github/pr-request/","text":"How to request a pull request review After you create a pull request, you can ask a specific person to review the changes you've proposed. If you're an organization member, you can also request a specific team to review your changes. The requested reviewer or team will receive a notification that you asked them to review the pull request. Once someone has reviewed your pull request and you've made the necessary changes, you can re-request review from the same reviewer. If the requested reviewer does not submit a review, and the pull request meets the repository's mergeability requirements, you can still merge the pull request, if needed. Under your repository name, click Pull requests . In the list of pull requests, click the pull request that you'd like to ask a specific person or a team to review. Navigate to Reviewers in the right sidebar. To request a review from a suggested person under Reviewers , next to their username, click Request . Optionally, to request a review from someone other than a suggested person, click Reviewers , then click on a name or a team in the dropdown menu. Optionally, if you know the name of the person or team you'd like a review from, click Reviewers , then type the username of the person or the name of the team you're asking to review your changes. Click their team name or username to request a review. After your pull request is reviewed and you've made the necessary changes, you can ask a reviewer to re-review your pull request. Navigate to Reviewers in the right sidebar and click rerequest logo next to the reviewer's name whose review you'd like. Source: github","title":"How to request a pull request review?"},{"location":"github/pr-request/#how-to-request-a-pull-request-review","text":"After you create a pull request, you can ask a specific person to review the changes you've proposed. If you're an organization member, you can also request a specific team to review your changes. The requested reviewer or team will receive a notification that you asked them to review the pull request. Once someone has reviewed your pull request and you've made the necessary changes, you can re-request review from the same reviewer. If the requested reviewer does not submit a review, and the pull request meets the repository's mergeability requirements, you can still merge the pull request, if needed. Under your repository name, click Pull requests . In the list of pull requests, click the pull request that you'd like to ask a specific person or a team to review. Navigate to Reviewers in the right sidebar. To request a review from a suggested person under Reviewers , next to their username, click Request . Optionally, to request a review from someone other than a suggested person, click Reviewers , then click on a name or a team in the dropdown menu. Optionally, if you know the name of the person or team you'd like a review from, click Reviewers , then type the username of the person or the name of the team you're asking to review your changes. Click their team name or username to request a review. After your pull request is reviewed and you've made the necessary changes, you can ask a reviewer to re-review your pull request. Navigate to Reviewers in the right sidebar and click rerequest logo next to the reviewer's name whose review you'd like. Source: github","title":"How to request a pull request review"},{"location":"github/pr/","text":"Open a Pull Request Pull Requests initiate discussion about your commits. Because they're tightly integrated with the underlying Git repository, anyone can see exactly what changes would be merged if they accept your request. You can open a Pull Request at any point during the development process: when you have little or no code but want to share some screenshots or general ideas, when you're stuck and need help or advice, or when you're ready for someone to review your work. By using GitHub's @mention system in your Pull Request message, you can ask for feedback from specific people or teams. More information on how to make a pull request can be found on github . Describe how to test your changes Pull Requests frame a problem by describing it. Maybe you're trying to fix a bug, add a feature, or trying out a new code pattern. Having a proper problem description ensures that your reviewer knows what their getting into. To let your reviewer know how to run your code, describe how to test it properly. Lay out a scenario that gives a basic runthrough from set up to expected outcome. The usual template goes like this: This PR solves the waterleak in the coffee machine, fixes issue #33. How to test : 0. Clone this feature! `git clone -b your-awesome-feature https://github.com/clinical-genomics/<your-repo>` 1. copy test db 2. test feature Expected outcome: leak is now solved! Are there multiple angles you can test your feature with? Awesome! Specify a test case for each scenario. Not only does this help your reviewer navigate your code, it also helps you. By describing the feature step by step for someone else, it ensures you have not missed anything critical. Discuss and review your code Once a Pull Request has been opened, the person or team reviewing your changes may have questions or comments. Read how to request a pull request review . You can also continue to push to your branch in light of discussion and feedback about your commits. If someone comments that you forgot to do something or if there is a bug in the code, you can fix it in your branch and push up the change. GitHub will show your new commits and any additional feedback you may receive in the unified Pull Request view. ProTip Pull Request comments are written in Markdown, so you can embed images and emoji, use pre-formatted text blocks, and other lightweight formatting. You even can suggest commitable changes right in the comment. A review focusses on the code health and on production readiness. If you're unsure on how to review a pull request, read more about it on github . Code health Perhaps the coding style doesn't match project guidelines, the change is missing unit tests, or maybe everything looks great and props are in order. Pull Requests are designed to encourage and capture this type of conversation. ProTip You can suggest changes directly in a comment, by hittin the . Production readiness The Pull Request template lays out one or more scenarios on how to test your code. The reviewer will go through a scenario and document the results and whether or not the test is passing. The reviewer should be generous with comments, descriptions, and screenshots and ask for explanation or improvements as the test is progressing. The reviewer will describe if the test scenario passes. Test your changes Before any code is deployed into production it needs to be tested in a production like environment: stage. To make your life easier, read up on how to make an update script to update your tool for each environment right here . Merge your feature Now that your changes have been verified in a production like environment, it is time to merge your code. Make sure the code has been signed off before proceding! ProTip Squashing commits of a pull request improves the readability of the commit history. During development commit early and often. Once your work is ready merge with a focussed commit message. By default, the merge button for repositories will be set to \"Squash and merge\". Once merged, Pull Requests preserve a record of the historical changes to your code. Because they're searchable, they let anyone go back in time to understand why and how a decision was made. ProTip By incorporating certain keywords into the text of your Pull Request, you can associate issues with code. When your Pull Request is merged, the related issues are also closed. For example, entering the phrase Closes #32 would close issue number 32 in the repository. For more information, check out our help article .","title":"How to make a pull request?"},{"location":"github/pr/#open-a-pull-request","text":"Pull Requests initiate discussion about your commits. Because they're tightly integrated with the underlying Git repository, anyone can see exactly what changes would be merged if they accept your request. You can open a Pull Request at any point during the development process: when you have little or no code but want to share some screenshots or general ideas, when you're stuck and need help or advice, or when you're ready for someone to review your work. By using GitHub's @mention system in your Pull Request message, you can ask for feedback from specific people or teams. More information on how to make a pull request can be found on github .","title":"Open a Pull Request"},{"location":"github/pr/#describe-how-to-test-your-changes","text":"Pull Requests frame a problem by describing it. Maybe you're trying to fix a bug, add a feature, or trying out a new code pattern. Having a proper problem description ensures that your reviewer knows what their getting into. To let your reviewer know how to run your code, describe how to test it properly. Lay out a scenario that gives a basic runthrough from set up to expected outcome. The usual template goes like this: This PR solves the waterleak in the coffee machine, fixes issue #33. How to test : 0. Clone this feature! `git clone -b your-awesome-feature https://github.com/clinical-genomics/<your-repo>` 1. copy test db 2. test feature Expected outcome: leak is now solved! Are there multiple angles you can test your feature with? Awesome! Specify a test case for each scenario. Not only does this help your reviewer navigate your code, it also helps you. By describing the feature step by step for someone else, it ensures you have not missed anything critical.","title":"Describe how to test your changes"},{"location":"github/pr/#discuss-and-review-your-code","text":"Once a Pull Request has been opened, the person or team reviewing your changes may have questions or comments. Read how to request a pull request review . You can also continue to push to your branch in light of discussion and feedback about your commits. If someone comments that you forgot to do something or if there is a bug in the code, you can fix it in your branch and push up the change. GitHub will show your new commits and any additional feedback you may receive in the unified Pull Request view. ProTip Pull Request comments are written in Markdown, so you can embed images and emoji, use pre-formatted text blocks, and other lightweight formatting. You even can suggest commitable changes right in the comment. A review focusses on the code health and on production readiness. If you're unsure on how to review a pull request, read more about it on github .","title":"Discuss and review your code"},{"location":"github/pr/#code-health","text":"Perhaps the coding style doesn't match project guidelines, the change is missing unit tests, or maybe everything looks great and props are in order. Pull Requests are designed to encourage and capture this type of conversation. ProTip You can suggest changes directly in a comment, by hittin the .","title":"Code health"},{"location":"github/pr/#production-readiness","text":"The Pull Request template lays out one or more scenarios on how to test your code. The reviewer will go through a scenario and document the results and whether or not the test is passing. The reviewer should be generous with comments, descriptions, and screenshots and ask for explanation or improvements as the test is progressing. The reviewer will describe if the test scenario passes.","title":"Production readiness"},{"location":"github/pr/#test-your-changes","text":"Before any code is deployed into production it needs to be tested in a production like environment: stage. To make your life easier, read up on how to make an update script to update your tool for each environment right here .","title":"Test your changes"},{"location":"github/pr/#merge-your-feature","text":"Now that your changes have been verified in a production like environment, it is time to merge your code. Make sure the code has been signed off before proceding! ProTip Squashing commits of a pull request improves the readability of the commit history. During development commit early and often. Once your work is ready merge with a focussed commit message. By default, the merge button for repositories will be set to \"Squash and merge\". Once merged, Pull Requests preserve a record of the historical changes to your code. Because they're searchable, they let anyone go back in time to understand why and how a decision was made. ProTip By incorporating certain keywords into the text of your Pull Request, you can associate issues with code. When your Pull Request is merged, the related issues are also closed. For example, entering the phrase Closes #32 would close issue number 32 in the repository. For more information, check out our help article .","title":"Merge your feature"},{"location":"mkdocs/","text":"How do I publish this manual? Good. It seems you want to publish your changes? It is as simple as: mkdocs gh-deploy Tada! Changes should be available shortly on http://www.clinicalgenomics.se/development/ I want more detail We are using github's gh-pages to store the generated manual. Once mkdocs has generated HTML from the markdown, it will be stored in the branch gh-pages . More explanation on the deploying your docs page.","title":"How do I publish this manual?"},{"location":"mkdocs/#how-do-i-publish-this-manual","text":"Good. It seems you want to publish your changes? It is as simple as: mkdocs gh-deploy Tada! Changes should be available shortly on http://www.clinicalgenomics.se/development/","title":"How do I publish this manual?"},{"location":"mkdocs/#i-want-more-detail","text":"We are using github's gh-pages to store the generated manual. Once mkdocs has generated HTML from the markdown, it will be stored in the branch gh-pages . More explanation on the deploying your docs page.","title":"I want more detail"},{"location":"perl/","text":"Perl Introduction Perl is a general-purpose programming language originally developed for text manipulation and now used for a wide range of tasks including system administration, web development, network programming, GUI development, and more. The language is intended to be practical (easy to use, efficient, complete) rather than beautiful (tiny, elegant, minimal). Its major features are that it's easy to use, supports both procedural and object-oriented (OO) programming, has powerful built-in support for text processing, and has one of the world's most impressive collections of third-party modules. A good place to start learning what perl is about is the perlintro at perldocs .","title":"What if I need perl?"},{"location":"perl/#perl","text":"","title":"Perl"},{"location":"perl/#introduction","text":"Perl is a general-purpose programming language originally developed for text manipulation and now used for a wide range of tasks including system administration, web development, network programming, GUI development, and more. The language is intended to be practical (easy to use, efficient, complete) rather than beautiful (tiny, elegant, minimal). Its major features are that it's easy to use, supports both procedural and object-oriented (OO) programming, has powerful built-in support for text processing, and has one of the world's most impressive collections of third-party modules. A good place to start learning what perl is about is the perlintro at perldocs .","title":"Introduction"},{"location":"perl/best_practises/Identifiers/","text":"Identifiers The single most important practice when creating names is to devise a set of grammar rules to which all names must conform. A grammar rules specifies one or more templates (e.g., Noun :: Adjective :: Adjective) that describe how to form the entity on the left of the arrow (e.g., namespace). A suitable grammar rule for naming packages and classes is: namespace \u279d Noun :: Adjective :: Adjective | Noun :: Adjective | Noun This rule might produce package names such as: package Disk; package Disk::Audio; package Disk::DVD; package Disk::DVD::Rewritable; In this scheme, specialized versions of an existing namespace are named by adding adjectives to the name of the more general namespace. Variables should be named according to the data they will store, and as specifically as possible. Variables that are used in more than one block should always have a two-part (or longer) name. A variable is named with a noun, preceded by zero or more adjectives: variable \u279d [adjective _ ]* noun The choice of nouns and adjectives is critical. The nouns in particular should indicate what the variable does in terms of the problem domain, not in terms of the implementation. Use adjectives whenever you can. There is one extra grammatical variation that applies only to hashes and arrays that are used as look-up tables: lookup_variable \u279d [adjective _ ]* noun preposition Adding a preposition to the end of the name makes hash and array accesses much more readable. my %title_of; my @sales_from; For subroutines and methods, a suitable grammatical rule for forming names is: routine \u279d imperative_verb [ _ adjective]? _ noun _ preposition | imperative_verb [ _ adjective]? _ noun _ participle | imperative_verb [ _ adjective]? _ noun This rule results in subroutine names such as: sub get_record; # imperative_verb noun sub get_record_for; # imperative_verb noun preposition sub eat_cookie; # imperative_verb noun sub eat_previous_cookie; # imperative_verb adjective noun sub build_profile; # imperative_verb noun sub build_execution_profile; # imperative_verb adjective noun sub build_execution_profile_using; # imperative_verb adjective noun participle These naming rules particularly the two that put participles or prepositions at the ends of names create identifiers that read far more naturally, often eliminating the need for any additional comments.","title":"Identifiers"},{"location":"perl/best_practises/Identifiers/#identifiers","text":"The single most important practice when creating names is to devise a set of grammar rules to which all names must conform. A grammar rules specifies one or more templates (e.g., Noun :: Adjective :: Adjective) that describe how to form the entity on the left of the arrow (e.g., namespace). A suitable grammar rule for naming packages and classes is: namespace \u279d Noun :: Adjective :: Adjective | Noun :: Adjective | Noun This rule might produce package names such as: package Disk; package Disk::Audio; package Disk::DVD; package Disk::DVD::Rewritable; In this scheme, specialized versions of an existing namespace are named by adding adjectives to the name of the more general namespace. Variables should be named according to the data they will store, and as specifically as possible. Variables that are used in more than one block should always have a two-part (or longer) name. A variable is named with a noun, preceded by zero or more adjectives: variable \u279d [adjective _ ]* noun The choice of nouns and adjectives is critical. The nouns in particular should indicate what the variable does in terms of the problem domain, not in terms of the implementation. Use adjectives whenever you can. There is one extra grammatical variation that applies only to hashes and arrays that are used as look-up tables: lookup_variable \u279d [adjective _ ]* noun preposition Adding a preposition to the end of the name makes hash and array accesses much more readable. my %title_of; my @sales_from; For subroutines and methods, a suitable grammatical rule for forming names is: routine \u279d imperative_verb [ _ adjective]? _ noun _ preposition | imperative_verb [ _ adjective]? _ noun _ participle | imperative_verb [ _ adjective]? _ noun This rule results in subroutine names such as: sub get_record; # imperative_verb noun sub get_record_for; # imperative_verb noun preposition sub eat_cookie; # imperative_verb noun sub eat_previous_cookie; # imperative_verb adjective noun sub build_profile; # imperative_verb noun sub build_execution_profile; # imperative_verb adjective noun sub build_execution_profile_using; # imperative_verb adjective noun participle These naming rules particularly the two that put participles or prepositions at the ends of names create identifiers that read far more naturally, often eliminating the need for any additional comments.","title":"Identifiers"},{"location":"perl/best_practises/best_practises/","text":"Perl Best Practices Following best practices is a good way to create maintainable and readable code and should always be encouraged. However, learning what these best practices are and when they apply in the context of your code can be hard to determine. Luckily, there are several tools to help guide you on your way. Literature We have already mentioned perldocs . Damian Conway's book Perl Best Practices is a very good reference. Code standards Perl Critic Perlcritic is a Perl source code analyzer. It is the executable front-end to the Perl::Critic engine, which attempts to identify awkward, hard to read, error-prone, or unconventional constructs in your code. Most of the rules are based on Damian Conway's book Perl Best Practices. Perl critic allows different degrees of severity. Severity values are integers ranging from 1 (least severe) to 5 (most severe) when analyzing your code. You can also use the severity names if you think it hard to remember the meaning of the integers (1 = brutal and 5 = gentle). The level is controlled with the '--severity' flag. There is also a verbose flag, to print more information about the identified deviations from the perl critic best practises. There are 11 levels of verbosity. Examples perlcritic --severity 4 --verbose 11 my_perl_script.pl Perl critic also has web interface to instantly analyze your code. Perl Tidy Perltidy is a Perl script which indents and reformats Perl scripts to make them easier to read. If you write Perl scripts, or spend much time reading them, you will probably find it useful. Perltidy is an excellent way to automate the code standardisation with minimum of effort. Examples perltidy somefile.pl This will produce a file somefile.pl.tdy containing the script reformatted using the default options, which approximate the style suggested in perlstyle(1). The source file somefile.pl is unchanged. perltidy -b -bext='/' file1.pl file2.pl Create backups of files and modify files in place. The backup files file1.pl.bak and file2.pl.bak will be deleted if there are no errors.","title":"What are perl's best practices?"},{"location":"perl/best_practises/best_practises/#perl-best-practices","text":"Following best practices is a good way to create maintainable and readable code and should always be encouraged. However, learning what these best practices are and when they apply in the context of your code can be hard to determine. Luckily, there are several tools to help guide you on your way.","title":"Perl Best Practices"},{"location":"perl/best_practises/best_practises/#literature","text":"We have already mentioned perldocs . Damian Conway's book Perl Best Practices is a very good reference.","title":"Literature"},{"location":"perl/best_practises/best_practises/#code-standards","text":"","title":"Code standards"},{"location":"perl/best_practises/best_practises/#perl-critic","text":"Perlcritic is a Perl source code analyzer. It is the executable front-end to the Perl::Critic engine, which attempts to identify awkward, hard to read, error-prone, or unconventional constructs in your code. Most of the rules are based on Damian Conway's book Perl Best Practices. Perl critic allows different degrees of severity. Severity values are integers ranging from 1 (least severe) to 5 (most severe) when analyzing your code. You can also use the severity names if you think it hard to remember the meaning of the integers (1 = brutal and 5 = gentle). The level is controlled with the '--severity' flag. There is also a verbose flag, to print more information about the identified deviations from the perl critic best practises. There are 11 levels of verbosity.","title":"Perl Critic"},{"location":"perl/best_practises/best_practises/#examples","text":"perlcritic --severity 4 --verbose 11 my_perl_script.pl Perl critic also has web interface to instantly analyze your code.","title":"Examples"},{"location":"perl/best_practises/best_practises/#perl-tidy","text":"Perltidy is a Perl script which indents and reformats Perl scripts to make them easier to read. If you write Perl scripts, or spend much time reading them, you will probably find it useful. Perltidy is an excellent way to automate the code standardisation with minimum of effort.","title":"Perl Tidy"},{"location":"perl/best_practises/best_practises/#examples_1","text":"perltidy somefile.pl This will produce a file somefile.pl.tdy containing the script reformatted using the default options, which approximate the style suggested in perlstyle(1). The source file somefile.pl is unchanged. perltidy -b -bext='/' file1.pl file2.pl Create backups of files and modify files in place. The backup files file1.pl.bak and file2.pl.bak will be deleted if there are no errors.","title":"Examples"},{"location":"perl/installation/perlbrew/","text":"Perlbrew Perlbrew is a tool to manage multiple perl installations. Allowing for testing your production code against different perl versions, while leaving the vendor perl alone. You can even run your programs against all installations of perl. Perl Install a specific perl version and use it as default. $ perlbrew install perl-5.26.0 $ perlbrew switch perl-5.26.0 Use a specific perl version in your current shell, run: $ perlbrew use perl-5.26.1 Cpanm Is a lightweigth CPAN client, which facilitates installing CPAN perl modules. It is a good idea to install it together with Perlbrew to always make them available across each your perlbrew perl installations i.e the CPANM library will change with the perlbrew switch command. Installing CPANM with perlbrew is done by this command: $ perlbrew install-cpanm Perlbrew makes it easy to create and switch between multiple libraries of CPAN modules. You can even have several libraries for the same distribution of perl. Here are some examples: ## To create a library named Basil to perl version 5.26.0 $ perlbrew lib create perl-5.26.0@Basil ## To switch library for that perl distribution permanently $ perlbrew switch perl-5.26.0@Basil ## If you have another library that you want to use for your current session $ perlbrew use perl-5.26.0@Manuel ## To delete your Manuel library $ perlbrew lib delete perl-5.26.0@Manuel ## You can view your perl distributions and libraries by running: $ perlbrew list If you need to reinstall a specific version of a cpanm library, run: $ cpanm --reinstall [your_cpanm_lib]@[version] More information on perlbrew is available on metacpan .","title":"How do I manage perl and cpanm?"},{"location":"perl/installation/perlbrew/#perlbrew","text":"Perlbrew is a tool to manage multiple perl installations. Allowing for testing your production code against different perl versions, while leaving the vendor perl alone. You can even run your programs against all installations of perl.","title":"Perlbrew"},{"location":"perl/installation/perlbrew/#perl","text":"Install a specific perl version and use it as default. $ perlbrew install perl-5.26.0 $ perlbrew switch perl-5.26.0 Use a specific perl version in your current shell, run: $ perlbrew use perl-5.26.1","title":"Perl"},{"location":"perl/installation/perlbrew/#cpanm","text":"Is a lightweigth CPAN client, which facilitates installing CPAN perl modules. It is a good idea to install it together with Perlbrew to always make them available across each your perlbrew perl installations i.e the CPANM library will change with the perlbrew switch command. Installing CPANM with perlbrew is done by this command: $ perlbrew install-cpanm Perlbrew makes it easy to create and switch between multiple libraries of CPAN modules. You can even have several libraries for the same distribution of perl. Here are some examples: ## To create a library named Basil to perl version 5.26.0 $ perlbrew lib create perl-5.26.0@Basil ## To switch library for that perl distribution permanently $ perlbrew switch perl-5.26.0@Basil ## If you have another library that you want to use for your current session $ perlbrew use perl-5.26.0@Manuel ## To delete your Manuel library $ perlbrew lib delete perl-5.26.0@Manuel ## You can view your perl distributions and libraries by running: $ perlbrew list If you need to reinstall a specific version of a cpanm library, run: $ cpanm --reinstall [your_cpanm_lib]@[version] More information on perlbrew is available on metacpan .","title":"Cpanm"},{"location":"publish/crontab/","text":"Crontab Cron is a unix tool to run programs at certain times. Automatically! How great is that? Cron saves this information into a file called crontab . Sometimes, cron and crontab are used interchangeably. This document outlines how to write the perfect crontab entry. TL;DR 0 0 * * * $CRONIC your-awesome-program >> your-awesome.log 2> >(tee -a your-awesome.log >&2) We use $CRONIC to make sure only errors are emailed. That's it! How does it work? Read on! Edit the crontab crontab -e This will drop you into a vim session. Crontab format \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 minute (0 - 59) \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 hour (0 - 23) \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 day of month (1 - 31) \u2502 \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 month (1 - 12) \u2502 \u2502 \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 day of week (0 - 6) (Sunday to Saturday; \u2502 \u2502 \u2502 \u2502 \u2502 7 is also Sunday on some systems) \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 * * * * * command to execute See Cron for more explanation on crontab's format. Errors! You really want to be notified when your automation goes wrong. This line will add an email address to the crontab 1 . MAILTO=clinical-logwatch@scilifelab.se This will mail any output made by a crontab started program to the above mentioned email address. Only errors! You don't want to be spammed with all output of your programs. You only want to be notified when something goes wrong. What does it mean \"to go wrong\"? On the highest level, this mean when a program exits with a non-zero exit code. To make sure we only get emailed with the full output of our program when the program exits with a non-zero exit code, we make use of cronic . Cronic will capture all output of your program, including everything printed to stderr. If all goes well, all that captured output will be printed to stdout, including everything that was caught from stderr. Only when the program exits with a non-zero exit status will everything be printed to stderr. Cronic How can we make use of this nifty feature? Like this: cronic your-awesome-program >> your-awesome.log All output to stdout will be caught by your log (all went well!). All output to stderr will be caught by crontab which will send it by email if MAILTO is set (program exited with non-zero exit code). Logging What if you also want to log all output, even when something goes wrong? Then comes in some bash magic 1 SHELL=/bin/bash cronic your-awesome-program >> your-awesome.log 2> >(tee -a your-awesome.log >&2) Here we add a stderr redirect ( 2> ) to the program tee . Tee captures all stdin (through >(tee ...) process substitution ), appends it to your-awesome.log, and prints stdin back to stdout. Stdout gets redirected to stderr ( >&2 ). And remember: stderr is sent to crontab, which will mail the content to MAILTO if set. $CRONIC Lastly, you do not want to write the full path to cronic for each crontab entry. Create a bash variable at the start of the crontab, which you then can use in your crontab entry: 1 CRONIC=~/server/resources/cronic 0 0 * * * $CRONIC your-awesome-program >> your-awesome.log 2> >(tee -a your-awesome.log >&2) Please be aware that in most certainty, the shell variables SHELL , MAILTO , and CRONIC will already by set at the top of the server's crontab. \u21a9 \u21a9 \u21a9","title":"How to automate running your tool?"},{"location":"publish/crontab/#crontab","text":"Cron is a unix tool to run programs at certain times. Automatically! How great is that? Cron saves this information into a file called crontab . Sometimes, cron and crontab are used interchangeably. This document outlines how to write the perfect crontab entry.","title":"Crontab"},{"location":"publish/crontab/#tldr","text":"0 0 * * * $CRONIC your-awesome-program >> your-awesome.log 2> >(tee -a your-awesome.log >&2) We use $CRONIC to make sure only errors are emailed. That's it! How does it work? Read on!","title":"TL;DR"},{"location":"publish/crontab/#edit-the-crontab","text":"crontab -e This will drop you into a vim session.","title":"Edit the crontab"},{"location":"publish/crontab/#crontab-format","text":"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 minute (0 - 59) \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 hour (0 - 23) \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 day of month (1 - 31) \u2502 \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 month (1 - 12) \u2502 \u2502 \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 day of week (0 - 6) (Sunday to Saturday; \u2502 \u2502 \u2502 \u2502 \u2502 7 is also Sunday on some systems) \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 * * * * * command to execute See Cron for more explanation on crontab's format.","title":"Crontab format"},{"location":"publish/crontab/#errors","text":"You really want to be notified when your automation goes wrong. This line will add an email address to the crontab 1 . MAILTO=clinical-logwatch@scilifelab.se This will mail any output made by a crontab started program to the above mentioned email address.","title":"Errors!"},{"location":"publish/crontab/#only-errors","text":"You don't want to be spammed with all output of your programs. You only want to be notified when something goes wrong. What does it mean \"to go wrong\"? On the highest level, this mean when a program exits with a non-zero exit code. To make sure we only get emailed with the full output of our program when the program exits with a non-zero exit code, we make use of cronic . Cronic will capture all output of your program, including everything printed to stderr. If all goes well, all that captured output will be printed to stdout, including everything that was caught from stderr. Only when the program exits with a non-zero exit status will everything be printed to stderr.","title":"Only errors!"},{"location":"publish/crontab/#cronic","text":"How can we make use of this nifty feature? Like this: cronic your-awesome-program >> your-awesome.log All output to stdout will be caught by your log (all went well!). All output to stderr will be caught by crontab which will send it by email if MAILTO is set (program exited with non-zero exit code).","title":"Cronic"},{"location":"publish/crontab/#logging","text":"What if you also want to log all output, even when something goes wrong? Then comes in some bash magic 1 SHELL=/bin/bash cronic your-awesome-program >> your-awesome.log 2> >(tee -a your-awesome.log >&2) Here we add a stderr redirect ( 2> ) to the program tee . Tee captures all stdin (through >(tee ...) process substitution ), appends it to your-awesome.log, and prints stdin back to stdout. Stdout gets redirected to stderr ( >&2 ). And remember: stderr is sent to crontab, which will mail the content to MAILTO if set.","title":"Logging"},{"location":"publish/crontab/#cronic_1","text":"Lastly, you do not want to write the full path to cronic for each crontab entry. Create a bash variable at the start of the crontab, which you then can use in your crontab entry: 1 CRONIC=~/server/resources/cronic 0 0 * * * $CRONIC your-awesome-program >> your-awesome.log 2> >(tee -a your-awesome.log >&2) Please be aware that in most certainty, the shell variables SHELL , MAILTO , and CRONIC will already by set at the top of the server's crontab. \u21a9 \u21a9 \u21a9","title":"$CRONIC"},{"location":"publish/prod/","text":"How to install your tool in production? Your code is ready for deployment. Well done bucko! Now what do I do? (all scripts are in ${PRODUCTION_HOME}/servers/resources ) Kick off a discussion by sending a pull request from your branch. Make changes on your branch as needed. Your pull request will update automatically. Ask for a pull request review Test the PR on stage by deploying your branch Do this with your reviewer Announce on TTNs daily standup Delete current stage Clone prod to stage: condacopy-prod-to-stage.sh Switch to stage: usestage Install branch into stage with the tool's update script: update-tool-stage.sh If it passes, merge the PR into master Add a small risk assessment to the merge message describing the potential impact on existing functionality Remove or rewrite any nonsense in the merge message into descriptive text Delete the branch Delete the development branch in conda Bumpversion on master Test the installation of the new master on stage But only test that the installation succeeds If it passes, deploy on prod! Announce on stand up Take backup of prod: e.g. savetheconda prod170926 Install with the tool's update script: update-tool-prod.sh","title":"How to publish your tool?"},{"location":"publish/prod/#how-to-install-your-tool-in-production","text":"Your code is ready for deployment. Well done bucko! Now what do I do? (all scripts are in ${PRODUCTION_HOME}/servers/resources ) Kick off a discussion by sending a pull request from your branch. Make changes on your branch as needed. Your pull request will update automatically. Ask for a pull request review Test the PR on stage by deploying your branch Do this with your reviewer Announce on TTNs daily standup Delete current stage Clone prod to stage: condacopy-prod-to-stage.sh Switch to stage: usestage Install branch into stage with the tool's update script: update-tool-stage.sh If it passes, merge the PR into master Add a small risk assessment to the merge message describing the potential impact on existing functionality Remove or rewrite any nonsense in the merge message into descriptive text Delete the branch Delete the development branch in conda Bumpversion on master Test the installation of the new master on stage But only test that the installation succeeds If it passes, deploy on prod! Announce on stand up Take backup of prod: e.g. savetheconda prod170926 Install with the tool's update script: update-tool-prod.sh","title":"How to install your tool in production?"},{"location":"publish/servers/","text":"Servers All you need to know on how to set up our servers. Servers configurations and resources can be found in the servers repo. If you have changed something in that repo and want to deploy it you proceed much like with any other tool: Create a PR Get the PR reviewed and approved ... but don't merge yet Get someone to review your code Is the code adhering to the language standards? Did any signatures of existing functions or methods change? Test the PR on stage by deploying your branch Do this with your reviewer Announce on TTNs daily standup Install branch into stage with the tool's update script: update-servers-stage.sh If it passes, merge the PR into master Remove or rewrite any nonsense in the merge message into descriptive text Delete the branch Deploy on prod! Announce on stand up Install with the tool's update script: update-servers-prod.sh Please note that some servers has no stage environment, then do for production what you would otherwise do for stage and then skip the rest.","title":"How to update configuration?"},{"location":"publish/servers/#servers","text":"All you need to know on how to set up our servers. Servers configurations and resources can be found in the servers repo. If you have changed something in that repo and want to deploy it you proceed much like with any other tool: Create a PR Get the PR reviewed and approved ... but don't merge yet Get someone to review your code Is the code adhering to the language standards? Did any signatures of existing functions or methods change? Test the PR on stage by deploying your branch Do this with your reviewer Announce on TTNs daily standup Install branch into stage with the tool's update script: update-servers-stage.sh If it passes, merge the PR into master Remove or rewrite any nonsense in the merge message into descriptive text Delete the branch Deploy on prod! Announce on stand up Install with the tool's update script: update-servers-prod.sh Please note that some servers has no stage environment, then do for production what you would otherwise do for stage and then skip the rest.","title":"Servers"},{"location":"publish/sign-off/","text":"When do you need a sign-off and who can sign off on tests, code reviews and deploys? All tools developed at clinical genomics need to be tested, reviewed and deployed according to a specific routine which is generally outlined under \"How to publish your tool\" . These three things also need a sign-off by an authorized person. For tools used in production this is especially important. There are two development models used at Clinical Genomics and depending on which you use the sign-offs happen at slightly different steps in the update process. Github flow In this model the sign-offs all happen in the pull request. The developer will as part of the initial pull request add two check boxes to the description; one for code review and one for functional testing. The authorized person(s) doing the test and review then ticks the boxes as the code has been checked and tested on stage according to the test case described in the pull request. The person ticking the last box also approves the pull request. Only when the boxes are ticked, the authorized person(s) have added their thumbs up and the pull request has been approved the decision to merge and deploy is made. The person(s) authorized to sign-off on this then adds the go-ahead as a comment in the pull request and merges the branch to master (or other source). This process is better described in \"How to publish your tool\" . People authorized to sign-off on code reviews Code owner(s) of the specific repository People authorized to sign-off on functional tests Clinical Genomics employee People authorized to sign-off on merge and deploy Tools in production - Team leader IT (see AM document 1047 Functions and signature list ) Tools in development - Code owner(s) of the specific repository Git flow In this model several pull requests are bunched in the same release and it is therefore necessary to do the sign-offs in three steps: First pull request to add changes to the feature branch - sign-offs needed are on code review, functional testing and merge as described under Github flow. AM Test specification and implementation plan - sign-offs needed on system testing (testing of the whole development or release branch to be deployed) and decision to merge to master and deploy the code. Test specifications and implementation plan templates are available in AM ( 1249 Test Specification - Template and 1187 Implementation plan - template ) Second pull request to merge into master - confirm the sign-offs in the AM test specification and implementation plan. People authorized to sign-off on code reviews Code owner(s) of the specific repository People authorized to sign-off on functional tests Test specification and implementation plan - Management team member, Facility manager and Quality control manager (see AM document 1047 Functions and signature list ) Pull request - Code owner(s) to confirm test specification and implementation plan People authorized to sign-off on merge and deploy Tools in production - Team leader IT (see AM document 1047 Functions and signature list ) Tools in development - Code owner(s) of the specific repository","title":"Who can sign off on tests, reviews and deploys?"},{"location":"publish/sign-off/#when-do-you-need-a-sign-off-and-who-can-sign-off-on-tests-code-reviews-and-deploys","text":"All tools developed at clinical genomics need to be tested, reviewed and deployed according to a specific routine which is generally outlined under \"How to publish your tool\" . These three things also need a sign-off by an authorized person. For tools used in production this is especially important. There are two development models used at Clinical Genomics and depending on which you use the sign-offs happen at slightly different steps in the update process.","title":"When do you need a sign-off and who can sign off on tests, code reviews and deploys?"},{"location":"publish/sign-off/#github-flow","text":"In this model the sign-offs all happen in the pull request. The developer will as part of the initial pull request add two check boxes to the description; one for code review and one for functional testing. The authorized person(s) doing the test and review then ticks the boxes as the code has been checked and tested on stage according to the test case described in the pull request. The person ticking the last box also approves the pull request. Only when the boxes are ticked, the authorized person(s) have added their thumbs up and the pull request has been approved the decision to merge and deploy is made. The person(s) authorized to sign-off on this then adds the go-ahead as a comment in the pull request and merges the branch to master (or other source). This process is better described in \"How to publish your tool\" .","title":"Github flow"},{"location":"publish/sign-off/#people-authorized-to-sign-off-on-code-reviews","text":"Code owner(s) of the specific repository","title":"People authorized to sign-off on code reviews"},{"location":"publish/sign-off/#people-authorized-to-sign-off-on-functional-tests","text":"Clinical Genomics employee","title":"People authorized to sign-off on functional tests"},{"location":"publish/sign-off/#people-authorized-to-sign-off-on-merge-and-deploy","text":"Tools in production - Team leader IT (see AM document 1047 Functions and signature list ) Tools in development - Code owner(s) of the specific repository","title":"People authorized to sign-off on merge and deploy"},{"location":"publish/sign-off/#git-flow","text":"In this model several pull requests are bunched in the same release and it is therefore necessary to do the sign-offs in three steps: First pull request to add changes to the feature branch - sign-offs needed are on code review, functional testing and merge as described under Github flow. AM Test specification and implementation plan - sign-offs needed on system testing (testing of the whole development or release branch to be deployed) and decision to merge to master and deploy the code. Test specifications and implementation plan templates are available in AM ( 1249 Test Specification - Template and 1187 Implementation plan - template ) Second pull request to merge into master - confirm the sign-offs in the AM test specification and implementation plan.","title":"Git flow"},{"location":"publish/sign-off/#people-authorized-to-sign-off-on-code-reviews_1","text":"Code owner(s) of the specific repository","title":"People authorized to sign-off on code reviews"},{"location":"publish/sign-off/#people-authorized-to-sign-off-on-functional-tests_1","text":"Test specification and implementation plan - Management team member, Facility manager and Quality control manager (see AM document 1047 Functions and signature list ) Pull request - Code owner(s) to confirm test specification and implementation plan","title":"People authorized to sign-off on functional tests"},{"location":"publish/sign-off/#people-authorized-to-sign-off-on-merge-and-deploy_1","text":"Tools in production - Team leader IT (see AM document 1047 Functions and signature list ) Tools in development - Code owner(s) of the specific repository","title":"People authorized to sign-off on merge and deploy"},{"location":"publish/update-scripts/","text":"How to easily update your tool? You will want to create two scripts, at least: one to update your tool on stage one to update your tool on prod Yes, I just wrote that out in full. It's that important! Stage CLI update script How does such a script look like. Well, let's have a look at one that closest resembles a template, update-trailblazer-stage.sh #!/usr/bin/env bash # exit on error set -e # make sure we run as hiseq.clinical sh ./assert_user.sh hiseq.clinical # make sure we run on rasta. Leave this assert out if this script can be run on all servers. sh ./assert_host.sh rastapopoulos.scilifelab.se # One optional argument BRANCH=${1} # make sure we can have access to aliases shopt -s expand_aliases # make sure we are closely resembling production source ~/.bashrc # go to stage. Yes, this conda env should already exist source activate stage # install with latest changes on \"master\" pip install -U git+https://github.com/Clinical-Genomics/trailblazer@$BRANCH You run it: cd ~/servers/resources sh update-trailblazer-stage.sh Or you update stage to a specific branch: cd ~/servers/resources sh update-trailblazer-stage.sh beta Web update script For web frontend, one can add everything you need to update a staging env: #!/usr/bin/env bash # exit on error set -e # make sure we run as hiseq.clinical sh ./assert_user.sh hiseq.clinical # make sure we run on clinical-db sh ./assert_host.sh clinical-db.scilifelab.se # One optional argument, prefilled BRANCH=${1-master} # make sure we can have access to aliases shopt -s expand_aliases # make sure we are closely resembling production source ~/.bashrc # go to stage. Yes, this conda env should already exist source activate stage # trailblazer-stage is already installed in a predefined location. The repo should already be cloned cd ~/STAGE/trailblazer # Update! git fetch git checkout $BRANCH git pull # install and update pip dependencies pip install -U --editable . # restart the Flask service supervisorctl restart trailblazer-stage supervisorctl restart trailblazer-api-stage # Drop you back to the initial directory cd - Naming of scripts The suggestion is to name your script: update-$component-stage.sh with $component the name of your component, be it trailblazer for the cli or trailblazer-ui for the web. What about config files? Premade config files for all packages used in production and stage have been made in the config dir of the servers repo. This can be found on rasta and clinical-db in the same location: cd ~/servers/config All config files point to the stage location of the package, if any, to lims-stage, and to the staging databases. What about databases? MySQL All databases are located on clinical-db. To make sure you have the latest uncorrupted data, you can make a copy of one or all databases like this: On clinical-db, copying all databases: cd ~/servers/resources bash dbcopy-prod-to-stage.sh On clinical-db, copy one database: cd ~/servers/resources bash dbcopy-prod-to-stage.sh trailblazer The credentials to the stage databases have already been set in the stage configs. Mongo All mongodbs are located on clinical-db. To copy both scout and loqusdb, you can issue: cd ~/servers/resources bash dbcopy-mongoprod-to-stage.sh This will reinstate a backuped version of scout (dated: 2018-05-18) and a fresh copy of loqusdb. Please be aware that this process takes several days to complete! The mongo-stage server is not running by default. To start the mongo-stage run: cd ~/servers/resources bash start-mongo-stage.sh & So, what is beta? The beta environment, for now, is not an environment at all. It is a bunch of branches in git for cg and trailblazer that are affected by the MIP6/Scout4 update. To update stage to use those branches, run Rasta: bash update-beta.sh","title":"How make updating your tool easy?"},{"location":"publish/update-scripts/#how-to-easily-update-your-tool","text":"You will want to create two scripts, at least: one to update your tool on stage one to update your tool on prod Yes, I just wrote that out in full. It's that important!","title":"How to easily update your tool?"},{"location":"publish/update-scripts/#stage-cli-update-script","text":"How does such a script look like. Well, let's have a look at one that closest resembles a template, update-trailblazer-stage.sh #!/usr/bin/env bash # exit on error set -e # make sure we run as hiseq.clinical sh ./assert_user.sh hiseq.clinical # make sure we run on rasta. Leave this assert out if this script can be run on all servers. sh ./assert_host.sh rastapopoulos.scilifelab.se # One optional argument BRANCH=${1} # make sure we can have access to aliases shopt -s expand_aliases # make sure we are closely resembling production source ~/.bashrc # go to stage. Yes, this conda env should already exist source activate stage # install with latest changes on \"master\" pip install -U git+https://github.com/Clinical-Genomics/trailblazer@$BRANCH You run it: cd ~/servers/resources sh update-trailblazer-stage.sh Or you update stage to a specific branch: cd ~/servers/resources sh update-trailblazer-stage.sh beta","title":"Stage CLI update script"},{"location":"publish/update-scripts/#web-update-script","text":"For web frontend, one can add everything you need to update a staging env: #!/usr/bin/env bash # exit on error set -e # make sure we run as hiseq.clinical sh ./assert_user.sh hiseq.clinical # make sure we run on clinical-db sh ./assert_host.sh clinical-db.scilifelab.se # One optional argument, prefilled BRANCH=${1-master} # make sure we can have access to aliases shopt -s expand_aliases # make sure we are closely resembling production source ~/.bashrc # go to stage. Yes, this conda env should already exist source activate stage # trailblazer-stage is already installed in a predefined location. The repo should already be cloned cd ~/STAGE/trailblazer # Update! git fetch git checkout $BRANCH git pull # install and update pip dependencies pip install -U --editable . # restart the Flask service supervisorctl restart trailblazer-stage supervisorctl restart trailblazer-api-stage # Drop you back to the initial directory cd -","title":"Web update script"},{"location":"publish/update-scripts/#naming-of-scripts","text":"The suggestion is to name your script: update-$component-stage.sh with $component the name of your component, be it trailblazer for the cli or trailblazer-ui for the web.","title":"Naming of scripts"},{"location":"publish/update-scripts/#what-about-config-files","text":"Premade config files for all packages used in production and stage have been made in the config dir of the servers repo. This can be found on rasta and clinical-db in the same location: cd ~/servers/config All config files point to the stage location of the package, if any, to lims-stage, and to the staging databases.","title":"What about config files?"},{"location":"publish/update-scripts/#what-about-databases","text":"","title":"What about databases?"},{"location":"publish/update-scripts/#mysql","text":"All databases are located on clinical-db. To make sure you have the latest uncorrupted data, you can make a copy of one or all databases like this: On clinical-db, copying all databases: cd ~/servers/resources bash dbcopy-prod-to-stage.sh On clinical-db, copy one database: cd ~/servers/resources bash dbcopy-prod-to-stage.sh trailblazer The credentials to the stage databases have already been set in the stage configs.","title":"MySQL"},{"location":"publish/update-scripts/#mongo","text":"All mongodbs are located on clinical-db. To copy both scout and loqusdb, you can issue: cd ~/servers/resources bash dbcopy-mongoprod-to-stage.sh This will reinstate a backuped version of scout (dated: 2018-05-18) and a fresh copy of loqusdb. Please be aware that this process takes several days to complete! The mongo-stage server is not running by default. To start the mongo-stage run: cd ~/servers/resources bash start-mongo-stage.sh &","title":"Mongo"},{"location":"publish/update-scripts/#so-what-is-beta","text":"The beta environment, for now, is not an environment at all. It is a bunch of branches in git for cg and trailblazer that are affected by the MIP6/Scout4 update. To update stage to use those branches, run Rasta: bash update-beta.sh","title":"So, what is beta?"},{"location":"python/","text":"Python Python is a popular and easy-to-learn interpreted programming language. To get a feel for the language, take a look at this comprehensive reference . With the surge in adoption of the most recent version of Python we are encouraging all new code to support Python 3 first and foremost . Separate topics Testing Logging Packaging Style guide We generally follow PEP8. Written code should pass linting using PyLint . Ignored rules include: E501: \"wrap lines at 80 characters\" => we use a 100 character limit Linting code A linter is a great way to ensure you are writing code of good quality; following best practices and without avoidable errors relating to missing imports and misspelled variable names. We recommend PyLint for working with Python code. You can set it up to run in the background when you edit code. Documenting code We try to make comments and document what we do as much as possible, rather more than less. At least make proper docstrings that explains the logic. The reasoning should be that if I read the docstring I should not have to look at the code. Try to document the following: Module documentation: At the top of the file document in broad sentences Docstring: What is the purpose of class/function. Input/Output Inline comments: Why this if? Why this variable etc.. We try to follow googles docstring convention Packaging Packaging Python code is a known pain point that many developers struggle with. There are some moves to standardize the experience but for now a great place to start would be to follow this guide . It will give you an idea of the minimal structure you need and beyond. We also have our own guide to packaging here For detailed information or if you need to look up specific options there a very detailed resource available as well. We should be using Pipfile, pipenv. Uploading a package to Pypi In order to be able to install a Python package directly from the pip command, or from the requirements file of another software package, it is necessary to upload the package to PyPI . A basic tutorial on how to do that is available here . Conda setup See conda . Awesome Python A curated list of awesome Python tools and libraries! Command line interface Click : composable and Flask-like CLI framework Halo : beautiful terminal spinners Testing Py.test Py.test Coverage : easily integrate test coverage with Py.test Py.test Flask : easily integrate Py.test and Flask All plugin-ins : list of all Py.test plugins! Suggested command: bash py.test --cov-report html --cov \"$(basename \"$PWD\")\" --verbose --color=yes tests/ Logging Coloredlogs : super simple setup for colorized logging","title":"Python"},{"location":"python/#python","text":"Python is a popular and easy-to-learn interpreted programming language. To get a feel for the language, take a look at this comprehensive reference . With the surge in adoption of the most recent version of Python we are encouraging all new code to support Python 3 first and foremost .","title":"Python"},{"location":"python/#separate-topics","text":"Testing Logging Packaging","title":"Separate topics"},{"location":"python/#style-guide","text":"We generally follow PEP8. Written code should pass linting using PyLint . Ignored rules include: E501: \"wrap lines at 80 characters\" => we use a 100 character limit","title":"Style guide"},{"location":"python/#linting-code","text":"A linter is a great way to ensure you are writing code of good quality; following best practices and without avoidable errors relating to missing imports and misspelled variable names. We recommend PyLint for working with Python code. You can set it up to run in the background when you edit code.","title":"Linting code"},{"location":"python/#documenting-code","text":"We try to make comments and document what we do as much as possible, rather more than less. At least make proper docstrings that explains the logic. The reasoning should be that if I read the docstring I should not have to look at the code. Try to document the following: Module documentation: At the top of the file document in broad sentences Docstring: What is the purpose of class/function. Input/Output Inline comments: Why this if? Why this variable etc.. We try to follow googles docstring convention","title":"Documenting code"},{"location":"python/#packaging","text":"Packaging Python code is a known pain point that many developers struggle with. There are some moves to standardize the experience but for now a great place to start would be to follow this guide . It will give you an idea of the minimal structure you need and beyond. We also have our own guide to packaging here For detailed information or if you need to look up specific options there a very detailed resource available as well. We should be using Pipfile, pipenv.","title":"Packaging"},{"location":"python/#uploading-a-package-to-pypi","text":"In order to be able to install a Python package directly from the pip command, or from the requirements file of another software package, it is necessary to upload the package to PyPI . A basic tutorial on how to do that is available here .","title":"Uploading a package to Pypi"},{"location":"python/#conda-setup","text":"See conda .","title":"Conda setup"},{"location":"python/#awesome-python","text":"A curated list of awesome Python tools and libraries! Command line interface Click : composable and Flask-like CLI framework Halo : beautiful terminal spinners Testing Py.test Py.test Coverage : easily integrate test coverage with Py.test Py.test Flask : easily integrate Py.test and Flask All plugin-ins : list of all Py.test plugins! Suggested command: bash py.test --cov-report html --cov \"$(basename \"$PWD\")\" --verbose --color=yes tests/ Logging Coloredlogs : super simple setup for colorized logging","title":"Awesome Python"},{"location":"python/conventions/","text":"Conventions Code is read much more often than it is written. - A Wise Pythonista PEP8 and \ud83d\udc8e PEP20 ( import this ) should be considered as the starting points. Unless specified otherwise, we follow the guidelines laid out there. For a brief overview, read the Python Guide 's entry on the subject. The official guide has some very nice tips on code layout . This can be a nice reference when you are unsure about how to e.g. style your hanging indents. Futhermore, we use Python 3.6+. We encourage the use of type annotations (even if it doesn't cover 100% of your functions) and the new format strings: name = 'Paul Anderson' string = f\"Hello {name}!\" Indentation Use 4 spaces to indent code. Never use hard tabs. Spaces are preferred over tabs for the following reason: spaces are spaces on every editor on every operating system. Tabs can be configured to act as 2, 4, 8 \"spaces\" and this can make code unreadable when being shared. Maximum line length: 100 chars Limit all lines to a maximum of 100 characters. Break down the line if it exceeds the maximum length. For example: # Python automatically concatenates consecutive strings a_long_string = (\"I am a very long string that can be split across\" \"multiple lines by automatic string concatenation.\") # multi conditional if-statements can be split into multiple statements # notice how the code reads a lot like regular English! first_condition = 'genius' in 'Paul Thomas Anderson' second_condition = 'genius' in 'Daniel Day-Lewis' if first_condition and second_condition: print('And the Oscar goes to... There Will Be Blood!') Imports I like to split up imports into three separate groups: 1. standard library imports, 2. third party imports, 3. intra-package imports. import os import re import numpy as np from path import path from .utils import read_config Variable names PEP8 recommends to use short variable names. Achieving that can take some coding hygiene and some experience. Get more advice in our variable hygiene section . Folders/packages A common folder structure for a Python projects looks like: myPackage \u251c\u2500\u2500 __init__.py \u251c\u2500\u2500 exc.py <-- custom exceptions \u251c\u2500\u2500 cli/ \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u251c\u2500\u2500 base.py <-- read app config, set up logging, import subcommands \u2502 \u2514\u2500\u2500 subcommand.py \u251c\u2500\u2500 server/ \u2502 \u251c\u2500\u2500 templates/ \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u2514\u2500\u2500 app.py \u2514\u2500\u2500 store/ \u251c\u2500\u2500 __init__.py \u251c\u2500\u2500 api.py \u2514\u2500\u2500 models.py <-- database models, DB schema definitions The important thing to note is how functionality is split up across the project. It's common that the same or similar functionality is available through many interfaces. For example it might be possible to list all recently added records in a database: on the command line printed to the console over HTTP through a JSON API programmatically through accessing an API class The cli package contains all command line related code and nothing more. Logic such as how to handle and report error messages to the user goes here. Avoid putting logic in CLI modules that directly handle things like manually filtering database queries. More generally think DRY - avoid doing the same thing manually in both CLI and server code. Always go through your Python API to interact with e.g. the database. Command line interface (CLI) We highly recommend implementing the CLI using Click . It has a very nice Pythonic interface for defining your interface and comes with helpers for reading input, printing colorful output, etc. Logging We recommend using Coloredlogs which makes it super easy to enable logging for your package: import coloredlogs coloredlogs.install(level=log_level) Read more in the logging section Server If it makes sense, a general app exposes a JSON API over HTTP(S). This API can then be consumed by a client side web app. Sometimes it makes sense to provide a the web interface as part of the Python package and other times it will be consumed by the central web portal interface. We write our web servers in Flask which shares many similarities with Click . To connect to the store/backend we use Flask-Alchy . We generally configure servers using environment variables which is a flexible and well supported option. Store We start out with a SQL backend unless our needs require something different. We run all production SQL databases in MySQL. For development and testing it's often nice to work against a SQLite copy. For that reason and to make database interactions more pythonic, we make heavy use of the SQLAlchemy-wrapper, Alchy . Note that Alchy has been deprecated in favor of SQLService, however, we have not been able to successfully set up the latter package with Flask to work with the request context which is why we stay with Alchy for now. Store API Alchy shares the database connection with associated model classes. This allows you to perform queries by simply importing them like: # ... connect to database from trailblazer.store import models for record in models.Sample.query: print(record) However, this feels a little magic and it's not so clear how the model exactly got access to that database connection. Therefore, we advocate explicitly passing around an API class holding the database connection. from . import models class DatabaseAPI(): Sample = models.Sample def __init__(self, uri): self.connect(uri) def samples(self): return self.Sample.query Exceptions We recommend that you base all your custom exceptions on a app-specific exception class. This way, 3-party packages that depend on it easily catch all exceptions from your package if they so choose. class TrailblazerError(Exception): def __init__(self, message): self.message = message class MissingFileError(TrailblazerError): pass Helpful tools We encourage using a linter to continuously check the syntax of your code. The Python community maintains a number of linters such as pep8, and PyFlakes . If your repo has been set up with continous integration, with for example Travis, you can easily make linting part of the build process together with Git-Lint . This will give you step wise improvements by automatic linting of those parts of the code that are new or changed. Another resource that can greatly ease collaboration is EditorConfig . It's an effort to provide a cross editor configuration format and in placed in your project and committed to source control. You also need to install a plugin for your favorite editor, e.g. Sublime .","title":"conventions"},{"location":"python/conventions/#conventions","text":"Code is read much more often than it is written. - A Wise Pythonista PEP8 and \ud83d\udc8e PEP20 ( import this ) should be considered as the starting points. Unless specified otherwise, we follow the guidelines laid out there. For a brief overview, read the Python Guide 's entry on the subject. The official guide has some very nice tips on code layout . This can be a nice reference when you are unsure about how to e.g. style your hanging indents. Futhermore, we use Python 3.6+. We encourage the use of type annotations (even if it doesn't cover 100% of your functions) and the new format strings: name = 'Paul Anderson' string = f\"Hello {name}!\"","title":"Conventions"},{"location":"python/conventions/#indentation","text":"Use 4 spaces to indent code. Never use hard tabs. Spaces are preferred over tabs for the following reason: spaces are spaces on every editor on every operating system. Tabs can be configured to act as 2, 4, 8 \"spaces\" and this can make code unreadable when being shared.","title":"Indentation"},{"location":"python/conventions/#maximum-line-length-100-chars","text":"Limit all lines to a maximum of 100 characters. Break down the line if it exceeds the maximum length. For example: # Python automatically concatenates consecutive strings a_long_string = (\"I am a very long string that can be split across\" \"multiple lines by automatic string concatenation.\") # multi conditional if-statements can be split into multiple statements # notice how the code reads a lot like regular English! first_condition = 'genius' in 'Paul Thomas Anderson' second_condition = 'genius' in 'Daniel Day-Lewis' if first_condition and second_condition: print('And the Oscar goes to... There Will Be Blood!')","title":"Maximum line length: 100 chars"},{"location":"python/conventions/#imports","text":"I like to split up imports into three separate groups: 1. standard library imports, 2. third party imports, 3. intra-package imports. import os import re import numpy as np from path import path from .utils import read_config","title":"Imports"},{"location":"python/conventions/#variable-names","text":"PEP8 recommends to use short variable names. Achieving that can take some coding hygiene and some experience. Get more advice in our variable hygiene section .","title":"Variable names"},{"location":"python/conventions/#folderspackages","text":"A common folder structure for a Python projects looks like: myPackage \u251c\u2500\u2500 __init__.py \u251c\u2500\u2500 exc.py <-- custom exceptions \u251c\u2500\u2500 cli/ \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u251c\u2500\u2500 base.py <-- read app config, set up logging, import subcommands \u2502 \u2514\u2500\u2500 subcommand.py \u251c\u2500\u2500 server/ \u2502 \u251c\u2500\u2500 templates/ \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u2514\u2500\u2500 app.py \u2514\u2500\u2500 store/ \u251c\u2500\u2500 __init__.py \u251c\u2500\u2500 api.py \u2514\u2500\u2500 models.py <-- database models, DB schema definitions The important thing to note is how functionality is split up across the project. It's common that the same or similar functionality is available through many interfaces. For example it might be possible to list all recently added records in a database: on the command line printed to the console over HTTP through a JSON API programmatically through accessing an API class The cli package contains all command line related code and nothing more. Logic such as how to handle and report error messages to the user goes here. Avoid putting logic in CLI modules that directly handle things like manually filtering database queries. More generally think DRY - avoid doing the same thing manually in both CLI and server code. Always go through your Python API to interact with e.g. the database.","title":"Folders/packages"},{"location":"python/conventions/#command-line-interface-cli","text":"We highly recommend implementing the CLI using Click . It has a very nice Pythonic interface for defining your interface and comes with helpers for reading input, printing colorful output, etc.","title":"Command line interface (CLI)"},{"location":"python/conventions/#logging","text":"We recommend using Coloredlogs which makes it super easy to enable logging for your package: import coloredlogs coloredlogs.install(level=log_level) Read more in the logging section","title":"Logging"},{"location":"python/conventions/#server","text":"If it makes sense, a general app exposes a JSON API over HTTP(S). This API can then be consumed by a client side web app. Sometimes it makes sense to provide a the web interface as part of the Python package and other times it will be consumed by the central web portal interface. We write our web servers in Flask which shares many similarities with Click . To connect to the store/backend we use Flask-Alchy . We generally configure servers using environment variables which is a flexible and well supported option.","title":"Server"},{"location":"python/conventions/#store","text":"We start out with a SQL backend unless our needs require something different. We run all production SQL databases in MySQL. For development and testing it's often nice to work against a SQLite copy. For that reason and to make database interactions more pythonic, we make heavy use of the SQLAlchemy-wrapper, Alchy . Note that Alchy has been deprecated in favor of SQLService, however, we have not been able to successfully set up the latter package with Flask to work with the request context which is why we stay with Alchy for now.","title":"Store"},{"location":"python/conventions/#store-api","text":"Alchy shares the database connection with associated model classes. This allows you to perform queries by simply importing them like: # ... connect to database from trailblazer.store import models for record in models.Sample.query: print(record) However, this feels a little magic and it's not so clear how the model exactly got access to that database connection. Therefore, we advocate explicitly passing around an API class holding the database connection. from . import models class DatabaseAPI(): Sample = models.Sample def __init__(self, uri): self.connect(uri) def samples(self): return self.Sample.query","title":"Store API"},{"location":"python/conventions/#exceptions","text":"We recommend that you base all your custom exceptions on a app-specific exception class. This way, 3-party packages that depend on it easily catch all exceptions from your package if they so choose. class TrailblazerError(Exception): def __init__(self, message): self.message = message class MissingFileError(TrailblazerError): pass","title":"Exceptions"},{"location":"python/conventions/#helpful-tools","text":"We encourage using a linter to continuously check the syntax of your code. The Python community maintains a number of linters such as pep8, and PyFlakes . If your repo has been set up with continous integration, with for example Travis, you can easily make linting part of the build process together with Git-Lint . This will give you step wise improvements by automatic linting of those parts of the code that are new or changed. Another resource that can greatly ease collaboration is EditorConfig . It's an effort to provide a cross editor configuration format and in placed in your project and committed to source control. You also need to install a plugin for your favorite editor, e.g. Sublime .","title":"Helpful tools"},{"location":"python/logging/","text":"Logging Logging is the logical next step when you realize the limitations of printing. Python includes a very useful logging module in the standard library so it's rather easy to get started. First make sure you know the basics . Now when you want to log some progress or program state in a module: import logging LOG = logging.getLogger(__name__) def foo(bar): \"\"\"My fancy function.\"\"\" LOG.info(\"incrementing the input: %s\", bar) return bar + 1 The not-so-intuitive part is that you need to configure the logging module to see any output from these calls. Luckily we can reduce the setup to a simple function call most of the time using the coloredlogs package. During e.g. your CLI initialization include: import click import coloredlogs import logging LOG = logging.getLogger(__name__) @click.command() @click.option('-l', '--log-level', default='INFO', help='Log message level to display') def cli(log_level): \"\"\"Base command line entry point.\"\"\" coloredlogs.install(level=log_level) LOG.info(\"Running cli\") # ... more code here","title":"logging"},{"location":"python/logging/#logging","text":"Logging is the logical next step when you realize the limitations of printing. Python includes a very useful logging module in the standard library so it's rather easy to get started. First make sure you know the basics . Now when you want to log some progress or program state in a module: import logging LOG = logging.getLogger(__name__) def foo(bar): \"\"\"My fancy function.\"\"\" LOG.info(\"incrementing the input: %s\", bar) return bar + 1 The not-so-intuitive part is that you need to configure the logging module to see any output from these calls. Luckily we can reduce the setup to a simple function call most of the time using the coloredlogs package. During e.g. your CLI initialization include: import click import coloredlogs import logging LOG = logging.getLogger(__name__) @click.command() @click.option('-l', '--log-level', default='INFO', help='Log message level to display') def cli(log_level): \"\"\"Base command line entry point.\"\"\" coloredlogs.install(level=log_level) LOG.info(\"Running cli\") # ... more code here","title":"Logging"},{"location":"python/uploading_to_pypi/","text":"Uploading a Python package to PyPI First thing to do is registering a user account on both TestPyPI and PyPI . TestPyPI is a test site where you can verify that the software package is ok before uploading it to the official PyPI repository. To handle these accounts and use the same settings for every submission to PyPI you'll do in the future, you could create a .pypirc file in your home directory. This file should contain the following lines: [distutils] index-servers = pypi testpypi [pypi] username = your pypi username password = your pypi password [testpypi] repository = https://test.pypi.org/legacy/ username = your testpypi username password = your testpypi password Otherwise you could always specify repository name, user and password every time you upload a package. This latter would be the safest option because this way you'd be avoiding storing passwords in plaintext in your home directory. After packaging your software as described here , you are ready to upload it to the Python Package Index (PyPI), a repository specific for Python software. Be sure that all files that have to be included in the package along with with the code are listed in the manifest file (MANIFEST.in). It is necessary for instance to include files such as requirements.txt and LICENSE.txt. Please note that markdown-formatted pages are rendered poorly on the PyPI web pages, so you might want to convert the README.md file to reStructuredText format (.rst). This can be done by using Pandoc . Next step of preparing your package for PyPI would be fixing the setup.py file to include a download_url link to a tarball url. According to the official documentation available on packaging.python.org, packages version should follow the flexible public version scheme specified in PEP 440 . Some examples: 1.2.0.dev1 # Development release 1.2.0a1 # Alpha Release 1.2.0b1 # Beta Release 1.2.0rc1 # Release Candidate 1.2.0 # Final Release 1.2.0.post1 # Post Release 15.10 # Date based release 23 # Serial release is link should be taking into account the version number of the software. If for instance the version of your software is 1.3.0, the setup.py file should be modified by adding this line: setup( ... download_url = 'https://github.com/Clinical-Genomics/Name_of_your_package/tarball/1.3.0', ... ) Note that this url doesn't exist yet and will be created at a later stage. After committing this change and fixing this version of the repository you should add a git tag to the package. This commands adds basically a version number and release to the project and will be showed under the \"releases\" page of the git project. The syntax to add a tag is the following: git tag 1.3.0 -m \"Added a 1.3.0 version tag for PyPI\" git push --tags origin master Next step consists in creating a distribution release of the package. If your package has different requirements under different platforms (Linux, Mac or Windows) you should create platform wheels to taking care of this issue. Here's a documentation page explaining how to create wheel files . If you don't need to care about wheels instead you can just create a distribution using the setup file: python setup.py sdist After creating a distribution you can test the upload on test.pypi. To do the actual upload you'll need twine (pip install twine if it isn't installed already). Upload to test.pypi: twine upload --repository-url https://test.pypi.org/legacy/ dist/* Check now that your repository is on PyPI test, under \"your packages\". You could now attempt to install the package from pip. To do that type: pip install --index-url https://test.pypi.org/simple --extra-index-url https://pypi.org/simple name_of_package The \"--extra-index-url https://pypi.org/simple\" option would instruct pip to retrieve all dependencies for your package from pypi itself. If your package can be found on the test repository and everything is as you expect, it is safe to upload it on the PyPI repository: twine upload --repository-url https://upload.pypi.org/legacy/ dist/* Your package should be now available for installation over internet by using the command \"pip install your_package\".","title":"Uploading a Python package to PyPI"},{"location":"python/uploading_to_pypi/#uploading-a-python-package-to-pypi","text":"First thing to do is registering a user account on both TestPyPI and PyPI . TestPyPI is a test site where you can verify that the software package is ok before uploading it to the official PyPI repository. To handle these accounts and use the same settings for every submission to PyPI you'll do in the future, you could create a .pypirc file in your home directory. This file should contain the following lines: [distutils] index-servers = pypi testpypi [pypi] username = your pypi username password = your pypi password [testpypi] repository = https://test.pypi.org/legacy/ username = your testpypi username password = your testpypi password Otherwise you could always specify repository name, user and password every time you upload a package. This latter would be the safest option because this way you'd be avoiding storing passwords in plaintext in your home directory. After packaging your software as described here , you are ready to upload it to the Python Package Index (PyPI), a repository specific for Python software. Be sure that all files that have to be included in the package along with with the code are listed in the manifest file (MANIFEST.in). It is necessary for instance to include files such as requirements.txt and LICENSE.txt. Please note that markdown-formatted pages are rendered poorly on the PyPI web pages, so you might want to convert the README.md file to reStructuredText format (.rst). This can be done by using Pandoc . Next step of preparing your package for PyPI would be fixing the setup.py file to include a download_url link to a tarball url. According to the official documentation available on packaging.python.org, packages version should follow the flexible public version scheme specified in PEP 440 . Some examples: 1.2.0.dev1 # Development release 1.2.0a1 # Alpha Release 1.2.0b1 # Beta Release 1.2.0rc1 # Release Candidate 1.2.0 # Final Release 1.2.0.post1 # Post Release 15.10 # Date based release 23 # Serial release is link should be taking into account the version number of the software. If for instance the version of your software is 1.3.0, the setup.py file should be modified by adding this line: setup( ... download_url = 'https://github.com/Clinical-Genomics/Name_of_your_package/tarball/1.3.0', ... ) Note that this url doesn't exist yet and will be created at a later stage. After committing this change and fixing this version of the repository you should add a git tag to the package. This commands adds basically a version number and release to the project and will be showed under the \"releases\" page of the git project. The syntax to add a tag is the following: git tag 1.3.0 -m \"Added a 1.3.0 version tag for PyPI\" git push --tags origin master Next step consists in creating a distribution release of the package. If your package has different requirements under different platforms (Linux, Mac or Windows) you should create platform wheels to taking care of this issue. Here's a documentation page explaining how to create wheel files . If you don't need to care about wheels instead you can just create a distribution using the setup file: python setup.py sdist After creating a distribution you can test the upload on test.pypi. To do the actual upload you'll need twine (pip install twine if it isn't installed already). Upload to test.pypi: twine upload --repository-url https://test.pypi.org/legacy/ dist/* Check now that your repository is on PyPI test, under \"your packages\". You could now attempt to install the package from pip. To do that type: pip install --index-url https://test.pypi.org/simple --extra-index-url https://pypi.org/simple name_of_package The \"--extra-index-url https://pypi.org/simple\" option would instruct pip to retrieve all dependencies for your package from pypi itself. If your package can be found on the test repository and everything is as you expect, it is safe to upload it on the PyPI repository: twine upload --repository-url https://upload.pypi.org/legacy/ dist/* Your package should be now available for installation over internet by using the command \"pip install your_package\".","title":"Uploading a Python package to PyPI"},{"location":"python/variables/","text":"Variable hygiene PEP8 recommends short variable names, but achieving this requires good programming hygiene. Here are some advices to keep variable names short. Variable name are not full descriptors First, do not think of variable names as full descriptors of their content. Names should be clear mainly to allow to keep track of where they come from and only then can they give a bit about the content. # This is very descriptive, but we can infer that by looking at the expression students_with_grades_above_90 = filter(lambda s: s.grade > 90, students) # This is short and still allows the reader to infer what the value was good_students = filter(lambda s: s.grade > 90, students) Put details in comments Use comments and doc strings for description of what is going on, not variable names. # We feel we need a long variable description because the function is not documented def get_good_students(students): return filter(lambda s: s.grade > 90, students) students_with_grades_above_90 = get_good_students(students) # If the reader is not sure about what get_good_students returns, # their reflex should be to look at the function docstring def get_good_students(students): \"\"\"Return students with grade above 90\"\"\" return filter(lambda s: s.grade > 90, students) # You can optionally comment here that good_students are the ones with grade > 90 good_students = get_good_students(students) Too specific name might mean too specific code If you feel you need a very specific name for a function, it might be that the function is too specific itself. # Very long name because very specific behaviour def get_students_with_grade_above_90(students): return filter(lambda s: s.grade > 90, students) # Adding a level of abstraction shortens our method name here def get_students_above(grade, students): return filter(lambda s: s.grade > grade, students) # What we mean here is very clear and the code is reusable good_students = get_students_above(90, students) Keep short scopes for quick lookup Finally, encapsulate logic in shorter scopes. This way, you don't need to give that much detail in the variable name, as it can be looked up quickly a few lines above. A rule of thumb is to make your functions fit in your IDE without scrolling and encapsulate some logic in new function if you go beyond that. # line 6 names = ['John', 'Jane'] ... # Hundreds of lines of code # line 371 # Wait what was names again? if 'Kath' in names: ... Here I lost track of what names was and scrolling up will make me lose track of even more. This is better: # line 6 names = ['John', 'Jane'] # I encapsulate part of the logic in another function to keep scope short x = some_function() y = maybe_a_second_function(x) # line 13 # Sure I can lookup names, it is right above if 'Kath' in names: ... Spend time thinking about readability The last but not least is to devote time thinking about how you can make your code more readable. This is as important as the time you spend thinking about the logic and code optimization. The best code is the code that everyone can read, and thus everyone can improve. The above text was shamelessly copied from StackOverflow","title":"variables"},{"location":"python/variables/#variable-hygiene","text":"PEP8 recommends short variable names, but achieving this requires good programming hygiene. Here are some advices to keep variable names short.","title":"Variable hygiene"},{"location":"python/variables/#variable-name-are-not-full-descriptors","text":"First, do not think of variable names as full descriptors of their content. Names should be clear mainly to allow to keep track of where they come from and only then can they give a bit about the content. # This is very descriptive, but we can infer that by looking at the expression students_with_grades_above_90 = filter(lambda s: s.grade > 90, students) # This is short and still allows the reader to infer what the value was good_students = filter(lambda s: s.grade > 90, students)","title":"Variable name are not full descriptors"},{"location":"python/variables/#put-details-in-comments","text":"Use comments and doc strings for description of what is going on, not variable names. # We feel we need a long variable description because the function is not documented def get_good_students(students): return filter(lambda s: s.grade > 90, students) students_with_grades_above_90 = get_good_students(students) # If the reader is not sure about what get_good_students returns, # their reflex should be to look at the function docstring def get_good_students(students): \"\"\"Return students with grade above 90\"\"\" return filter(lambda s: s.grade > 90, students) # You can optionally comment here that good_students are the ones with grade > 90 good_students = get_good_students(students)","title":"Put details in comments"},{"location":"python/variables/#too-specific-name-might-mean-too-specific-code","text":"If you feel you need a very specific name for a function, it might be that the function is too specific itself. # Very long name because very specific behaviour def get_students_with_grade_above_90(students): return filter(lambda s: s.grade > 90, students) # Adding a level of abstraction shortens our method name here def get_students_above(grade, students): return filter(lambda s: s.grade > grade, students) # What we mean here is very clear and the code is reusable good_students = get_students_above(90, students)","title":"Too specific name might mean too specific code"},{"location":"python/variables/#keep-short-scopes-for-quick-lookup","text":"Finally, encapsulate logic in shorter scopes. This way, you don't need to give that much detail in the variable name, as it can be looked up quickly a few lines above. A rule of thumb is to make your functions fit in your IDE without scrolling and encapsulate some logic in new function if you go beyond that. # line 6 names = ['John', 'Jane'] ... # Hundreds of lines of code # line 371 # Wait what was names again? if 'Kath' in names: ... Here I lost track of what names was and scrolling up will make me lose track of even more. This is better: # line 6 names = ['John', 'Jane'] # I encapsulate part of the logic in another function to keep scope short x = some_function() y = maybe_a_second_function(x) # line 13 # Sure I can lookup names, it is right above if 'Kath' in names: ...","title":"Keep short scopes for quick lookup"},{"location":"python/variables/#spend-time-thinking-about-readability","text":"The last but not least is to devote time thinking about how you can make your code more readable. This is as important as the time you spend thinking about the logic and code optimization. The best code is the code that everyone can read, and thus everyone can improve. The above text was shamelessly copied from StackOverflow","title":"Spend time thinking about readability"},{"location":"python/count_variants/count_module/","text":"3. First module {#first_module} Ok so finally we can start to write some actual python code! In this section we will learn about file structure, imports and a little bit about command line interfaces. When we finnish this section we should have a small program that counts the number of variants in a vcf. We will try to keep the code separated inside our projekt as described here , we store the counting code in a submodule called utils and the cli code in a module called cli. So run: mkdir count_variants/cli count_variants/cli touch count_variants/cli/__init__.py touch count_variants/utils/__init__.py touch count_variants/cli/root.py touch count_variants/utils/count.py Ok a lot of files created here, the __init__.py files we know from before, they are there to tell that this is a module. count_variants/cli/root.py will include the command line interface to our program and count_variants/utils/count.py will include the code that counts vcf variants. 3.1 Writing the first function {#first_function} Open count_variants/utils/count.py and write the following def count_variants(vcf): \"\"\"Count the number of variants in a vcf file Args: vcf(iterable): An iterable with variants Returns: nr_variants(int): Number of variants in file \"\"\" nr_variants = 0 for variant in vcf: nr_variants += 1 return nr_variants The text within quotation marks is a docstring, EVERY function you write should have a docstring that at least explains what the intention of the function is and input and output. 3.2 Writing the CLI {#cli} Now open count_variants/cli/root.py and write: import logging import click import coloredlogs from cyvcf2 import VCF from count_variants.utils.count import count_variants LOG = logging.getLogger(__name__) @click.command() @click.argument('vcf', type=click.Path(exists=True)) def cli(vcf): \"\"\"Count the variants in a vcf\"\"\" coloredlogs.install(level='INFO') LOG.info(\"Reading vcf file: %s\", vcf) vcf_obj = VCF(vcf) nr_variants = count_variants(vcf_obj) click.echo(\"Nr variants in vcf: %s\" % nr_variants) It is a good convention to import standard library modules first, then third party modules and finally local imports. In this case we will go through the code line by line. logging is the standard library solution for logging in python. click is our prefered package to build command line interfaces. coloredlogs is a wrapper for logging that makes logging look good and super easy. cyvcf2 is a reliable and fast parser for VCF files count_variants.utils.count on this line we import the function that we wrote in the previous section LOG = logging.getLogger(__name__) here we create a logging object and name it LOG @click.command() is a decorator , you do not need to understand the concept right now. It basically give the following function some properties @click.argument() this is how an argument is defined in click Then comes the function definition, the docstring and some code that should be rather straight forward. 3.3 Missing modules The modules in standard library follows with the python distribution so there is no need to install those. As you may have noticed there are some modules used here that are missing in our requirements.txt file. It can be a bit hard to know what third party modules that will be used when starting a project. So lets add coloredlogs and cyvcf2 to requirements.txt , when you are done it should look like: $ cat requirements.txt click coloredlogs cyvcf2 If everything is done correct the folder structure should look like this now: count_variants/ \u251c\u2500\u2500 LICENSE \u251c\u2500\u2500 README.md \u251c\u2500\u2500 MANIFEST.in \u251c\u2500\u2500 .gitignore \u251c\u2500\u2500 setup.py \u251c\u2500\u2500 count_variants/ \u251c\u2500\u2500 __init__.py \u251c\u2500\u2500 __version__.py \u251c\u2500\u2500 cli/ \u251c\u2500\u2500 __init__.py \u251c\u2500\u2500 root.py \u251c\u2500\u2500 utils/ \u251c\u2500\u2500 __init__.py \u251c\u2500\u2500 count.py If you haven't done so already, stage all the files and commit them!","title":"3. First module {#first_module}"},{"location":"python/count_variants/count_module/#3-first-module-first_module","text":"Ok so finally we can start to write some actual python code! In this section we will learn about file structure, imports and a little bit about command line interfaces. When we finnish this section we should have a small program that counts the number of variants in a vcf. We will try to keep the code separated inside our projekt as described here , we store the counting code in a submodule called utils and the cli code in a module called cli. So run: mkdir count_variants/cli count_variants/cli touch count_variants/cli/__init__.py touch count_variants/utils/__init__.py touch count_variants/cli/root.py touch count_variants/utils/count.py Ok a lot of files created here, the __init__.py files we know from before, they are there to tell that this is a module. count_variants/cli/root.py will include the command line interface to our program and count_variants/utils/count.py will include the code that counts vcf variants.","title":"3. First module {#first_module}"},{"location":"python/count_variants/count_module/#31-writing-the-first-function-first_function","text":"Open count_variants/utils/count.py and write the following def count_variants(vcf): \"\"\"Count the number of variants in a vcf file Args: vcf(iterable): An iterable with variants Returns: nr_variants(int): Number of variants in file \"\"\" nr_variants = 0 for variant in vcf: nr_variants += 1 return nr_variants The text within quotation marks is a docstring, EVERY function you write should have a docstring that at least explains what the intention of the function is and input and output.","title":"3.1 Writing the first function {#first_function}"},{"location":"python/count_variants/count_module/#32-writing-the-cli-cli","text":"Now open count_variants/cli/root.py and write: import logging import click import coloredlogs from cyvcf2 import VCF from count_variants.utils.count import count_variants LOG = logging.getLogger(__name__) @click.command() @click.argument('vcf', type=click.Path(exists=True)) def cli(vcf): \"\"\"Count the variants in a vcf\"\"\" coloredlogs.install(level='INFO') LOG.info(\"Reading vcf file: %s\", vcf) vcf_obj = VCF(vcf) nr_variants = count_variants(vcf_obj) click.echo(\"Nr variants in vcf: %s\" % nr_variants) It is a good convention to import standard library modules first, then third party modules and finally local imports. In this case we will go through the code line by line. logging is the standard library solution for logging in python. click is our prefered package to build command line interfaces. coloredlogs is a wrapper for logging that makes logging look good and super easy. cyvcf2 is a reliable and fast parser for VCF files count_variants.utils.count on this line we import the function that we wrote in the previous section LOG = logging.getLogger(__name__) here we create a logging object and name it LOG @click.command() is a decorator , you do not need to understand the concept right now. It basically give the following function some properties @click.argument() this is how an argument is defined in click Then comes the function definition, the docstring and some code that should be rather straight forward.","title":"3.2 Writing the CLI {#cli}"},{"location":"python/count_variants/count_module/#33-missing-modules","text":"The modules in standard library follows with the python distribution so there is no need to install those. As you may have noticed there are some modules used here that are missing in our requirements.txt file. It can be a bit hard to know what third party modules that will be used when starting a project. So lets add coloredlogs and cyvcf2 to requirements.txt , when you are done it should look like: $ cat requirements.txt click coloredlogs cyvcf2 If everything is done correct the folder structure should look like this now: count_variants/ \u251c\u2500\u2500 LICENSE \u251c\u2500\u2500 README.md \u251c\u2500\u2500 MANIFEST.in \u251c\u2500\u2500 .gitignore \u251c\u2500\u2500 setup.py \u251c\u2500\u2500 count_variants/ \u251c\u2500\u2500 __init__.py \u251c\u2500\u2500 __version__.py \u251c\u2500\u2500 cli/ \u251c\u2500\u2500 __init__.py \u251c\u2500\u2500 root.py \u251c\u2500\u2500 utils/ \u251c\u2500\u2500 __init__.py \u251c\u2500\u2500 count.py If you haven't done so already, stage all the files and commit them!","title":"3.3 Missing modules"},{"location":"python/count_variants/installing/","text":"Installing So it is time to install our package and see that everything works, exciting! \ud83d\ude03 There is only one thing missing; tell setup.py how to use the command line interface that we wrote earlier. Open setup.py and look for entry_points section. Change it to look like: entry_points={ 'console_scripts': [ 'count_variants = count_variants.cli.root:cli' ], }, We are telling setup.py that if someone write count_variants on the command line run the function cli() in the file count_variants.cli.root . Now we are going to run the command that installs all dependencies and the package itself: pip install --editable . Watch the magic happen! \ud83d\udd2e When you are done you should be able to write: $ count_variants Usage: count_variants [OPTIONS] VCF Error: Missing argument \"vcf\". Great job!!! If you have a vcf file lying around you could try and feed that to the program and see if it works. In the following sections we will add some complexity, write tests and push the code to GitHub so others can see it.","title":"Installing"},{"location":"python/count_variants/installing/#installing","text":"So it is time to install our package and see that everything works, exciting! \ud83d\ude03 There is only one thing missing; tell setup.py how to use the command line interface that we wrote earlier. Open setup.py and look for entry_points section. Change it to look like: entry_points={ 'console_scripts': [ 'count_variants = count_variants.cli.root:cli' ], }, We are telling setup.py that if someone write count_variants on the command line run the function cli() in the file count_variants.cli.root . Now we are going to run the command that installs all dependencies and the package itself: pip install --editable . Watch the magic happen! \ud83d\udd2e When you are done you should be able to write: $ count_variants Usage: count_variants [OPTIONS] VCF Error: Missing argument \"vcf\". Great job!!! If you have a vcf file lying around you could try and feed that to the program and see if it works. In the following sections we will add some complexity, write tests and push the code to GitHub so others can see it.","title":"Installing"},{"location":"python/count_variants/overview/","text":"Creating a Python package To be able to reuse code it is important to package things we are doing in a way that makes them easy to install and operate. There are of course a number of ways to handle this and we will describe our prefered way to do it here. I will use an example as we go along; we are going to create a package called count_variants that is used to count variants with different selection criteria in a VCF file. In this example we will tie together many of the topics described in this guide, like testing , conventions , git , logging and perhaps some more. Overview 1. Start a structure 1.1 Basics 1.2 README 1.3 LICENSE etc 1.4 First commit 2. setup.py 3. First Module 3.1 first function 3.2 cli 3.3 missing modules 3. Installing next","title":"Creating a Python package"},{"location":"python/count_variants/overview/#creating-a-python-package","text":"To be able to reuse code it is important to package things we are doing in a way that makes them easy to install and operate. There are of course a number of ways to handle this and we will describe our prefered way to do it here. I will use an example as we go along; we are going to create a package called count_variants that is used to count variants with different selection criteria in a VCF file. In this example we will tie together many of the topics described in this guide, like testing , conventions , git , logging and perhaps some more.","title":"Creating a Python package"},{"location":"python/count_variants/overview/#overview","text":"1. Start a structure 1.1 Basics 1.2 README 1.3 LICENSE etc 1.4 First commit 2. setup.py 3. First Module 3.1 first function 3.2 cli 3.3 missing modules 3. Installing next","title":"Overview"},{"location":"python/count_variants/setup_py/","text":"2. setup.py Every python package contains a special file called setup.py in the root directory. This file describes some meta information about the package and how to create a distribution. There is a excellent guide for how to setup a package and create the setup.py here . Lets continue with our example following @kennethreitz guide, please read the guide first. Copy setup.py from the guide and open it, if you do it right it should look like: #!/usr/bin/env python # -*- coding: utf-8 -*- # Note: To use the 'upload' functionality of this file, you must: # $ pip install twine import io import os import sys from shutil import rmtree from setuptools import find_packages, setup, Command # Package meta-data. NAME = 'mypackage' DESCRIPTION = 'My short description for my project. ' URL = 'https://github.com/me/myproject' EMAIL = 'me@example.com' AUTHOR = 'Awesome Soul' # What packages are required for this module to be executed? REQUIRED = [ # 'requests', 'maya', 'records', ] # The rest you shouldn't have to touch too much :) # ------------------------------------------------ # Except, perhaps the License and Trove Classifiers! # If you do change the License, remember to change the Trove Classifier for that! here = os.path.abspath(os.path.dirname(__file__)) # Import the README and use it as the long-description. # Note: this will only work if 'README.rst' is present in your MANIFEST.in file! with io.open(os.path.join(here, 'README.rst'), encoding='utf-8') as f: long_description = '\\n' + f.read() # Load the package's __version__.py module as a dictionary. about = {} with open(os.path.join(here, NAME, '__version__.py')) as f: exec(f.read(), about) class UploadCommand(Command): \"\"\"Support setup.py upload.\"\"\" description = 'Build and publish the package.' user_options = [] @staticmethod def status(s): \"\"\"Prints things in bold.\"\"\" print('\\033[1m{0}\\033[0m'.format(s)) def initialize_options(self): pass def finalize_options(self): pass def run(self): try: self.status('Removing previous builds\u2026') rmtree(os.path.join(here, 'dist')) except OSError: pass self.status('Building Source and Wheel (universal) distribution\u2026') os.system('{0} setup.py sdist bdist_wheel --universal'.format(sys.executable)) self.status('Uploading the package to PyPi via Twine\u2026') os.system('twine upload dist/*') sys.exit() # Where the magic happens: setup( name=NAME, version=about['__version__'], description=DESCRIPTION, long_description=long_description, author=AUTHOR, author_email=EMAIL, url=URL, packages=find_packages(exclude=('tests',)), # If your package is a single module, use this instead of 'packages': # py_modules=['mypackage'], # entry_points={ # 'console_scripts': ['mycli=mymodule:cli'], # }, install_requires=REQUIRED, include_package_data=True, license='MIT', classifiers=[ # Trove classifiers # Full list: https://pypi.python.org/pypi?%3Aaction=list_classifiers 'License :: OSI Approved :: MIT License', 'Programming Language :: Python', 'Programming Language :: Python :: 2.6', 'Programming Language :: Python :: 2.7', 'Programming Language :: Python :: 3', 'Programming Language :: Python :: 3.3', 'Programming Language :: Python :: 3.4', 'Programming Language :: Python :: 3.5', 'Programming Language :: Python :: 3.6', 'Programming Language :: Python :: Implementation :: CPython', 'Programming Language :: Python :: Implementation :: PyPy' ], # $ setup.py publish support. cmdclass={ 'upload': UploadCommand, }, ) We will now edit the file so to follow our package. We are going to change the name of our package, the author name and description . Since we keep our requirements in a special file we are going to edit how to tell setup.py about those. We will also change README.rst to README.md . When you are done the file should look like: #!/usr/bin/env python # -*- coding: utf-8 -*- # Note: To use the 'upload' functionality of this file, you must: # $ pip install twine import io import os import sys from shutil import rmtree from setuptools import find_packages, setup, Command # Package meta-data. NAME = 'count_variants' DESCRIPTION = 'Count variants in VCF files based on different criterias' URL = 'https://github.com/moonso/count_variants' EMAIL = 'mans.magnusson@scilifelab.se' AUTHOR = 'M\u00e5ns Magnusson' here = os.path.abspath(os.path.dirname(__file__)) def parse_reqs(req_path='./requirements.txt'): \"\"\"Recursively parse requirements from nested pip files.\"\"\" install_requires = [] with io.open(os.path.join(here, 'requirements.txt'), encoding='utf-8') as handle: # remove comments and empty lines lines = (line.strip() for line in handle if line.strip() and not line.startswith('#')) for line in lines: # check for nested requirements files if line.startswith('-r'): # recursively call this function install_requires += parse_reqs(req_path=line[3:]) else: # add the line as a new requirement install_requires.append(line) return install_requires # What packages are required for this module to be executed? REQUIRED = parse_reqs() # The rest you shouldn't have to touch too much :) # ------------------------------------------------ # Except, perhaps the License and Trove Classifiers! # If you do change the License, remember to change the Trove Classifier for that! # Import the README and use it as the long-description. # Note: this will only work if 'README.rst' is present in your MANIFEST.in file! with io.open(os.path.join(here, 'README.md'), encoding='utf-8') as f: long_description = '\\n' + f.read() # Load the package's __version__.py module as a dictionary. about = {} with open(os.path.join(here, NAME, '__version__.py')) as f: exec(f.read(), about) class UploadCommand(Command): \"\"\"Support setup.py upload.\"\"\" description = 'Build and publish the package.' user_options = [] @staticmethod def status(s): \"\"\"Prints things in bold.\"\"\" print('\\033[1m{0}\\033[0m'.format(s)) def initialize_options(self): pass def finalize_options(self): pass def run(self): try: self.status('Removing previous builds\u2026') rmtree(os.path.join(here, 'dist')) except OSError: pass self.status('Building Source and Wheel (universal) distribution\u2026') os.system('{0} setup.py sdist bdist_wheel --universal'.format(sys.executable)) self.status('Uploading the package to PyPi via Twine\u2026') os.system('twine upload dist/*') sys.exit() # Where the magic happens: setup( name=NAME, version=about['__version__'], description=DESCRIPTION, long_description=long_description, author=AUTHOR, author_email=EMAIL, url=URL, packages=find_packages(exclude=('tests',)), # If your package is a single module, use this instead of 'packages': # py_modules=['mypackage'], # entry_points={ # 'console_scripts': ['mycli=mymodule:cli'], # }, install_requires=REQUIRED, include_package_data=True, license='MIT', classifiers=[ # Trove classifiers # Full list: https://pypi.python.org/pypi?%3Aaction=list_classifiers 'License :: OSI Approved :: MIT License', 'Programming Language :: Python', 'Programming Language :: Python :: 2.6', 'Programming Language :: Python :: 2.7', 'Programming Language :: Python :: 3', 'Programming Language :: Python :: 3.3', 'Programming Language :: Python :: 3.4', 'Programming Language :: Python :: 3.5', 'Programming Language :: Python :: 3.6', 'Programming Language :: Python :: Implementation :: CPython', 'Programming Language :: Python :: Implementation :: PyPy' ], # $ setup.py publish support. cmdclass={ 'upload': UploadCommand, }, ) Now we are about to add the directory where we will write the actual python code. Each module in the package must have a file called __init__.py to tell python that this is a module. From the documentation The init .py files are required to make Python treat the directories as containing packages; this is done to prevent directories with a common name, such as string, from unintentionally hiding valid modules that occur later (deeper) on the module search path. In the simplest case, init .py can just be an empty file, but it can also execute initialization code for the package or set the all variable, described later. mkdir count_variants touch count_variants/__init__.py touch count_variants/__version__.py open count_variants/__version__.py and write: __version__ = '0.1.0' The folder structure should now look like: count_variants/ \u251c\u2500\u2500 LICENSE \u251c\u2500\u2500 README.md \u251c\u2500\u2500 MANIFEST.in \u251c\u2500\u2500 .gitignore \u251c\u2500\u2500 setup.py \u251c\u2500\u2500 count_variants/ \u251c\u2500\u2500 __init__.py \u251c\u2500\u2500 __version__.py Add all the new files to a new git commit. (Tip: use git status to see if all files are committed.)","title":"2. setup.py"},{"location":"python/count_variants/setup_py/#2-setuppy","text":"Every python package contains a special file called setup.py in the root directory. This file describes some meta information about the package and how to create a distribution. There is a excellent guide for how to setup a package and create the setup.py here . Lets continue with our example following @kennethreitz guide, please read the guide first. Copy setup.py from the guide and open it, if you do it right it should look like: #!/usr/bin/env python # -*- coding: utf-8 -*- # Note: To use the 'upload' functionality of this file, you must: # $ pip install twine import io import os import sys from shutil import rmtree from setuptools import find_packages, setup, Command # Package meta-data. NAME = 'mypackage' DESCRIPTION = 'My short description for my project. ' URL = 'https://github.com/me/myproject' EMAIL = 'me@example.com' AUTHOR = 'Awesome Soul' # What packages are required for this module to be executed? REQUIRED = [ # 'requests', 'maya', 'records', ] # The rest you shouldn't have to touch too much :) # ------------------------------------------------ # Except, perhaps the License and Trove Classifiers! # If you do change the License, remember to change the Trove Classifier for that! here = os.path.abspath(os.path.dirname(__file__)) # Import the README and use it as the long-description. # Note: this will only work if 'README.rst' is present in your MANIFEST.in file! with io.open(os.path.join(here, 'README.rst'), encoding='utf-8') as f: long_description = '\\n' + f.read() # Load the package's __version__.py module as a dictionary. about = {} with open(os.path.join(here, NAME, '__version__.py')) as f: exec(f.read(), about) class UploadCommand(Command): \"\"\"Support setup.py upload.\"\"\" description = 'Build and publish the package.' user_options = [] @staticmethod def status(s): \"\"\"Prints things in bold.\"\"\" print('\\033[1m{0}\\033[0m'.format(s)) def initialize_options(self): pass def finalize_options(self): pass def run(self): try: self.status('Removing previous builds\u2026') rmtree(os.path.join(here, 'dist')) except OSError: pass self.status('Building Source and Wheel (universal) distribution\u2026') os.system('{0} setup.py sdist bdist_wheel --universal'.format(sys.executable)) self.status('Uploading the package to PyPi via Twine\u2026') os.system('twine upload dist/*') sys.exit() # Where the magic happens: setup( name=NAME, version=about['__version__'], description=DESCRIPTION, long_description=long_description, author=AUTHOR, author_email=EMAIL, url=URL, packages=find_packages(exclude=('tests',)), # If your package is a single module, use this instead of 'packages': # py_modules=['mypackage'], # entry_points={ # 'console_scripts': ['mycli=mymodule:cli'], # }, install_requires=REQUIRED, include_package_data=True, license='MIT', classifiers=[ # Trove classifiers # Full list: https://pypi.python.org/pypi?%3Aaction=list_classifiers 'License :: OSI Approved :: MIT License', 'Programming Language :: Python', 'Programming Language :: Python :: 2.6', 'Programming Language :: Python :: 2.7', 'Programming Language :: Python :: 3', 'Programming Language :: Python :: 3.3', 'Programming Language :: Python :: 3.4', 'Programming Language :: Python :: 3.5', 'Programming Language :: Python :: 3.6', 'Programming Language :: Python :: Implementation :: CPython', 'Programming Language :: Python :: Implementation :: PyPy' ], # $ setup.py publish support. cmdclass={ 'upload': UploadCommand, }, ) We will now edit the file so to follow our package. We are going to change the name of our package, the author name and description . Since we keep our requirements in a special file we are going to edit how to tell setup.py about those. We will also change README.rst to README.md . When you are done the file should look like: #!/usr/bin/env python # -*- coding: utf-8 -*- # Note: To use the 'upload' functionality of this file, you must: # $ pip install twine import io import os import sys from shutil import rmtree from setuptools import find_packages, setup, Command # Package meta-data. NAME = 'count_variants' DESCRIPTION = 'Count variants in VCF files based on different criterias' URL = 'https://github.com/moonso/count_variants' EMAIL = 'mans.magnusson@scilifelab.se' AUTHOR = 'M\u00e5ns Magnusson' here = os.path.abspath(os.path.dirname(__file__)) def parse_reqs(req_path='./requirements.txt'): \"\"\"Recursively parse requirements from nested pip files.\"\"\" install_requires = [] with io.open(os.path.join(here, 'requirements.txt'), encoding='utf-8') as handle: # remove comments and empty lines lines = (line.strip() for line in handle if line.strip() and not line.startswith('#')) for line in lines: # check for nested requirements files if line.startswith('-r'): # recursively call this function install_requires += parse_reqs(req_path=line[3:]) else: # add the line as a new requirement install_requires.append(line) return install_requires # What packages are required for this module to be executed? REQUIRED = parse_reqs() # The rest you shouldn't have to touch too much :) # ------------------------------------------------ # Except, perhaps the License and Trove Classifiers! # If you do change the License, remember to change the Trove Classifier for that! # Import the README and use it as the long-description. # Note: this will only work if 'README.rst' is present in your MANIFEST.in file! with io.open(os.path.join(here, 'README.md'), encoding='utf-8') as f: long_description = '\\n' + f.read() # Load the package's __version__.py module as a dictionary. about = {} with open(os.path.join(here, NAME, '__version__.py')) as f: exec(f.read(), about) class UploadCommand(Command): \"\"\"Support setup.py upload.\"\"\" description = 'Build and publish the package.' user_options = [] @staticmethod def status(s): \"\"\"Prints things in bold.\"\"\" print('\\033[1m{0}\\033[0m'.format(s)) def initialize_options(self): pass def finalize_options(self): pass def run(self): try: self.status('Removing previous builds\u2026') rmtree(os.path.join(here, 'dist')) except OSError: pass self.status('Building Source and Wheel (universal) distribution\u2026') os.system('{0} setup.py sdist bdist_wheel --universal'.format(sys.executable)) self.status('Uploading the package to PyPi via Twine\u2026') os.system('twine upload dist/*') sys.exit() # Where the magic happens: setup( name=NAME, version=about['__version__'], description=DESCRIPTION, long_description=long_description, author=AUTHOR, author_email=EMAIL, url=URL, packages=find_packages(exclude=('tests',)), # If your package is a single module, use this instead of 'packages': # py_modules=['mypackage'], # entry_points={ # 'console_scripts': ['mycli=mymodule:cli'], # }, install_requires=REQUIRED, include_package_data=True, license='MIT', classifiers=[ # Trove classifiers # Full list: https://pypi.python.org/pypi?%3Aaction=list_classifiers 'License :: OSI Approved :: MIT License', 'Programming Language :: Python', 'Programming Language :: Python :: 2.6', 'Programming Language :: Python :: 2.7', 'Programming Language :: Python :: 3', 'Programming Language :: Python :: 3.3', 'Programming Language :: Python :: 3.4', 'Programming Language :: Python :: 3.5', 'Programming Language :: Python :: 3.6', 'Programming Language :: Python :: Implementation :: CPython', 'Programming Language :: Python :: Implementation :: PyPy' ], # $ setup.py publish support. cmdclass={ 'upload': UploadCommand, }, ) Now we are about to add the directory where we will write the actual python code. Each module in the package must have a file called __init__.py to tell python that this is a module. From the documentation The init .py files are required to make Python treat the directories as containing packages; this is done to prevent directories with a common name, such as string, from unintentionally hiding valid modules that occur later (deeper) on the module search path. In the simplest case, init .py can just be an empty file, but it can also execute initialization code for the package or set the all variable, described later. mkdir count_variants touch count_variants/__init__.py touch count_variants/__version__.py open count_variants/__version__.py and write: __version__ = '0.1.0' The folder structure should now look like: count_variants/ \u251c\u2500\u2500 LICENSE \u251c\u2500\u2500 README.md \u251c\u2500\u2500 MANIFEST.in \u251c\u2500\u2500 .gitignore \u251c\u2500\u2500 setup.py \u251c\u2500\u2500 count_variants/ \u251c\u2500\u2500 __init__.py \u251c\u2500\u2500 __version__.py Add all the new files to a new git commit. (Tip: use git status to see if all files are committed.)","title":"2. setup.py"},{"location":"python/count_variants/structure/","text":"1. Setup a structure To start with let's create a environment and a directory for our package. Assuming you have conda installed, run the following: 1.1 Basics conda create -n count_variants python=3 source activate count_variants mkdir count_variants cd count_variants We wan't to do version handling with git, to make this a git repository do git init touch .gitignore The .gitignore file is explained in the git section , for now we only need to know that this file tells git what to care and not care about. 1.2 README Every decent python package should at least include a README file that describes the intention of the writer. Let's create a README.md file, this will be updated as we go along. For now do touch README.md open the file with an editor of your choice and write the following: # count_variants A package to count variants in VCF files based on different criterias. You might recognize this as \"markdown\", a markup language for formatting text. You can read more about it under Documentation . 1.3 LICENSE, MANIFEST.in, requirements.txt {#license} Next step is to include a LICENSE file, every project should have one. There are many different licenses, you can read more here . For now we will choose a MIT license . Open a file called LICENSE and copy paste the MIT License and fill in year and name. MANIFEST.in is a file that explains what non python static files should be included in the distribution. For now we will leave that one empty. Just do: touch MANIFEST.in requirements.txt is for declaring which third party software our package depends on. Right now we only know that we will be using Click to build a cli, so run: echo click > requirements.txt 1.4 First commit {#first_commit} Save the changes. It is now time to do our first commit! Run: $ git status On branch master Initial commit Untracked files: (use \"git add <file>...\" to include in what will be committed) .gitignore README.md LICENSE MANIFEST.in requirements.txt nothing added to commit but untracked files present (use \"git add\" to track) $ git add .gitignore README.md LICENSE MANIFEST.in requirements.txt $ git commit -m \"First commit\" $ git status On branch master nothing to commit, working tree clean","title":"1. Setup a structure"},{"location":"python/count_variants/structure/#1-setup-a-structure","text":"To start with let's create a environment and a directory for our package. Assuming you have conda installed, run the following:","title":"1. Setup a structure"},{"location":"python/count_variants/structure/#11-basics","text":"conda create -n count_variants python=3 source activate count_variants mkdir count_variants cd count_variants We wan't to do version handling with git, to make this a git repository do git init touch .gitignore The .gitignore file is explained in the git section , for now we only need to know that this file tells git what to care and not care about.","title":"1.1 Basics"},{"location":"python/count_variants/structure/#12-readme","text":"Every decent python package should at least include a README file that describes the intention of the writer. Let's create a README.md file, this will be updated as we go along. For now do touch README.md open the file with an editor of your choice and write the following: # count_variants A package to count variants in VCF files based on different criterias. You might recognize this as \"markdown\", a markup language for formatting text. You can read more about it under Documentation .","title":"1.2 README"},{"location":"python/count_variants/structure/#13-license-manifestin-requirementstxt-license","text":"Next step is to include a LICENSE file, every project should have one. There are many different licenses, you can read more here . For now we will choose a MIT license . Open a file called LICENSE and copy paste the MIT License and fill in year and name. MANIFEST.in is a file that explains what non python static files should be included in the distribution. For now we will leave that one empty. Just do: touch MANIFEST.in requirements.txt is for declaring which third party software our package depends on. Right now we only know that we will be using Click to build a cli, so run: echo click > requirements.txt","title":"1.3 LICENSE, MANIFEST.in, requirements.txt  {#license}"},{"location":"python/count_variants/structure/#14-first-commit-first_commit","text":"Save the changes. It is now time to do our first commit! Run: $ git status On branch master Initial commit Untracked files: (use \"git add <file>...\" to include in what will be committed) .gitignore README.md LICENSE MANIFEST.in requirements.txt nothing added to commit but untracked files present (use \"git add\" to track) $ git add .gitignore README.md LICENSE MANIFEST.in requirements.txt $ git commit -m \"First commit\" $ git status On branch master nothing to commit, working tree clean","title":"1.4 First commit {#first_commit}"},{"location":"python/testing/","text":"Python testing In general we use pytest for testing Python code. You might ask why! Well, this is why: testandcode Motivation Reasons to test code are plentiful; performance, quality, usability, security, and stability. However, the focus will be on testing to ensure expected behavior . The main idea is that your suite of tests should insprire confidence that the code performs as expected. Any additional updates should pass all tests before being considered in the main branch. This way, developer who contribtute to the project don\u2019t fear breaking something \ud83d\udc1e. One more thing: when you write tests for your code, you write code which is easy to test. The result is functional, maintainable, and composable! The habit loop A habit is formed when you no longer need to motivate yourself to perform a routine. You do it automatically. Following a cue, acting out your routine, and claiming your reward. It\u2019s only natural to feel like tests are but an additional step that you would rather skip and just deploy! But without tests you will never feel confident about the execution of your code. Just like working out, writing tests is a perfect activity to design a habit around. This is my own habit loop for testing: my cue is writing or altering a function. The routine is writing tests for it. My reward is better/complete test coverage . I now crave to maintain a green badge of 100% coverage and peace of mind that executing my code does what I expect. It\u2019s important to make this loop as simple as possible. That\u2019s why you should invest time in learning and setting up test automation. Read more about habits and how to master them in The Power of Habit , by Charles Duhigg. Strategy: GIVEN-WHEN-THEN It can be difficult to get started writing tests. Where do I begin? My routine is to follow step-by-step instructions in a very simple model: GIVEN-WHEN-THEN. GIVEN : describe the prerequisites for the test you will run and optionally make assertions about your setup WHEN : run your function and explain what is supposed to happen THEN : assert the outcome of your test; return values or side effects Here\u2019s a quick example: # GIVEN the database doesn't contain any rows assert DatabaseRow.query.count() == 0 # WHEN adding a new row to the database new_name = 'Paul' add_row(name=new_name, age=12) # THEN there should be ONE new row added to the database assert DatabaseRow.query.count() == 1 # ... with the expected name assert DatabaseRow.query.first().name == new_name By being explicit about what you're testing, you can get past the initial mental obstacles. It also adds relevancy to writing tests. Without clear comments it can often feel like indecipherable lines that does something you\u2019ve long forgot. This tutorial was originally written by our favorite alumni: Robin Andeer !","title":"GIVEN-WHEN-THEN"},{"location":"python/testing/#python-testing","text":"In general we use pytest for testing Python code. You might ask why! Well, this is why: testandcode","title":"Python testing"},{"location":"python/testing/#motivation","text":"Reasons to test code are plentiful; performance, quality, usability, security, and stability. However, the focus will be on testing to ensure expected behavior . The main idea is that your suite of tests should insprire confidence that the code performs as expected. Any additional updates should pass all tests before being considered in the main branch. This way, developer who contribtute to the project don\u2019t fear breaking something \ud83d\udc1e. One more thing: when you write tests for your code, you write code which is easy to test. The result is functional, maintainable, and composable!","title":"Motivation"},{"location":"python/testing/#the-habit-loop","text":"A habit is formed when you no longer need to motivate yourself to perform a routine. You do it automatically. Following a cue, acting out your routine, and claiming your reward. It\u2019s only natural to feel like tests are but an additional step that you would rather skip and just deploy! But without tests you will never feel confident about the execution of your code. Just like working out, writing tests is a perfect activity to design a habit around. This is my own habit loop for testing: my cue is writing or altering a function. The routine is writing tests for it. My reward is better/complete test coverage . I now crave to maintain a green badge of 100% coverage and peace of mind that executing my code does what I expect. It\u2019s important to make this loop as simple as possible. That\u2019s why you should invest time in learning and setting up test automation. Read more about habits and how to master them in The Power of Habit , by Charles Duhigg.","title":"The habit loop"},{"location":"python/testing/#strategy-given-when-then","text":"It can be difficult to get started writing tests. Where do I begin? My routine is to follow step-by-step instructions in a very simple model: GIVEN-WHEN-THEN. GIVEN : describe the prerequisites for the test you will run and optionally make assertions about your setup WHEN : run your function and explain what is supposed to happen THEN : assert the outcome of your test; return values or side effects Here\u2019s a quick example: # GIVEN the database doesn't contain any rows assert DatabaseRow.query.count() == 0 # WHEN adding a new row to the database new_name = 'Paul' add_row(name=new_name, age=12) # THEN there should be ONE new row added to the database assert DatabaseRow.query.count() == 1 # ... with the expected name assert DatabaseRow.query.first().name == new_name By being explicit about what you're testing, you can get past the initial mental obstacles. It also adds relevancy to writing tests. Without clear comments it can often feel like indecipherable lines that does something you\u2019ve long forgot. This tutorial was originally written by our favorite alumni: Robin Andeer !","title":"Strategy: GIVEN-WHEN-THEN"},{"location":"python/testing/fixtures/","text":"Fixtures (and pytest) Python has a number of test runners to extend and simplify writing (unit) tests. We recommend pytest : a super robust and feature rich test framework. It lets you write tests as simple \u201casserts\u201c, has a brilliant plugin ecosystem that \u201cjust works\u201d after pip install pytest-[somePlugin], and let\u2019s you leverage powerful fixtures to keep things DRY. A small flavor of what tests look like with pytest: import pytest from mypackage import best_movie, perform_division def test_best_movie(): movie = best_movie(director='P.T. Anderson') assert movie == 'There Will Be Blood' def test_perform_division(): with pytest.raises(ValueError): # call with parameters that should yield error perform_division(12, 0) Running your tests is as easy as: $ py.test --verbose Organizing tests pytest does a great job of detecting tests. All you need to do is name test modules with a prefix: test_* . Each test function should similarly be named def test_*: . Furthermore, organize test files to reflect the source code. The following package: myPackage |-- utils.py |-- tools |-- docker.py \u2026 would result in the following test structure: tests |-- test_utils.py |-- tools |-- test_tools_docker.py You notice that the term \u201ctools\u201d is \u201crepeating\u201d for the \u201cdocker\u201d-test module. This is because pytest requires globally unique test module names! Test fixtures Fixtures is the key concept to start mastering tests. They are pluggable components that can be shared across many tests to setup pre-conditions like: setup a database connection read in lines form a file They each have their own setup and tear down blocks and you control if they are reset on a function/module/session basis. Let\u2019s add a few items to our setup: tests |-- fixtures # store static files here |-- test_utils.py |-- tools |-- test_tools_docker.py |-- conftest.py # write fixture functions here Inside conftest.py you can add fixture functions that will be exposed to your tests. You mark a function as a fixture with a decorator. If you don\u2019t need setup/tear down you can use a simple @pytest.fixture . Otherwise it's easiest to use @pytest.yield_fixture . # conftest.py import pytest from myPackage import DatabaseAPI @pytest.yield_fixture(scope='function') def db_connection(): _db_connection = DatabaseAPI(uri=':memory:') _db_connection.create_tables() yield _db_connection _db_connection.teardown_tables() # test_utils.py def test_add_row(db_connection): name = 'Paul T. Anderson' add_row(name=name, age=34) db_connection.save() assert db_connection.get_row(name=name).age == 34 When pytest runs the above function it will look for a fixture called db_connection and run it. Whatever is yielded (or returned) will be passed along to the test function. We set the \u201cscope\u201d of the fixture to \u201cfunction\u201d so as soon as the test is complete, the block after the yield statement will run. You can pass as many fixtures as you want to a test. Tip: test fixtures accept parameter-dependencies the same way as test functions. It\u2019s perfectly possible to combine several test fixtures. Additional fixtures can be installed through plugins and pytest itself comes with a few built in. For example there\u2019s the handy tmpdir fixture that provides unique temporary folders where you can test various side effects. from mypackage import touch def test_write_file(tmpdir): # GIVEN an empty dir assert len(tmpdir.listdir()) == 0 # WHEN touching a new file new_path = tmpdir.join('newfile.txt') touch(str(new_path)) # THEN there should be a new file created assert len(tmpdir.listdir()) == 1","title":"Are you using fixtures?"},{"location":"python/testing/fixtures/#fixtures-and-pytest","text":"Python has a number of test runners to extend and simplify writing (unit) tests. We recommend pytest : a super robust and feature rich test framework. It lets you write tests as simple \u201casserts\u201c, has a brilliant plugin ecosystem that \u201cjust works\u201d after pip install pytest-[somePlugin], and let\u2019s you leverage powerful fixtures to keep things DRY. A small flavor of what tests look like with pytest: import pytest from mypackage import best_movie, perform_division def test_best_movie(): movie = best_movie(director='P.T. Anderson') assert movie == 'There Will Be Blood' def test_perform_division(): with pytest.raises(ValueError): # call with parameters that should yield error perform_division(12, 0) Running your tests is as easy as: $ py.test --verbose","title":"Fixtures (and pytest)"},{"location":"python/testing/fixtures/#organizing-tests","text":"pytest does a great job of detecting tests. All you need to do is name test modules with a prefix: test_* . Each test function should similarly be named def test_*: . Furthermore, organize test files to reflect the source code. The following package: myPackage |-- utils.py |-- tools |-- docker.py \u2026 would result in the following test structure: tests |-- test_utils.py |-- tools |-- test_tools_docker.py You notice that the term \u201ctools\u201d is \u201crepeating\u201d for the \u201cdocker\u201d-test module. This is because pytest requires globally unique test module names!","title":"Organizing tests"},{"location":"python/testing/fixtures/#test-fixtures","text":"Fixtures is the key concept to start mastering tests. They are pluggable components that can be shared across many tests to setup pre-conditions like: setup a database connection read in lines form a file They each have their own setup and tear down blocks and you control if they are reset on a function/module/session basis. Let\u2019s add a few items to our setup: tests |-- fixtures # store static files here |-- test_utils.py |-- tools |-- test_tools_docker.py |-- conftest.py # write fixture functions here Inside conftest.py you can add fixture functions that will be exposed to your tests. You mark a function as a fixture with a decorator. If you don\u2019t need setup/tear down you can use a simple @pytest.fixture . Otherwise it's easiest to use @pytest.yield_fixture . # conftest.py import pytest from myPackage import DatabaseAPI @pytest.yield_fixture(scope='function') def db_connection(): _db_connection = DatabaseAPI(uri=':memory:') _db_connection.create_tables() yield _db_connection _db_connection.teardown_tables() # test_utils.py def test_add_row(db_connection): name = 'Paul T. Anderson' add_row(name=name, age=34) db_connection.save() assert db_connection.get_row(name=name).age == 34 When pytest runs the above function it will look for a fixture called db_connection and run it. Whatever is yielded (or returned) will be passed along to the test function. We set the \u201cscope\u201d of the fixture to \u201cfunction\u201d so as soon as the test is complete, the block after the yield statement will run. You can pass as many fixtures as you want to a test. Tip: test fixtures accept parameter-dependencies the same way as test functions. It\u2019s perfectly possible to combine several test fixtures. Additional fixtures can be installed through plugins and pytest itself comes with a few built in. For example there\u2019s the handy tmpdir fixture that provides unique temporary folders where you can test various side effects. from mypackage import touch def test_write_file(tmpdir): # GIVEN an empty dir assert len(tmpdir.listdir()) == 0 # WHEN touching a new file new_path = tmpdir.join('newfile.txt') touch(str(new_path)) # THEN there should be a new file created assert len(tmpdir.listdir()) == 1","title":"Test fixtures"},{"location":"tools/","text":"1: Git, GitHub, and Markdown These are all essential skills in the modern developer toolbox. Git is our version control system. It's abundant in use in the industry and knowing it well is vital for collaborating and writing code. Readup ! GitHub is a central Git-server which makes collaborating with code easy. We use it to store all our code. GitHub hosts an impressive amount of open source software and is something you should be intimately familiar with! Check out our organisation . Markdown is the go to markup language for authoring documentation, comments, websites, and more. Markdown let's you write documents in plain text without worrying about the formatting - great for productivity! It's another core tool that you want to make friends with \ud83d\ude04 2: Conda, perlbrew Conda (expand) Perlbrew (expand)","title":"What are the commonly used tools?"},{"location":"tools/#1-git-github-and-markdown","text":"These are all essential skills in the modern developer toolbox. Git is our version control system. It's abundant in use in the industry and knowing it well is vital for collaborating and writing code. Readup ! GitHub is a central Git-server which makes collaborating with code easy. We use it to store all our code. GitHub hosts an impressive amount of open source software and is something you should be intimately familiar with! Check out our organisation . Markdown is the go to markup language for authoring documentation, comments, websites, and more. Markdown let's you write documents in plain text without worrying about the formatting - great for productivity! It's another core tool that you want to make friends with \ud83d\ude04","title":"1: Git, GitHub, and Markdown"},{"location":"tools/#2-conda-perlbrew","text":"Conda (expand) Perlbrew (expand)","title":"2: Conda, perlbrew"}]}